[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "\"CSCI\" + \" 0451\"\n\n'CSCI 0451'"
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Classifying Palmer Penguins",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html#setting-up-the-data",
    "href": "posts/new-test-post/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for trainging:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what the data looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/new-test-post/index.html#lets-explore-the-data",
    "href": "posts/new-test-post/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\n\ntrain.groupby(['Species', 'Island']).aggregate(\"mean\")\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 1:10]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\n\n\n\n\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nPAL0708\n115\n22\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN61A1\nN11A2\n\n\nGentoo\nPAL0708\n124\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN8A2\nN11A1\n\n\nDream\nAdelie\nPAL0708\n152\n31\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN85A2\nN21A1\n\n\nChinstrap\nPAL0708\n68\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN99A2\nN100A1\n\n\nTorgersen\nAdelie\nPAL0708\n132\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN9A2\nN10A1\n\n\n\n\n\n\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 10:20]\n\n\n\n\n\n\n\n\n\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nYes\nNo\n11/9/09\n11/10/07\n45.6\n34.5\n21.1\n16.0\n203.0\n172.0\n\n\nGentoo\nYes\nNo\n12/3/07\n11/13/08\n55.9\n40.9\n17.3\n13.1\n230.0\n207.0\n\n\nDream\nAdelie\nYes\nNo\n11/9/07\n11/10/08\n44.1\n34.0\n21.2\n16.5\n208.0\n178.0\n\n\nChinstrap\nYes\nNo\n12/3/07\n11/14/08\n58.0\n40.9\n20.8\n16.4\n212.0\n178.0\n\n\nTorgersen\nAdelie\nYes\nNo\n11/9/08\n11/11/07\n46.0\n34.1\n21.5\n15.9\n210.0\n176.0\n\n\n\n\n\n\n\nPotentially egg date and Flipper length and island.\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 20:29]\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\n4725.0\n2850.0\n9.79532\n8.08138\n-24.36130\n-26.78958\n\n\nGentoo\n6300.0\n3950.0\n8.83352\n7.63220\n-25.00169\n-27.01854\n\n\nDream\nAdelie\n4650.0\n2975.0\n9.72764\n8.01485\n-24.52698\n-26.69543\n\n\nChinstrap\n4800.0\n2700.0\n10.02544\n8.47173\n-23.78767\n-25.14550\n\n\nTorgersen\nAdelie\n4700.0\n2900.0\n9.59462\n7.69778\n-23.90309\n-26.53870\n\n\n\n\n\n\n\n\ntrain.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\") \n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\nSpecies\nClutch Completion\nIsland\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nDream\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n2\n3\n\n\nTorgersen\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n\n\nYes\nBiscoe\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n0\n\n\nDream\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n41\n39\n39\n3\n\n\nTorgersen\n35\n35\n35\n35\n35\n35\n34\n34\n34\n34\n30\n29\n29\n8\n\n\nChinstrap\nNo\nDream\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n0\n\n\nYes\nDream\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n46\n47\n0\n\n\nGentoo\nNo\nBiscoe\n8\n8\n8\n8\n8\n8\n7\n7\n7\n7\n7\n7\n7\n0\n\n\nYes\nBiscoe\n90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n87\n89\n89\n0\n\n\n\n\n\n\n\n\ntabledf = train.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\")\ntabledf = tabledf.drop(tabledf.columns[1:], axis=1)\ntabledf = tabledf.rename(columns={\"studyName\": \"Count\"})\ntabledf.reset_index(level = ['Clutch Completion', 'Island'], inplace = True)\ntabledf\n\n\n\n\n\n\n\n\nClutch Completion\nIsland\nCount\n\n\nSpecies\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n\n\nAdelie\nNo\nDream\n3\n\n\nAdelie\nNo\nTorgersen\n7\n\n\nAdelie\nYes\nBiscoe\n31\n\n\nAdelie\nYes\nDream\n42\n\n\nAdelie\nYes\nTorgersen\n35\n\n\nChinstrap\nNo\nDream\n10\n\n\nChinstrap\nYes\nDream\n47\n\n\nGentoo\nNo\nBiscoe\n8\n\n\nGentoo\nYes\nBiscoe\n90\n\n\n\n\n\n\n\n\ntabledf.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"sum\") / tabledf.groupby(['Species', 'Island']).aggregate(\"sum\")\n\n\n\n\n\n\n\n\n\n\nCount\n\n\nSpecies\nIsland\nClutch Completion\n\n\n\n\n\nAdelie\nBiscoe\nNo\n0.060606\n\n\nYes\n0.939394\n\n\nDream\nNo\n0.066667\n\n\nYes\n0.933333\n\n\nTorgersen\nNo\n0.166667\n\n\nYes\n0.833333\n\n\nChinstrap\nDream\nNo\n0.175439\n\n\nYes\n0.824561\n\n\nGentoo\nBiscoe\nNo\n0.081633\n\n\nYes\n0.918367\n\n\n\n\n\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html",
    "href": "posts/Auditing-bias-blog-post/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog post, a Machine Learning Model is trained, using the American Community’s Public Use Microdata Sample data, to predict employment status of individuals using features other than race. A bias audit is then performed to evaluate if the model exhibits racial bias. In our bias audit, we prove that proxies are leading to individuals from specific racial categories being treated very differently with regards to error rate balance and calibration. This, and the implications with regards to our model, are discussed further in the final discussion."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#abstract",
    "href": "posts/Auditing-bias-blog-post/index.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog post, a Machine Learning Model is trained, using the American Community’s Public Use Microdata Sample data, to predict employment status of individuals using features other than race. A bias audit is then performed to evaluate if the model exhibits racial bias. In our bias audit, we prove that proxies are leading to individuals from specific racial categories being treated very differently with regards to error rate balance and calibration. This, and the implications with regards to our model, are discussed further in the final discussion."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#descriptive-analysis",
    "href": "posts/Auditing-bias-blog-post/index.html#descriptive-analysis",
    "title": "Auditing Bias",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis"
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#loading-the-data",
    "href": "posts/Auditing-bias-blog-post/index.html#loading-the-data",
    "title": "Auditing Bias",
    "section": "Loading the Data",
    "text": "Loading the Data\nDownload the data from the PUMAs fataset for the survey from 2018, at the individual level, and for the state of California.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport pandas as pd\nimport numpy as np\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000004\n9\n1\n3701\n4\n6\n1013097\n32\n30\n...\n34\n60\n60\n7\n8\n59\n33\n8\n58\n32\n\n\n1\nP\n2018GQ0000013\n9\n1\n7306\n4\n6\n1013097\n45\n18\n...\n0\n0\n0\n91\n46\n46\n0\n89\n45\n0\n\n\n2\nP\n2018GQ0000016\n9\n1\n3755\n4\n6\n1013097\n109\n69\n...\n105\n232\n226\n110\n114\n217\n2\n111\n2\n106\n\n\n3\nP\n2018GQ0000020\n9\n1\n7319\n4\n6\n1013097\n34\n25\n...\n67\n0\n34\n34\n69\n0\n34\n35\n0\n0\n\n\n4\nP\n2018GQ0000027\n9\n1\n6511\n4\n6\n1013097\n46\n31\n...\n47\n81\n10\n11\n79\n47\n44\n81\n47\n10\n\n\n\n\n5 rows × 286 columns\n\n\n\nLet’s limit the scope of features which are relevant to our problem:\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\nRAC1P is our data variable for race, and so that should be excluded from our set of predictor variables. ESR is our target variable for employment status. Let’s make a subset of features excluding these:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we can construct a BasicProblem around our dataset and these different sets of features we have created. This problem defines our target (employment status) by transforming it from a categorical variable to a dichotemus one that is 1 if the individual is employed, and 0 for all other categories of employment and unemployment. It defines our grouping variable as the race of the individual (RAC1P).\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(378817, 15)\n(378817,)\n(378817,)\n\n\nNow that we have set up our features, label, and group we should perform our train test split:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nLet’s look through the data in dataframe form:\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf.shape\n\n(303053, 17)\n\n\nThere are 303053 individuals who filled out the survey in California in the year of 2018.\nHow many of these individuals are employed?\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n77.0\n16.0\n2.0\n0.0\n2.0\n0.0\n4.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n1\n29.0\n20.0\n5.0\n2.0\n2.0\n0.0\n3.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n6\nTrue\n\n\n2\n60.0\n21.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n\n\n3\n27.0\n19.0\n5.0\n13.0\n2.0\n0.0\n1.0\n1.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n\n\n4\n63.0\n23.0\n3.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n\n\n\n\n\n\ndf.groupby(\"label\").size()\n\nlabel\nFalse    164678\nTrue     138375\ndtype: int64\n\n\nThere are 138,375 employed individuals in our data subset, and 164,678 unemployed individuals in our dataset.Let’s break that down by racial category.\n\ndf.groupby([\"group\", \"label\"]).size()\n\ngroup  label\n1      False    100725\n       True      85639\n2      False      8845\n       True       5702\n3      False      1371\n       True        895\n4      False        17\n       True          7\n5      False       410\n       True        307\n6      False     23987\n       True      23493\n7      False       517\n       True        459\n8      False     19069\n       True      16227\n9      False      9737\n       True       5646\ndtype: int64\n\n\nThe number of employed individuals who filled out the survey, based on racial category, are as follows: White: 85,639 Black/African American: 5,702 American Indian: 895 Alaska Native: 7 American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races: 307 Asian: 23,493 Native Hawaiian and Other Pacific Islander: 459 Any other race alone: 16,227 Two or More Races: 5,646\n\ntrue_group_prop = df.groupby(\"group\")[\"label\"].mean()\ntrue_group_prop\n\ngroup\n1    0.459525\n2    0.391971\n3    0.394969\n4    0.291667\n5    0.428173\n6    0.494798\n7    0.470287\n8    0.459740\n9    0.367029\nName: label, dtype: float64\n\n\nThe ratio of employed individuals in each racial category are as follows:\nWhite: 0.459525\nBlack/African American: 0.391971\nAmerican Indian: 0.394969\nAlaska Native: 0.291667\nAmerican Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races: 0.428173\nAsian: 0.494798\nNative Hawaiian and Other Pacific Islander: 0.470287\nAny other race alone: 0.459740\nTwo or More Races: 0.367029\n\nIntersectional Trends of Race, Gender, and Employment\nLet’s create a relational visualization of Employment based on Race and Gender:\n\nimport seaborn as sns\ndf_plot = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\ndf_plot[\"SEX\"] = df_plot[\"SEX\"].replace({1: \"MALE\", 2: \"FEMALE\"})\n\n\np1 = sns.barplot(df_plot, x = \"group\", y = \"label\", hue = \"SEX\")\n\n\n\n\n\n\n\n\nWhile in most racial categories, Male employment rates (in our dataset) are higher than Female employment rates, there is a lot of variation. Other races, Asian, Alaskan Native, and White Male’s have especially higher levels of employment compared to their Female counterparts. Female’s outperform male’s in the Black/African American categories and the “American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races” category."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#training-the-model",
    "href": "posts/Auditing-bias-blog-post/index.html#training-the-model",
    "title": "Auditing Bias",
    "section": "Training the Model",
    "text": "Training the Model\nI chose to use a decision tree classifier for this model. In order to maximize the performance of this model, all max_depths from 1 to 20 are tested in order to identify the most accurate one. Accuracy in this sense is defined as the highest mean accuracy in a cross validation with cv = 5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nmax_depth_best = 1\nCV_score_best = 0\nfor i in range(1, 20):\n    model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=i))\n    model.fit(X_train, y_train)\n    cv_scores_RF = cross_val_score(model, X_train, y_train, cv=5)\n    if cv_scores_RF.mean( ) &gt; CV_score_best:\n        max_depth_best = i\n        CV_score_best = cv_scores_RF.mean()\n\nprint(\"Best Maximum Depth: \", max_depth_best)\nprint(\"Cross Validation Mean: \", CV_score_best)\n\nBest Maximum Depth:  11\nCross Validation Mean:  0.8178305372286386\n\n\nWith the optimal maximum depth determined, let’s train our model with it, a standard scalar, and our training data. The pipeline first puts our data through a standard scalar. This scales our feature variables to all be on a standard scale so that features with a much larger scale don’t have a larger impact on our predictions. The scaled data is then fed through the pipeline to our decision tree classifier with the optimal maximum depth identified.\n\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=max_depth_best))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])\n\n\n\nCross Validation\nLet’s test our model using cross validation\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_RF = cross_val_score(model, X_train, y_train, cv=5)\ncv_scores_RF\n\narray([0.81867978, 0.81640296, 0.82061012, 0.81752186, 0.81598746])\n\n\nOur highest level of cross validation is achieved with a max depth of 11 in our Decision Classifier Tree. Our cross validation is scoring between .816 and .820 on all divisions."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#audit",
    "href": "posts/Auditing-bias-blog-post/index.html#audit",
    "title": "Auditing Bias",
    "section": "Audit",
    "text": "Audit\n\nOverall Accuracy\nLet’s see how our accurate our model performs on the test data:\n\ny_hat = model.predict(X_test)\ndf_audit = pd.DataFrame(X_test, columns = features_to_use)\ndf_audit[\"group\"] = group_test\ndf_audit[\"label\"] = y_test\ndf_audit[\"pred\"] = y_hat\n\n\n\n(y_hat == y_test).mean()\n\n0.8180270313077451\n\n\nOur overall testing accuracy is .818, which is in the same range as the values we observed in our cross validation. Knowing that our model is performing similar to how we expected based on the training data.\n\n\nOverall Positive Predictive Value\nLet’s calculate the value of our PPV:\n\ndf_audit[\"correct\"] = df_audit[\"label\"] == df_audit[\"pred\"]\ndf_audit.groupby(\"pred\")[\"correct\"].mean()\n\npred\nFalse    0.872495\nTrue     0.766095\nName: correct, dtype: float64\n\n\nThe model is more accurate at predicting unemployment than it is at predicting employment. Our positive predictive value (PPV) of .766 means that for all individuals predicted to be employed, the model was correct about that prediction 76.6% of the time.\n\n\nError Rate Balance\nNow Let’s calculate our overall error rate balance.\n\ndf_audit.groupby(\"label\")[\"pred\"].mean()\n\nlabel\nFalse    0.219470\nTrue     0.863048\nName: pred, dtype: float64\n\n\nThis shows us that the model predicted 86.3% of the employed individuals to be employed. It was similarly accurate on the unempoloyed—21.8% of unemployed individuals were incorrectly predicted to be employed. Our False positive rate is therefor 21.8% and our false negative rate is 13.7%.\n\n\nFNR and FPR\nSimilarly, we can demonstrate the overall False Negative and False Positive rates.\n\ndf_audit.groupby(\"label\")[\"pred\"].mean()\n\nlabel\nFalse    0.219470\nTrue     0.863048\nName: pred, dtype: float64\n\n\n\n\n\nTN = df_audit[(df_audit[\"label\"] == False) & (df_audit[\"pred\"] == False)].shape[0]\nFP = df_audit[(df_audit[\"label\"] == False) & (df_audit[\"pred\"] == True)].shape[0]\nFN = df_audit[(df_audit[\"label\"] == True) & (df_audit[\"pred\"] == False)].shape[0]\nTP = df_audit[(df_audit[\"label\"] == True) & (df_audit[\"pred\"] == True)].shape[0]\n\nFPR = FP / (FP + TN)\nFNR = FN / (FN + TP)\n\nFPR\n\n0.21946971163150764\n\n\nOur False Positive rate is .219. This means that for all individuals who are unemployed, 21.9% were predicted to be employed.\n\nFNR\n\n0.1369524805390961\n\n\nOur False Negative Rate (FNR) is .137. This means that for all individuals who are employed, 13.7% were predicted to be unemployed.\nThis implies that our model probably, although we have not statistically proven it, has a bias towards accuracy for individuals who are employed. Simply, our model is more likely to accurately predict an individuals employment status if they are employed than if they are unemployed.\nLet’s turn to an analysis of bias at the group level.\n\n\nGroup Level Audit\nLet’s see how the model performed when comparing performance on individuals of different races.\n\nAccuracy\n\ndf_audit.groupby(\"group\")[\"correct\"].mean()\n\ngroup\n1    0.818402\n2    0.819832\n3    0.794918\n4    0.750000\n5    0.768750\n6    0.808710\n7    0.767241\n8    0.815245\n9    0.857067\nName: correct, dtype: float64\n\n\nThe two groups which include alaskan natives, and pacific islanders have significantly lower accuracy than the other groups as seen above. The group with the highest accuracy is that for individuals who identify as more than 1 race.\n\n\nPPV — By group\nLet’s calculate the PPV again, but this time focussing on group level statistics.\n\nsufficiency_group = df_audit.groupby([\"pred\", \"group\"])[\"correct\"].mean().reset_index()\nsufficiency_group\n\n\n\n\n\n\n\n\npred\ngroup\ncorrect\n\n\n\n\n0\nFalse\n1\n0.867996\n\n\n1\nFalse\n2\n0.904984\n\n\n2\nFalse\n3\n0.910714\n\n\n3\nFalse\n4\n1.000000\n\n\n4\nFalse\n5\n0.789474\n\n\n5\nFalse\n6\n0.855773\n\n\n6\nFalse\n7\n0.859813\n\n\n7\nFalse\n8\n0.874412\n\n\n8\nFalse\n9\n0.927180\n\n\n9\nTrue\n1\n0.769908\n\n\n10\nTrue\n2\n0.726912\n\n\n11\nTrue\n3\n0.675277\n\n\n12\nTrue\n4\n0.666667\n\n\n13\nTrue\n5\n0.750000\n\n\n14\nTrue\n6\n0.772183\n\n\n15\nTrue\n7\n0.688000\n\n\n16\nTrue\n8\n0.761661\n\n\n17\nTrue\n9\n0.763060\n\n\n\n\n\n\n\nThe composite group for native north american’s have a significantly lower accuracy for negatively predicted values at 79%. This means that the model is more likely to predict individuals who are unemployed as employed from this group. It is interesting to not that this group does not have a corresponding different Positive predictive value. Instead, Alaskan Natives have the lowest PPV at .667. This means that alaskan natives, who are predicted to be employed are only employed 66.67% of the time. For both the NPV and PPV, their is significant variation among groups. While it is not perfect, I would say that this model is not horribly calibrated, especially when compared to it’s error rate balance below.\n\n\n\nError Rate Balance — By Group\n\ndf_audit.groupby([\"label\", \"group\"])[\"pred\"].mean()\n\nlabel  group\nFalse  1        0.213280\n       2        0.216629\n       3        0.256560\n       4        0.500000\n       5        0.259259\n       6        0.255399\n       7        0.297710\n       8        0.231342\n       9        0.160084\nTrue   1        0.856422\n       2        0.875171\n       3        0.879808\n       4        1.000000\n       5        0.797468\n       6        0.873391\n       7        0.851485\n       8        0.870073\n       9        0.886561\nName: pred, dtype: float64\n\n\nThe False FPR and TPRs shown above demonstrate that once again our model is treating different groups very differently. The range for FPR are from .16 for individuals of 2 or more races up to .5 for Alaskan Natives. Our model is very likely to predict Alaskan Native’s as being employed, with 100% of employed individuals accurately predicted and 50% of unemployed individuals predicted as employed as well. Most other race categories are in similar ranges of .21 - .26 for FPV and .86-.88 for TPV values. The group with the lowest FPR are those from 2 or more races with a FPR of .16 and a TPR of .89. Similarly with the PPVs, the group 5, for individuals from Alaskan Native or American Indian tribes not specified, the TPR is the lowest of all the groups at .797. Based on these numbers it is clear that our model is not quite calibrated. Certain groups clearly have been scrutinized much more by the data, and it is hard to predict what the negative consequences of a model like this that is not well calibrated might be. Particularly, the treatment of the group 5 worries me, as that group and Alaskan Natives are consistently treated differently from the other groups in terms of Error Rate Balance and PPV. While Black and White groups are balanced quite well, it is concerning to me that other, likely smaller and even more disadvantaged groups are not similarly balanced.\n\n\nStatistical Parity\nLet’s see how race influences the chances of being categorized as employed:\n\ndf_audit.groupby(\"group\")[\"pred\"].mean().reset_index()\n\n\n\n\n\n\n\n\ngroup\npred\n\n\n\n\n0\n1\n0.505611\n\n\n1\n2\n0.478190\n\n\n2\n3\n0.491833\n\n\n3\n4\n0.750000\n\n\n4\n5\n0.525000\n\n\n5\n6\n0.563024\n\n\n6\n7\n0.538793\n\n\n7\n8\n0.524757\n\n\n8\n9\n0.427205\n\n\n\n\n\n\n\nOur data shows that the model does not reach statistical parity. The range of predicted proportions of employed individuals across groups range from .75 to .427. While we have not proven that this is statistically significant, the difference is large enough that I highly doubt it would not be.\n\n\nFigure\nLet’s make a figure to explore the FPR and FNR.\nFirst lets make a function to get the rates by group and store them.\n\ndef get_rates(group):\n    FP = sum((group[\"pred\"] == 1) & (group[\"label\"] == 0))\n    FN = sum((group[\"pred\"] == 0) & (group[\"label\"] == 1))\n    TN = sum((group[\"pred\"] == 0) & (group[\"label\"] == 0))\n    TP = sum((group[\"pred\"] == 1) & (group[\"label\"] == 1))\n    FPR = FP / (FP + TN) if FP + TN &gt; 0 else 0\n    FNR = FN / (FN + TP) if FN + TP &gt; 0 else 0\n    return pd.Series({\"FPR\": FPR, \"FNR\": FNR})\n\nNow lets pass in the data to get the rates for each group, and lets make a color palette for our visualization.\n\nrates = df_audit.groupby(\"group\").apply(get_rates).reset_index()\nrates\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nFPR\nFNR\n\n\n\n\n0\n1\n0.213280\n0.143578\n\n\n1\n2\n0.216629\n0.124829\n\n\n2\n3\n0.256560\n0.120192\n\n\n3\n4\n0.500000\n0.000000\n\n\n4\n5\n0.259259\n0.202532\n\n\n5\n6\n0.255399\n0.126609\n\n\n6\n7\n0.297710\n0.148515\n\n\n7\n8\n0.231342\n0.129927\n\n\n8\n9\n0.160084\n0.113439\n\n\n\n\n\n\n\nWe will use the PPV of 0.666667, as it falls close to the middle of the existing values of our model for all groups. We can calculate each groups prevalence of employment by taking the mean of the label group.\nNow we can pass the data to a plot to create our visualization. This plot will include a scatter plot of the observed values, and lines showing the theoretical values.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nPPV = 0.666667\np = df_audit.groupby(\"group\")[\"label\"].mean().reset_index()\ndef calc_FPR(FNR, PPV, prev):\n    return ((prev)/(1-prev))*((1-PPV)/(PPV))*(1-FNR)\n\ndf_list = []\nfor i, row in p.iterrows():\n    FNR_vals = np.linspace(0, 1, 100)\n    FPR_vals = calc_FPR(FNR_vals, PPV, row[\"label\"])\n    df_FC = pd.DataFrame({\"FNR\": FNR_vals, \"FPR\": FPR_vals, \"group\": row[\"group\"]})\n    df_list.append(df_FC)\n\ndf_FC = pd.concat(df_list)\ndf_FC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFNR\nFPR\ngroup\n\n\n\n\n0\n0.000000\n0.416650\n1.0\n\n\n1\n0.010101\n0.412441\n1.0\n\n\n2\n0.020202\n0.408232\n1.0\n\n\n3\n0.030303\n0.404024\n1.0\n\n\n4\n0.040404\n0.399815\n1.0\n\n\n...\n...\n...\n...\n\n\n95\n0.959596\n0.011748\n9.0\n\n\n96\n0.969697\n0.008811\n9.0\n\n\n97\n0.979798\n0.005874\n9.0\n\n\n98\n0.989899\n0.002937\n9.0\n\n\n99\n1.000000\n0.000000\n9.0\n\n\n\n\n900 rows × 3 columns\n\n\n\n\npalette = [\n    \"#1f77b4\",  # Muted blue\n    \"#ff7f0e\",  # Safety orange\n    \"#2ca02c\",  # Cooked asparagus green\n    \"#d62728\",  # Brick red\n    \"#9467bd\",  # Muted purple\n    \"#8c564b\",  # Chestnut brown\n    \"#e377c2\",  # Raspberry yogurt pink\n    \"#7f7f7f\",  # Middle gray\n    \"#bcbd22\"   # Curry yellow-green\n]\n\ngroup_colors = {group: color for group, color in zip(df_FC['group'].unique(), palette)}\n\nfig, ax = plt.subplots(1,1,figsize=(10, 6))\nfor group, color in group_colors.items():\n    df_group = df_FC[df_FC['group'] == group]\n    df_rates = rates[rates['group'] == group]\n    ax.scatter(x=df_rates[\"FNR\"],y =df_rates[\"FPR\"],  color=color, label=f\"Scatter {group}\")\n    ax.plot(df_group[\"FNR\"], df_group[\"FPR\"],color=color, label=f\"Line {group}\")\n\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"False Negative Rate\")\nplt.title(\"Feasible (FNR, FPR) Combinations\")\nplt.suptitle(\"PPV = 0.666667\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn order, for example, to make our FPR for Native Hawaiian or Pacific Islander (group 7) to match our FPR for white individuals (group 1), we would need to increase the false negative rate for group 7 to about .5. This would significantly reduce our accuracy, and would not necessarily be beneficial for either group involved. For a group like alaskan native (group 4) we would have to accept a FNR of between .6 and .7. Either of these changes would probably result in a significant reduction in overall accuracy and not result in a more equal model."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#discussion",
    "href": "posts/Auditing-bias-blog-post/index.html#discussion",
    "title": "Auditing Bias",
    "section": "Discussion",
    "text": "Discussion\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\n\nThe government might be interested in a model for predicting employment status in order to deliver aid to communities most in need. On the other hand, any commercial entity that requires economic trust in an individual(credit card companies, banks, land loards) might find it beneficial to be able to predict the employment status of individuals if they cannot sift through all applications they are receiving for their service.\nOur model would result in significant disparities in the ways in which different groups are treated. Most interestingly, the Alaskan Native population had a FPR of .5 and a FNR of 0. While such a low FNR might seem great, the tradeoff is that individuals are quite often being identified as employed when they are in-fact not. Even this seemingly beneficial categorization could have a very negative implication for the individual such as not receiving necessary aid, or being given a loan they cannot afford.\nMy model exhibits significant Error Rate balance issues, such as the one mentioned in the paragraph above. It is especially concerning to me how differently the various groups are being treated in this category, with FPR ranging from 0 to 21% and FNR ranging from 50 to 16%. Its error rate balance, while still exhibiting some levels of bias is much less concerning than the error rate balance.\nI generally do not feel comfortable deploying models that have to do with classifying individuals. I believe especially strongly about this when it comes to government use of these models. If governments are supposed to treat individuals equally, it makes no sense to allow them to treat you differently based on similarities between group categorizations. I think individuals deserve to be reviewed as an individual."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html",
    "href": "posts/Sparse-Kernel-Machines/index.html",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "This blog post explores how using a sparse kernel method and how different lambda and gamma values impact the fitting of machine learning methods. Adjusting each of these values impacts the fit and potential overfit of the model. Gamma determines how many of the closest points are taken into account in the classification of each other point, while lambda determines the strength of the regularization of the model—how many or few weights will be forced to be zero.\n\n%load_ext autoreload\n%autoreload 2\nfrom sparse_kernel_logistic import KernelLogisticRegression, GradientDescentOptimizer\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nLet’s generate some data that is mostly linearly separable:\n\nimport torch \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.random.seed(123)\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Basic Classification Data\")\n\nText(0.5, 0.98, 'Basic Classification Data')\n\n\n\n\n\n\n\n\n\n\n\nUse this link to access the code for our Sparse Kernel Machine.\nFor this blog post we will be looking at the use of the rbf_kernel defined in the following code block.\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n\nTo start, let’s explore how the model is classified using a relatively low lambda of 0.1 and a relatively low gamma of 1. The low lambda will mean that more points will have non-zero weights, but the medium level of gamma means that each point will influence a larger region during classification\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.1700)\n\n\nLet’s define a function to plot decision boundaries as well as highlighting those points with non-zero values for their weights.\n\ndef plot_decision_boundary(KR, X, y, ax = None, highlight = True):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    x1 = X1.ravel()\n    x2 = X2.ravel()\n\n    X_ = torch.stack((x1, x2), dim = 1)\n\n    preds = KR.score(X_, recompute_kernel = True)\n    preds = 1.0*torch.reshape(preds, X1.size())\n\n\n    ax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \n    vmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n    )\n    plot_classification_data(X, y, ax)\n    if highlight:\n        ax.scatter(KR.Xt[ix, 0].detach(), KR.Xt[ix, 1].detach(), \n            facecolors=\"none\", edgecolors=\"black\", linewidths=1.5)\n\n\n\n\nplot_decision_boundary(KR, X, y, ax=None)\nplt.title(\"Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\\(\\lambda\\) is very large, there may be only one point in the training data with weight distinguishable from zero.\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 5, gamma = 1)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_decision_boundary(KR, X, y, ax)\nplt.title(\"Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\nAs we can see in the above visualization, when lambda is high, most of the weights for the points are regularized down to zero. Only a couple of points at most have non-zero weights.\n\n\n\nWith lambda reset to a normal value, and gamma made to a very high bandwidth/value of 10 we will see more points with non-zero weights, as each point has a very local decision region: each point only looks at the points very close to it for classification.\n\nKR1 = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n\nKR1.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\nplot_decision_boundary(KR1, X, y)\nplt.title(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$')\n\n\n\n\n\n\n\n\n\nAs we can see from the large quantity of highlighted plots, as well as the wavy decision boundaries, when gamma is high the decision regions become highly localized to nearby plots.\n\n\n\nLet’s use the Moons from Sklearn to generate non-linear data:\n\nfrom sklearn.datasets import make_moons\n\nX_np, y_np = make_moons(200, shuffle=True, noise=0.3)\n\n#convert to torch tensors\nX = torch.tensor(X_np, dtype=torch.float32)\ny = torch.tensor(y_np, dtype=torch.float32)\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.01)\n\n\nplot_decision_boundary(KR, X, y, ax = None)\nplt.suptitle(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 5$\")\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nBy adjusting the gamma value to be quite high, while leaving the lambda value relatively low, we enable the model to classify non-linearly separable data pretty well. However, the decision regions we have here are highly localized and therefore at risk of overfitting-a phenomenon we will explore more in Part B. The localized nature can be seen in the wavy decision boundry, and high number of points with non-zero weights. This is compounded by the fact that we have a low regularization/lambda value so few points have non-zero weights.\n\n\n\n\nSparse Kernel Machines give us a lot of flexibility in how the data will be fit by our models, but this added control means that we have a high risk of overfitting our model on our training data. Let’s explore this possibility further. Let’s define a function to generate data, and use that function to generate a train and test set:\n\ndef make_moons_train_test(n, shuffle=True, noise=0.4):\n    X, y = make_moons(n, shuffle=shuffle, noise=noise)\n    X = torch.tensor(X, dtype=torch.float32)\n    y = torch.tensor(y, dtype=torch.float32)\n    return X, y\n\n#convert to torch tensors\nX_train, y_train = make_moons_train_test(50, shuffle=True)\nX_test, y_test = make_moons_train_test(50, shuffle=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Test Data\")\nfig.suptitle(\"Overfitting: Nonlinearly Separable\", fontsize=16)\n\n\n\nText(0.5, 0.98, 'Overfitting: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nAs we can see our training and test data have similar trends, but also significant variation. if we fit the training data too closely we will most likely overfit the model.\nLet’s making a training loop so that we can track accuracy of train and test data at each iteration.\n\ndef train_model(model, num_steps, X_train, y_train, X_test, y_test, lr):\n    model.Xt = X_train\n\n    if model.a is None:\n        model.a = torch.zeros(X_train.shape[0], requires_grad=False)\n\n\n    # instantiate an optimizer -- gradient descent today\n    opt = GradientDescentOptimizer(model)\n\n    # collect the values of the loss in each step\n    acc_train_vec = []\n    acc_test_vec   = []\n\n\n\n    for i in range(num_steps): \n                \n        opt.step(X_train, y_train, lr=lr)  \n\n        # for tracking model progress on the training set\n        train_acc = (model.predict(X_train) == y_train).float().mean()   \n        acc_train_vec.append(train_acc.item())   \n\n        test_acc = (model.predict(X_test) == y_test).float().mean()\n        acc_test_vec.append(test_acc.item())\n\n    return acc_train_vec, acc_test_vec\n\n\n# KR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n# loss_train, loss_test = train_model(model = KR, num_steps=5000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)\n\nThe following experiment will track each step in the fitting of the model to our training data, recording the models accuracy on the training and testing data at each step. In doing so we can track the performance of the model with each iteration.\n\ndef plot_experiment(model):\n    fig, ax = plt.subplots(1, 3, figsize = (9, 3))\n    acc_train, acc_test = train_model(model = model, num_steps=50000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)  \n    ax[0].plot(acc_train, c = \"steelblue\", label = \"Training\")\n    ax[0].plot(acc_test, c = \"goldenrod\", label = \"Testing\")\n    ax[0].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n    ax[0].legend()\n    ax[0].set_title(\"Training and Testing Accuracy vs Iteration\")\n    ax[1].set_title(\"Training Data\")\n    ax[2].set_title(\"Testing Data\")    \n\n    plot_decision_boundary(model, X_train, y_train, ax = ax[1], highlight=True)  \n    plot_decision_boundary(model, X_test, y_test, ax = ax[2], highlight=False)   \n    plt.tight_layout()\n\n\nNow let’s plot our findings.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\nplot_experiment(KR)\n\n\n\n\n\n\n\n\nFascinating. As we can see in the first plot, our accuracy begins relatively high for both our testing and training data, but decreases on the testing data as more iterations occur. This makes sense when looking at the decision regions. We have highly localized decision regions built around the training data, that does not fit the variation of the testing data.\n\n\naccuracy = (KR.predict(X_train) == y_train).float().mean()\nprint(f\"Train accuracy: {accuracy:.2f}\")\n\nTrain accuracy: 0.94\n\n\nThe result of this is that we have a very high training accuracy, but this is not a good thing.\n\naccuracy = (KR.predict(X_test) == y_test).float().mean()\nprint(f\"Test accuracy: {accuracy:.2f}\")\n\nTest accuracy: 0.80\n\n\nOur testing accuracy is significantly worse.\n\n\n\nAs we have demonstrated in the blog post, using sparse kernel machines gives significantly more control over how our models are fit. With control of the bandwidth (gamma), or the relationship between the distance between two points and the similarity of them, and lambda, the regularization of the weights, we can get very high accuracy when training models. This can even be done on non-linearly separable data. But this comes with risks. If we fit the data too much on the training data, then variation in training and testing data will result in overfitting. This was demonstrated in Part B, where we showed that a highly localized and non-normalized training of non linear data with a decent amount of noise will result in a poor performance when tresting our model.\nAll of this blog post demonstrates the importance of cross validation for the selection of parameters during training in order to prevent overfitting. But cross validation requires more time, energy, and computing power."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#data-generation",
    "href": "posts/Sparse-Kernel-Machines/index.html#data-generation",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "Let’s generate some data that is mostly linearly separable:\n\nimport torch \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.random.seed(123)\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Basic Classification Data\")\n\nText(0.5, 0.98, 'Basic Classification Data')\n\n\n\n\n\n\n\n\n\n\n\nUse this link to access the code for our Sparse Kernel Machine.\nFor this blog post we will be looking at the use of the rbf_kernel defined in the following code block.\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n\nTo start, let’s explore how the model is classified using a relatively low lambda of 0.1 and a relatively low gamma of 1. The low lambda will mean that more points will have non-zero weights, but the medium level of gamma means that each point will influence a larger region during classification\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.1700)\n\n\nLet’s define a function to plot decision boundaries as well as highlighting those points with non-zero values for their weights.\n\ndef plot_decision_boundary(KR, X, y, ax = None, highlight = True):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    x1 = X1.ravel()\n    x2 = X2.ravel()\n\n    X_ = torch.stack((x1, x2), dim = 1)\n\n    preds = KR.score(X_, recompute_kernel = True)\n    preds = 1.0*torch.reshape(preds, X1.size())\n\n\n    ax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \n    vmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n    )\n    plot_classification_data(X, y, ax)\n    if highlight:\n        ax.scatter(KR.Xt[ix, 0].detach(), KR.Xt[ix, 1].detach(), \n            facecolors=\"none\", edgecolors=\"black\", linewidths=1.5)\n\n\n\n\nplot_decision_boundary(KR, X, y, ax=None)\nplt.title(\"Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$')"
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#basic-experiments",
    "href": "posts/Sparse-Kernel-Machines/index.html#basic-experiments",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "When ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\\(\\lambda\\) is very large, there may be only one point in the training data with weight distinguishable from zero.\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 5, gamma = 1)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_decision_boundary(KR, X, y, ax)\nplt.title(\"Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\nAs we can see in the above visualization, when lambda is high, most of the weights for the points are regularized down to zero. Only a couple of points at most have non-zero weights.\n\n\n\nWith lambda reset to a normal value, and gamma made to a very high bandwidth/value of 10 we will see more points with non-zero weights, as each point has a very local decision region: each point only looks at the points very close to it for classification.\n\nKR1 = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n\nKR1.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\nplot_decision_boundary(KR1, X, y)\nplt.title(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$')\n\n\n\n\n\n\n\n\n\nAs we can see from the large quantity of highlighted plots, as well as the wavy decision boundaries, when gamma is high the decision regions become highly localized to nearby plots.\n\n\n\nLet’s use the Moons from Sklearn to generate non-linear data:\n\nfrom sklearn.datasets import make_moons\n\nX_np, y_np = make_moons(200, shuffle=True, noise=0.3)\n\n#convert to torch tensors\nX = torch.tensor(X_np, dtype=torch.float32)\ny = torch.tensor(y_np, dtype=torch.float32)\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.01)\n\n\nplot_decision_boundary(KR, X, y, ax = None)\nplt.suptitle(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 5$\")\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nBy adjusting the gamma value to be quite high, while leaving the lambda value relatively low, we enable the model to classify non-linearly separable data pretty well. However, the decision regions we have here are highly localized and therefore at risk of overfitting-a phenomenon we will explore more in Part B. The localized nature can be seen in the wavy decision boundry, and high number of points with non-zero weights. This is compounded by the fact that we have a low regularization/lambda value so few points have non-zero weights."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#part-b-demonstrating-overfitting",
    "href": "posts/Sparse-Kernel-Machines/index.html#part-b-demonstrating-overfitting",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "Sparse Kernel Machines give us a lot of flexibility in how the data will be fit by our models, but this added control means that we have a high risk of overfitting our model on our training data. Let’s explore this possibility further. Let’s define a function to generate data, and use that function to generate a train and test set:\n\ndef make_moons_train_test(n, shuffle=True, noise=0.4):\n    X, y = make_moons(n, shuffle=shuffle, noise=noise)\n    X = torch.tensor(X, dtype=torch.float32)\n    y = torch.tensor(y, dtype=torch.float32)\n    return X, y\n\n#convert to torch tensors\nX_train, y_train = make_moons_train_test(50, shuffle=True)\nX_test, y_test = make_moons_train_test(50, shuffle=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Test Data\")\nfig.suptitle(\"Overfitting: Nonlinearly Separable\", fontsize=16)\n\n\n\nText(0.5, 0.98, 'Overfitting: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nAs we can see our training and test data have similar trends, but also significant variation. if we fit the training data too closely we will most likely overfit the model.\nLet’s making a training loop so that we can track accuracy of train and test data at each iteration.\n\ndef train_model(model, num_steps, X_train, y_train, X_test, y_test, lr):\n    model.Xt = X_train\n\n    if model.a is None:\n        model.a = torch.zeros(X_train.shape[0], requires_grad=False)\n\n\n    # instantiate an optimizer -- gradient descent today\n    opt = GradientDescentOptimizer(model)\n\n    # collect the values of the loss in each step\n    acc_train_vec = []\n    acc_test_vec   = []\n\n\n\n    for i in range(num_steps): \n                \n        opt.step(X_train, y_train, lr=lr)  \n\n        # for tracking model progress on the training set\n        train_acc = (model.predict(X_train) == y_train).float().mean()   \n        acc_train_vec.append(train_acc.item())   \n\n        test_acc = (model.predict(X_test) == y_test).float().mean()\n        acc_test_vec.append(test_acc.item())\n\n    return acc_train_vec, acc_test_vec\n\n\n# KR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n# loss_train, loss_test = train_model(model = KR, num_steps=5000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)\n\nThe following experiment will track each step in the fitting of the model to our training data, recording the models accuracy on the training and testing data at each step. In doing so we can track the performance of the model with each iteration.\n\ndef plot_experiment(model):\n    fig, ax = plt.subplots(1, 3, figsize = (9, 3))\n    acc_train, acc_test = train_model(model = model, num_steps=50000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)  \n    ax[0].plot(acc_train, c = \"steelblue\", label = \"Training\")\n    ax[0].plot(acc_test, c = \"goldenrod\", label = \"Testing\")\n    ax[0].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n    ax[0].legend()\n    ax[0].set_title(\"Training and Testing Accuracy vs Iteration\")\n    ax[1].set_title(\"Training Data\")\n    ax[2].set_title(\"Testing Data\")    \n\n    plot_decision_boundary(model, X_train, y_train, ax = ax[1], highlight=True)  \n    plot_decision_boundary(model, X_test, y_test, ax = ax[2], highlight=False)   \n    plt.tight_layout()\n\n\nNow let’s plot our findings.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\nplot_experiment(KR)\n\n\n\n\n\n\n\n\nFascinating. As we can see in the first plot, our accuracy begins relatively high for both our testing and training data, but decreases on the testing data as more iterations occur. This makes sense when looking at the decision regions. We have highly localized decision regions built around the training data, that does not fit the variation of the testing data.\n\n\naccuracy = (KR.predict(X_train) == y_train).float().mean()\nprint(f\"Train accuracy: {accuracy:.2f}\")\n\nTrain accuracy: 0.94\n\n\nThe result of this is that we have a very high training accuracy, but this is not a good thing.\n\naccuracy = (KR.predict(X_test) == y_test).float().mean()\nprint(f\"Test accuracy: {accuracy:.2f}\")\n\nTest accuracy: 0.80\n\n\nOur testing accuracy is significantly worse."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#discussion",
    "href": "posts/Sparse-Kernel-Machines/index.html#discussion",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "As we have demonstrated in the blog post, using sparse kernel machines gives significantly more control over how our models are fit. With control of the bandwidth (gamma), or the relationship between the distance between two points and the similarity of them, and lambda, the regularization of the weights, we can get very high accuracy when training models. This can even be done on non-linearly separable data. But this comes with risks. If we fit the data too much on the training data, then variation in training and testing data will result in overfitting. This was demonstrated in Part B, where we showed that a highly localized and non-normalized training of non linear data with a decent amount of noise will result in a poor performance when tresting our model.\nAll of this blog post demonstrates the importance of cross validation for the selection of parameters during training in order to prevent overfitting. But cross validation requires more time, energy, and computing power."
  },
  {
    "objectID": "posts/Final-Project/main.html",
    "href": "posts/Final-Project/main.html",
    "title": "Evolution Based Weight Vector Optimization",
    "section": "",
    "text": "Abstract\nThis blog post explores the application of the principals of evolution to weight vector optimization problems. A comprehensive evolutionary optimizer class, with hyperparameters allowing for control over selection, inheritance, diversity, mutations rates, and mutation styles, was created. Exploratory experiments were then run to attempt to understand the strengths and limitations of the various values for each of these hyperparameters. Experiments were performed on generated data as well as the MNIST dataset. Due to computation limitations, a complete optimization loop on the MNIST dataset was out of scope for this project. However, a final accuracy of 82% was achieved.\n\n\nIntroduction\nThis blog post explores how features of evolution in nature can inspire solutions to overcoming the shortcomings of gradient descent. Gradient Descent only works on differentiable loss functions, meaning it can become stuck in local loss minima when attempting to model non-convex loss functions. In other words, gradient descent cannot explore the entire solution space on nondifferentiable loss functions. This limitation can be overcome by harnessing the characteristics of evolution and natural selection in nature. Evolution has a wide variety of applications concerning Machine Learning, but this project focuses on its applications to weight vector optimization Telikani et al. (2021).\nLewontin identifies 3 key population characteristics for evolution: phenotypic variation in a population, differential fitness, and fitness must be heritable Lewontin (1970). With these 3 characteristics, evolution then occurs as ‘fitter’ individuals are better able to pass on their traits to future generations, while less fit individuals are not. At the individual level, evolution requires a blueprint, self-replication, mutation, and selection. By applying these principles to machine learning models, this blog post explores the strengths and limitations of evolutionary principles when applied to weight vector optimization in machine learning. To satisfy the requirement of phenotypic variation, each evolutionary optimizer has an entire population of weight vectors storing different weights. The different weights result in different losses, which in combination with selection pressures regarding the resulting different losses, satisfy the differential fitness requirement. With weight vectors serving as our genetic blueprint, those weight vectors can be duplicated to create or refill the population of weight vectors. Slight random adjustments to those weight vectors during replication serve as the mutations, ensuring the continuation of phenotypic variation. A variety of methods can be used to eliminate population vectors during an iteration, including loss and diversity, which function as selection. Eliminating high-loss weight vectors allows only vectors with high accuracy to pass on their characteristics, while eliminating low diversity can ensure that the solution space is adequately explored. Through the implementation of hyperparameters, many variations of evolutionary machine learning algorithms are explored to better understand their strengths and weaknesses.\nThe many hyperparameters are then tested on both generated and real data from the MNIST dataset to develop initial hypotheses regarding the optimal parameterization for evolutionary weight vector optimization to succeed.\n\n\nValues Statement\nThe potential users of the evolutionary-based weight vector optimization class are researchers, data scientists, and developers, especially those who work on non-differentiable problems with which gradient descent-based solutions struggle. Our class provides both a potential solution to overcoming the limitations of gradient descent on non-differentiable classification problems and serves as a potential benchmark against which other algorithms can be compared.\nOne major potential impact of the widespread use of our algorithm, or similar ones, is the increase in computational power required to run them. Because each epoch of an evolutionary algorithm requires the computation of an entire population of new weight vectors, the computational power required for an epoch is higher than most algorithms. This has potential positive implications for the manufacturers of computational chips and the owners of servers. On the other hand, the potential negative effects of increased energy and material consumption to perform these computations cannot be overlooked either.\nBecause the majority of our work was focused on the creation of a class, and not the optimization of a specific algorithm, the potential for positive and negative impacts of our class depends on who gains access to the class and what they decide to do with it.\n\n\nMaterials and Methods\nProject dependencies: - torch - numpy - pandas - scikit-learn - matplotlib\nOur evolutionary optimizer translates biological principles into a deep neural network optimiser. Biological evolution, and thus our algorithmic approach, rely on four core attributes: blueprint, self-replication, mutation, and selection. Our “blueprints,” genes or DNA in the natural world, are our weight vectors for the parameters of the neural network. We begin with an initial “population” of \\(N\\) such vectors that are sampled uniformly at random. In each generation, every individual is evaluated on a mini-batch of examples, combining cross-entropy loss (exploitation) with an optional diversity penalty (exploration). The lowest‐loss individuals (and occasionally a small “sneaker” fraction of high-loss outliers) serve as parents for the next generation. Some elite low-loss survivors carry forward unchanged. New offspring are created via uniform crossover, where each weight entry, or gene, is inherited from \\(k\\) randomly chosen parents, then mutated by adding small Gaussian or Laplacian noise with some probability. Optionally, each child can receive a single gradient‐descent step to fine-tune its accuracy. Initially, we relied on synthetic binary-classification data generated using torch.rand to train and validate our evolutionary approach. This allowed us to develop a proof of concept that evolution could, in fact, solve problems and that our selection, crossover, mutation, etc., behaved as expected before we moved on to real-world inputs.\n\n\nExample of MNIST Digits\n\nWe decided to employ our evolutionary approach to the MNIST handwritten-digit dataset LeCun, Cortes, and Burges (2010). This dataset is made up of 70,000 gray-scale images of size 28×28 pixels, labeled 0–9. We accessed the dataset through torch datasets. In smaller experiments with the MNIST dataset, we opted to draw a random subset of anywhere from 1,000 to 20,000 digits to improve computational efficiency. Although the smaller subsets enabled rapid prototyping, they may have overrepresented certain rarer handwriting styles and potentially skewed accuracy.\n\n\nHyperparameters\n\nProof of concept/vanilla evolution:\n\n\nSelection Processes:\nIn nature, genetic traits are passed from one generation to the next by individuals that survive and successfully reproduce. These survivors make up the gene pool of their generation, while those that fail to reproduce are effectively excluded from the evolutionary process. In our implementation, we emulate this principle by defining fitness based on standard cross-entropy loss or diversity-augmented loss, depending on the user. At each generation, we sort the population in a minheap based on loss, then the top 50% (the half with the lowest loss) are selected to form the gene pool. The other half does not have the chance to reproduce. In the next section, we will dive into how we handle creating the next generation from the gene pool.\n\n\nIllustration of Gene Pool Selection\n\nMirroring the random nature of evolution, we incorporate some chance in the makeup of our gene pool. A small number of lower-performing individuals (10% by default) are included in the gene pool with low probability. These individuals, whom we call sneakers, introduce genetic variation that helps maintain a diversified population and prevents premature convergence.\n\n\nIllustration of Sneaker Population\n\nFinally, we employ an elitist strategy to preserve our high-performing solutions. Each generation, a percentage of the top performers based purely on cross-entropy loss are included in the gene pool and also survive unchanged and unmutated to the next generation. This preserves the integrity of the best solutions by keeping a lineage of high-performing individuals.\n\n\nOverview of New Generation Gene Makeup\n\n\n\nInheritance and Parent Quantity:\nAt each iteration of our evolutionary optimization, following the creation of a ‘gene pool’ in the selection stage, the population must be replenished with new individuals. There are three ways that this can be accomplished. 1: All new individuals are new randomized weight vectors with no input from the gene pool. 2: Each new individual has a single parent randomly selected from the gene pool from which its weights are inherited with random mutations. 3: Each individual has n parents randomly selected from the gene pool. Each feature weight is then inherited from the corresponding feature weight of a random one of its parents.\nThe first scenario, with no inherited weight vectors, is a baseline against which our true evolutionary models can be tested. This is not truly evolution, as it does not include any heritability of fitness for new individuals in the population Lewontin (1970).\nThe second Scenario, includes heritability of fitness, but with only a single parent for each child individual, the diversity can be expected to be more limited.\n\n\nDiagram of Inheritance when num_parents = 1\n\nThe Third Scenario, allows for a slightly reduced heritability of fitness, with the addition of diverse new individuals produced with each generation. The diversity rate is specifically limitted by the mutation_rate and mutation_intensity hyperparameters.\n\n\nDiagram of Inheritance when num_parents = 2\n\nFunctionally, this process occurs after selection has occured, and an overall gene pool of parents has been created. A random sampling with replacement is then performed on that gene pool, in which each new child is assigned n, the value of the num_parents hyperparameter passed to the function, parents from the gene pool. For each weight in each child’s weight vector, a random one of that child’s n parents is then chosen from which it inherits that specific weight. If num_parents = 0, then every child recieves a completely random weight vector. Once the weights have been assigned, the child weight vector is then mutated.\nAs discussed in the results section, the choice of the number of parents can have a significant impact on loss, accuracy, and diversity.\n\n\nHybridizing evolution with gradient descent:\nOur approach to evolutionary optimization incorporates a gradient-based refinement step that allows individuals to local optimize their performance (slightly) after being created. In essence, this hybrid evolutionary-gradient approach combines the global search strengths of evolutionary algorithms with the precise, local updates enabled by backpropagation. For each new individual generated during the evolutionary step, we apply a single gradient update to refine its weights. This is accomplished using a method implemented within the model that performs a forward pass, calculates cross-entropy, and uses PyTorch’s automatic differentiation to compute gradients. Weights are then updated according to the direction of the negative gradient, and scaled by a learning rate set to 0.5.\nThe backpropagation step is called once per individual in the evolutionary loop, immediately after crossover and mutation have produced a new weight vector candidate. The updated individual is then re-inserted into the population. By integrating this light update of gradient descent, the optimizer benefits from enhancing convergence rates - while still being able to prioritize diversity - with fewer generations of evolution.\n\n\nComputing Diversity and Diversity-Based Loss:\nOur evolutionary optimization implementation includes a mechanism for encouraging population diversity by directly incorporating a diversity term into the model’s loss function. Diversity is measured over the entire population of weight vectors, with four distinct methods implemented to quantify it. These include Euclidean distance, cosine dissimilarity, standard deviation, and variance. The Euclidean distance metric calculates the mean spatial difference between every pair of individuals in the population. Cosine dissimilarity measures the angular dissimilarity of weight vectors by computing one mines the cosine similarity between weight vectors. The standard deviation and variance metrics, on the other hand, operate across the whole population of weight vectors by computing the average distribution/variance of all weight vectors within a generation.\nOnce computed, the diversity score is used to modify the model’s loss. Specifically, during each evaluation of an individual weight vector in the population, the standard cross-entropy loss is calculated and then a diversity term is subtracted from it. This diversity term equals the above mentioned diversity value scaled by a user-set diversity coefficient. The effect of this subtraction is that models with higher diversity scores receive a lower total loss, incentivizing the optimizer to explore a broader range of solutions. This diversity-aware loss is only applied when explicitly enabled through a boolean flag in the model, giving flexibility for experiments that compare/evaluate the performance of diversity-based and non-diversity based evolutionary optimization.\n\n\nAdjustment from binary to multi-class classification:\nBecause our target task is classification on the MNIST dataset - which involves 10 possible output classes (digits 0 through 9), we implemented multiclass classification using Pytorch’s CrossEntropyLoss function. Unlike binary cross-entropy, which assumes a binary classification problem and compares scalar outputs to binary labels, cross-entropy loss compares a vector of probabilities (logits) against a single target value label. This function internally applies a softmax operation which evaluates the likelihood of each logit being the right output class.\nIn our implementation, the CrossEntropyLoss function is used in both the model’s forward loss evaluation and backpropagation step. This ensures that each prediction is treated as a multiclass decision and that the model can properly learn to distinguish between all 10 classes in the MNIST dataset.\n\n\nMutation Methods:\nOne thing that we created in our vanilla EVO optimizer was a random mutation mechanism. This mutation mechanism let’s us assign a small probability that each of the weight’s entries’ values can be nudged by a certain amount positively or negatively. This nudge and its intensity is modeled by a normal distribution around the current value, and what we call “Mutation Intensity” is the standard deviation of that normal distribution. This ensures that we are constantly updating our weights randomly, and that there is a chance for weights to get better. What we noticed is that only using normal distribution might not be sufficient in achieving fast convergence. Because the normal distribution’s tails flatten with the X-axis quickly, it does not give the slightest opportunity for the model to get an aggressive nudge.\n\nThis led us to explore different distributions that also share the characteristic that ensures the nudge is usually not too aggressive, but also allows ever so rarely for it to change the weight’s entry significantly. This distribution that we introduced is the Laplacian distribution.\n\nThis became another hyperparameter that allows us to see how different mutation methods affect different models that our EVO optimizer tries to solve.\n\n\n\nResults\n\nChoices in Selection Processes:\nBy default, we select the best 50% of the population to enter the gene pool, however, this is a hyperparameter that users can play with. We conducted some experiments on a small subset of 1000 digits from the MNIST dataset to examine how different gene pool sizes (10%–90% of the population) would affect our accuracy, loss, and diversity over 500 generations.\n\n\nGene Pool Size vs Accuracy\n\n\n\nGene Pool Size vs Loss\n\n\n\nGene Pool Size vs Diversity\n\nThere are several interesting things to note about these figures. Focusing on the extremes first, only picking 10% of the best individuals is advantageous if we look purely at accuracy. However, this came at the cost of significantly reducing diversity, with such a small portion of the population passing through at each generation. Having too homogeneous a population can lead to getting stuck in local minima without exploring the wider loss landscape. On the other hand, having too many members of the population reproduce increases exploration of the loss landscape, but reduces selection pressure, as individuals with suboptimal solutions continue to reproduce. We can see this illustrated above as the accuracy lags far behind all of the other gene pool sizes. Keeping the best half performed is a strong middle ground with comparatively great accuracy, second only to keeping the top 10%, while remaining diverse.\nWe also investigated the effects of varying the probability that “sneakers”—individuals from the bottom 10% of the population—could enter the gene pool. We tested probabilities from 0–45%.\n\n\nSneaker Probability vs Accuracy\n\n\n\nSneaker Probability vs Loss\n\n\n\nSneaker Probability vs Diversity\n\nInterestingly, across a range of sneaker probabilities, we didn’t observe much variation in loss or diversity. So it doesn’t impact our learning dynamics to a noticeable degree. However, having a 45% sneaker probability performed quite well, accuracy-wise. This may be a reflection of random variation of our dataset or starting genepool, but it may also suggest that a degree of genetic noise can occasionally help guide the population out of local minima. In future experiments, it would be insightful to set the hyperparameter to be above 50% and see the results.\nFinally, we explored the impacts of elitism by varying the percentage of top-performing individuals who we carried unchanged to the next generation.\n\n\nElitist Population Size vs Accuracy\n\n\n\nElitist Population Size vs Loss\n\n\n\nElitist Population Size vs Diversity\n\nWhen we have too many elites, we slow down evolutionary convergence. We aren’t introducing enough change from generation to generation to explore the landscape and improve our solution. We can see evidence of this in our stunted accuracy, low diversity, and higher loss when we increase the size of our elite population. However, when we eliminate elites or keep only 5%, we see noticeable improvements. Our loss is converging faster, we maintain a diverse population, and our accuracies after 500 generations are the highest. Keeping the elite population helps our accuracy, outperforming the population without elites by over 5% over 500 generations. Overall, we observed that on MNIST, modest elitism provides a valuable balance between preserving high-quality solutions and allowing diversity within the population.\n\n\nInheritance and Parent Quantity:\nTwo experiments were performed to explore the limitations and strengths of different inheritance methods, specifically the adjustment in the number of parents from which each child weight vector receives it’s own weight vector values.\nFor both experiments below hyperparameters that were held constant were:\n-Survivor Ratio: 0.1\n-Fitness Ratio: 0.5\n-Sneaker Probability: 0.01\n-Sneakers ratio: 0.1\n-mutation rate: 0.05\n-mutation intensity: 0.05\n\nGenerated Data Experiment:\nThis generated data experiment explores the performance of our model when varying the num_parents hyperparameter. A multi parent classification experiment was run on generated 2 dimensional data with 0.2 noise and 300 points. The accuracy, loss, and Euclidean diversity was tracked across 300 iterations. The experiment was run with the hyperparameter num_parents set to 0, 1, 2, 3, 5, and 10.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on Generated data using 0, 1, 2, 3, 5, and 10 parents over 300 iterations\n\nAs seen in the above visualization, Loss and Accuracy were comparable across all quantities of parents, while diversity varied significantly. In particular, with num_parents set to 0 and to a lesser extent 1, diversity was much lower than all other quantities of parents. The accuracy of the 0 parent model also performed worse than the other models over more iterations. With 0 parents evolution is conceptually replaced by random chance. The heritability, defined as a requirement for evolution by Lewontin (1970), is eliminated from the process.\nWhile this had a much smaller impact on this relatively simple experiment of generated data, the implications on a much more complex classification problem, such as MNIST, could be significant.\n\n\nMNIST Experiment:\nA similar, more complex experiment performed on a subset 1000 images from the MNIST dataset tested the accuracy, loss, and diversity of num_parents = 0, 1, 2, 5, 10 over 1000 iterations. As a significantly more complex classification problem, the strengths of including more parents become much clearer.\n\n\n\nFigures demonstrating loss, diversity, and accuracy performance of Evolutionary Optimization on a Subset of the MNIST dataset using 0, 1, 2, 5, and 10 parents over 1,000 iterations\n\nThe benefits of inheritance are clear, as the zero parent model has a significantly higher loss and lower accuracy throughout the 1000 iterations compared to all other models.\nStarting with loss, we can see that the loss for all test groups are relatively similar with the exception of num_parents = 0.\nWith regards to accuracy, we see a more nuanced picture. 0 parents performs poorly throughout the experiment, never reaching 21% accuracy. 1 parent has logarithmic like improvement in accuracy at around 35%. All higher quantities of parents follow a similar trajectory, but with major jumps in accuracy breaking the logarithmic like trend. This can be better understood by looking at diversity levels.\nDue to the random nature of the weight vectors for the 0-parent group, the diversity is constant and extremely high at 65.124. For all other groups, it is clearly shown that more parents results in maintained diversity. As selection occurs, and the population is replenished with weight vectors inherited from the gene pool, diversity decreases overall. But by allowing for more varied combinations from that gene pool, some level of diversity is preserved. This appears to have diminishing benefits as demonstrated by the similar diversity for both 5 and 10 parents.\nThis becomes a problem of optimizing the heritability of fitness and the phenotypic variation mentioned by Lewontin (1970). fewer parents means more pure inheritance of fitness, as the child will more closely resemble its parents. It also means less phenotypic diversity, as completely new weight vectors are less likely to emerge. The opposite with regards to both phenotypic variation and fitness heritability applies. The benefits of multi-parent inheritance are demonstrated by the declining improvement in accuracy when num_parents = 1. The lower diversity compared to the other models, and the importance of diversity in evolutionary algorithms in allowing for the exploration of the solution space, results in poorer performance of the single parent model. A single parent allows for the inheritance of fitness,leading to better performance compared to the 0 parent model Lewontin (1970). However, it does not allow for large enough variation in fitness. With lower diversity, the 1 parent model is less likely to find a global minimum compared to the 2+ parent models. While it does find some form of a local minimum, the lack of diversity results in a drop off in improvement at around 600 iterations, while the models with 2, 5, and 10 parents continue to have spikes in improvement.\nIn the context of classification of the MNIST dataset, evolutionary models benefit from the added diversity resulting from the use of larger quantities of parents contributing weights to each new child in the subsequent generation. While more computing power, and more iterations are required to truly optimize this hyperparameter, these experiments clearly demonstrate the benefits of multi-parent inheritance.\n\n\n\nQuantifying Diversity and Diversity-Based Loss:\nThis section evaluates the effect of diversity-aware loss functions in evolutionary training of a neural network classifier on a subset of the MNIST handwritten dataset. We experimented with four diversity metrics - Euclidean Distance, Cosine Dissimilarity, Standard Deviation (STD) and variance - and measured their influence on test accuracy, cross-entropy loss, and diversity levels in our weight population over 200 generations. Additional hyperparameter tuning was performed for the Cosine diversity metric to explore how mutation rate, mutation intensity, population size, and diversity coefficient influence outcomes.\nThe first experiment (figure 1) compared the test accuracy, loss, and normalized diversity across all four diversity metrics under a fixed training setup. All metrics enabled the model to reach between 75%-81% accuracy over 200 generations, with all other hyperparameters held constant. euclidean distance and STD slightly outperformed others in final diversity. All methods reduced loss substantially within 60 generations. When it came to Normalized diversity, all metrics except for, interestingly, cosine dissimilarity between weight vectors increased/maintained high diversity over time. Cosine dissimilarity diversity rapidly decayed to near-zero within 100 generations, while STD, variance and euclidean distance maintained high diversity levels, suggesting that cosine may be more prone to premature convergence or intrinsically mediates the impact of diverse weight populations.\n\nFigure 1: Comparing test accuracy, loss, and normalized diversity values for all 4 diversity metrics.\nTo better understand the behavior of the cosine dissimilarity metric, we ran additional training with varied diversity coefficients, population sizes, and mutation hyperparameters. The default hyperparameters used were population size 50, mutation rate 0.4, mutation intensity 0.5 and diversity coefficient 0.1. Increasing the diversity coefficient to 0.3 (figure 4) significantly improved diversity values - up to 0.2 - over each generation, confirming that the penalty term has a regulating effect on population diversity. When the diversity coefficient was set to 0.0 (figure 3), the model still trained to reasonable accuracy but showed completely flat diversity values, indicating the diversity term is implemented correctly to at least affect our metric value. Increasing population size to 100 (figure 5) improved diversity over each generation, especially in the first 100 generations, but did not substantially improve test accuracy. This suggests diminishing returns from larger populations in this setting. Raising mutation rate to 0.7 and intensity to 0.8 (figure 6) had a negligible to slightly positive impact on accuracy while maintaining diversity at moderate levels. Accuracy did experience more noisiness under these conditions, but ultimately achieved reasonable levels.\n\nFigure 2: Baseline experiment outputs to provide reference test accuracy, loss, and diversity values for cosine driven loss.\n\nFigure 3: Confirming working implementation of diversity coefficient’s effect on diversity based loss by setting diversity coefficient to 0.0\n\nFigure 4: Results showing the effects of increased diversity coefficient of 0.3 - i.e. higher effect of diversity punishment/reward on loss - on test accuracy, loss, and diversity values.\n\nFigure 5: Results for increased population size of 100 weight vectors on test accuracy, loss, and diversity values.\n\nFigure 6: Results for impact of high mutation rate and mutation intensity on test accuracy, loss, and diversity values.\nIn summary, all four diversity metrics led to successful convergence and comparable final test accuracies, with euclidean distance and STD slightly ahead. Cosine dissimilarity driven diversity tends to descend quickly, requiring further parameter tuning to explore what it takes to keep diversity high. Enabling the diversity penalty to loss had a clear and measurable effect on both training behavior and final diversity levels, validating its implementation. Mutation and population hyperparameters affected convergence stability and final accuracy but had less influence than the choice of diversity metric.\nThis study was constrained by computational limitations, which restricted the breadth of hyperparameter combinations we could explore. In particular, both the population size and the number of generations were limited in order to keep training time feasible. Larger populations and longer training schedules could potentially yield more robust insights into the effects of diversity-aware loss function. Further investigation into the behavior of cosine dissimilarity is warranted. Across multiple experiments we observed a consistent decline in diversity when using this metric. One possible explanation for this is that cosine dissimilarity only measures angular differences between vectors, ignoring their magnitudes. As a result, the population may converge to a set of similarly oriented but differently scaled vectors, which could be interpreted as low diversity by this metric. This limitation could implicitly constrain the optimizer’s ability to maintain variation during training, and future work could test this hypothesis more directly or explore hybrid metrics that include both angular and magnitude components. Additionally, we were limited in the size of training and test batches, which may influence generalization performance. It would be valuable to evaluate how increasing batch size or dataset subset size impact both diversity value and resulting model accuracy. Please note, all of these experiementes were run on a hybridized version of the evolution optimized DNNs which included, for every step of training one gradient descent step on each weight vector. This was done in hopes to reduce runtimes without straying too far from pure evolution. Pure evolution, we speculated, would have needed to require high data inputs, generation numbers, and population sizes to produce valuable results, which did not fit the computational capacities of our computers, nor our time constraints.\n\n\nFinal MNIST Results:\nAfter combining all of our implementations together, we trained a deep neural network with layers [764,32,10] to classify our MNIST dataset. We settled on the following hyperparameters:\nmodel.diversity_coeff = 0.2 optimizer = EvolutionOptimizer(model) optimizer.set_population_size(200) optimizer.use_backprop = False optimizer.set_survivors_ratio(0.1) optimizer.set_fitness_ratio(0.5) optimizer.set_sneaker_prob(0) optimizer.set_mutation_intensity(0.05) optimizer.mutation_rate = 0.05\n\nThe plot above shows that the accuracy rapidly increases during the early generations, indicating that the evolutionary algorithm quickly identifies promising weight vectors, significantly reducing the initial error. This is likely because the selection criteria is too strict so it rapidly eliminates poorly performing members of the population. The curve starts to smooth out, reflecting a deceleration in accuracy improvement as the optimizer converges on better solutions. After around 2000 generations, the accuracy curve stabilizes, indicating that the optimizer has reached a near-optimal solution for the given problem. The final accuracy appears to stabilize around 82%, suggesting that the current hyperparameter settings and genetic operators are effective but may have room for further optimization, possibly through adjustments to mutation rates, diversity coefficients, or parent selection mechanisms.\n\n\n\nConcluding Discussion:\nAs we combined all of our implementations of various evolutionary components, and created our unified optimizer that has significant flexibility in deciding the environment where our weights can evolve to solve the ML problems over time, we were ready to test this optimizer at a famous problem for deep neural networks: Classifying handwritten digits. Tweaking many of our hyperparameters lead to significantly different converging speeds, diversity metrics, and overall performance. This flexibility can be helpful in tailoring our algorithm to work in different context and on different models.\nOur intention from the beginning was to simulate how living creatures solve the problem of survival: Evolution. What encouraged us to explore this algorithm is the beauty of how living beings have evolved to solve the same problems very differently. This diversity that exist in nature is what got us thinking about ways we could achieve this concept in optimizing machine learning models on different datasets. If we can create a population of weights that can reproduce over time, and spread their genes and cross it with one another, what can we notice about the diversity of their solutions? This took us on a journey of simulating this natural process, abstracting it into simpler components, and specifying it to our context.\nOur project worked in many ways: our EVO optimizer managed to get the population to converge through random mutation, and maintain diversity by adjusting the diversity coefficient hyperparameter. We did however see a natural decay in diversity as the exploration phase ends and the exploitation phase begins where the population begin to converge around good solutions it found in the initial phase. Beyond the scope of the optimizer, our project worked in a sense that it provided us with the opportunity to investigate, design, and implement a complex environment for evolution. This has been the project that taught me at least the most about object oriented programming, and has taught us a lot about how to write legible code that will be built on by others.\nOur results are comparable to other evolutionary optimization algorithms in terms of convergence speed and diversity emphasis, however, different implementations have allowed for even more complex design decisions of the environment, more complex selection criteria (like tournament style), adaptive mutation rate, limitations on the mating process. These added complexity unlocks many different combinations of hyperparameters that outperform our simple-er implementation.\nIf we had more time, we would definitely work on improving speed. Currently, our code does not fully utilize GPU. There are a few python for-loops when popping our populations based on total loss and cross entropy loss. These are operations that, when vectorized, could speed up the training process significantly. In addition, we can add more design options for more complex evolutionary algorithms. Also, we would perform a grid search to find the best hyperparameters that would optimize for a deep neural network for handwritten MNIST dataset in terms of accuracy and diversity. Finally, we would implement an inference mechanism that would classify data using majority voting, assuming that the diversity in the population allows for a broader knowledge base to solve the problem, i.e, it would be interesting to see if the phenomenon of the wisdom of the crowd emerges under our current evolutionary algorithm within a population.\n\n\nGroup Contributions:\n\nLukka:\nAs a unit, the whole team contributed to the conceptualization and the early stages of building a working prototype. Lukka worked mainly on implementing, refining, and exploring the selection mechanisms in our evolutionary model. he also helped integrate the Laplacian mutation distribution. Lukka also helped include and streamline my work and the work of others into a central working file. This was work that helped build the base of how we would handle our object-oriented programming approach and handle tuning hyperparameters. He also spent considerable effort and time getting MNIST to run correctly on the Middlebury cluster to facilitate larger-scale experimentation. In all the team meetings, we all spent time digging into one another’s code, learning and helping on implementation, debugging, and developing conceptual frameworks.\n\n\nJiffy:\nFor this project, Jiffy contributed to both the conceptual development and the technical implementation of our evolutionary optimization framework. Early in the project, je created a demo notebook (evolution_demo_warmup.ipynb) that introduced the basic principles of evolutionary algorithms using synthetic data, aiming to outline a clear conceptual framework of evolution’s purpose and potential in our project. Jiffy was primarily responsible for implementing and optimizing the diversity-aware loss framework, including vectorized versions of the Euclidean distance and cosine dissimilarity metrics, as well as additional metrics based on standard deviation and variance. He added support for toggling these metrics and integrating them into the final loss calculation. Jiffy also extended our codebase to support multiclass classification, enabling us to apply our models to the MNIST dataset. Much of Jiffy’s experimentation involved running classification trials with varying diversity metrics and hyperparameters - mutation rate, intensity, and diversity coefficient - which he documented in a jupyter notebook (ExploringDiversity.ipynb). Jiffy wrote an initial training logic and data loading code for MNIST, and developed visualization tools using matplotlib to track accuracy, loss, and diversity across generations. He also implemented the hybrid optimization step, which combines evolution with gradient descent via backpropagation. For the final blog post, Jiffy focused on writing detailed technical explanations of the algorithmic components I implemented, along with reporting and analyzing the results of my experiments. This includes the materials/methods section on hybridizing evolution with gradient descent, computing diversity and how diversity-based loss was implemented, and the transition from binary to multiclass classification. It also includes the results writeup for ‘Quantifying Diversity and Diversity-based Loss’.\n\n\nJames:\nJames’ main contribution to this project was the creation, implementation, and experimentation on the benefits and limitations of adjusting the inheritance process, in terms of how many parents each child’s weight vector has. This included identifying scholarly sources which provided a framework for understanding how each change to our hyperparameters, and in the case of inheritance specifically, in the number of parents influences the forces of evolution Lewontin (1970). The majority his work is found in the multi-parent and multi-parent2 folders, although the important changes to the evo class were eventually merged with the blog-post EVO class. While the most important contributions can be found in /multi-parent2/MultiParent.ipynb, James spent considerable time working to overcome computational limitations of my computer and then working to have my code run on ADA. James wrote the abstract, the introduction, the values statement, and the Inheritance and Parent Quantity subsections of the Hyperparameter and results sections.\n\n\nYahya:\nFor this project, I contributed to both the conceptual foundation and technical implementation of our evolutionary optimization framework. Early in the project, I helped conceptualize and explain various mathematical approaches to designing the optimizer, providing the initial direction for our implementations. I also supplied resources to support my teammates in understanding the theoretical aspects of evolutionary algorithms. I implemented a proof-of-concept version of the optimizer, integrating basic mutation, crossover, and selection mechanisms, which served as the foundation for our more complex final implementation. This included designing and implementing the core structure of the optimizer, which became the base for further development. I also investigated different diversity metrics and incorporated them as terms in the loss function to maintain diverse populations and reduce the risk of premature convergence. I designed and refined the OOP structure throughout the project. Additionally, I designed the mutation mechanism and introduced a Laplacian distribution as an alternative to Gaussian mutation, allowing for a more varied exploration of the solution space. To address computational bottlenecks, I collaborated with Professor Vaccari to set up scripts for running Jupyter servers on Middlebury’s GPU nodes, resolving runtime issues for the team. Finally, I helped assemble the final version of the EVO optimizer and experimented with different hyperparameter combinations to fine-tune the model and achieve the results presented in our report.\n\n\n\n\n\n\n\n\n\nPersonal Reflection:\n\nLeCun, Yann, Corinna Cortes, and CJ Burges. 2010. “MNIST Handwritten Digit Database.” ATT Labs [Online]. Available: Http://Yann.lecun.com/Exdb/Mnist 2.\n\n\nLewontin, R. C. 1970. “The Units of Selection.” Annual Review of Ecology and Systematics 1: 1–18. http://www.jstor.org/stable/2096764.\n\n\nTelikani, Akbar, Amirhessam Tahmassebi, Wolfgang Banzhaf, and Amir H. Gandomi. 2021. “Evolutionary Machine Learning: A Survey.” ACM Comput. Surv. 54 (8). https://doi.org/10.1145/3467477."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html",
    "href": "posts/Logistic-Regression/index.html",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "This blog post explores the Logistic Regression Machine Learning model. By working through vanilla gradient descent, Gradient Descent with momentum, and then overfitting. These concepts are then applied to the Wisconsin Brest cancer dataset in order to practice building a machine learning pipeline without overfitting while optimizing learning and momentum rates.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nimport torch\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nUse this link to access the Logistic Regression source code.\n\n\n\nLet’s generate some data with a minimal amount of noise.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Classification Data\")\n\nText(0.5, 0.98, 'Classification Data')\n\n\n\n\n\n\n\n\n\nMake a function to plot decision regions:\n\ndef plot_decision_boundary(model, X, y, ax=None, highlight=False):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    grid = torch.stack([X1.ravel(), X2.ravel()], dim=1)\n\n    # Append column of ones to match model input shape (for intercept)\n    grid = torch.cat([grid, torch.ones(grid.shape[0], 1)], dim=1)\n\n    preds = model.score(grid).reshape(X1.shape)\n\n    # Decision boundary plot\n    ax.contourf(X1, X2, preds, levels=50, cmap=\"BrBG\", alpha=0.8)\n    ax.contour(X1, X2, preds, levels=[0.0], colors='black', linewidths=2)\n\n    # Plot original data\n    plot_classification_data(X, y, ax)\n\n\n\n\n\n\nFor our frist itteration, we will train our model with a learning rate (alpha) of .1, and a beta of 0. This means we are performing a logistic regression without momentum.\n\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(10000):\n    loss_vec.append(LR.loss(X, y).item())\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n\n\n\nWith our training complete, lets plot the loss over time\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\nplot_decision_boundary(LR, X, y)\n\n\n\n\n\n\n\n\nA Logistic regression, as demonstrated by the graph above, is going to be limited in the loss it can achieve by the noise within the data. There is no line that would perfectly separate this data without any feature mapping.\n\n\n\nNow let’s train our model on the same data, but this time we will use a beta of .9.\n\n# instantiate a model and an optimizer\nLR_momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\n# for keeping track of loss values\nloss_vec_momentum = []\n\nfor _ in range(10000):\n    loss_vec_momentum.append(LR_momentum.loss(X, y).item())\n    opt_momentum.step(X, y, alpha = 0.1, beta = 0.9)\n\nWith the model trained, let’s plot the two loss vectors on the same plot to compare.\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\nplt.xlim(0, 100)\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see from the above graph, with momentum our model reaches the minimum loss (Variance/noise of the date) much faster. In-fact, within 100 iterations, the without momentum model does not reach the variance of the data.(minimum loss)\nHowever, if given enough time, both models will converge on the minimum loss:\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to generate a new set of data that. Let’s generate 2 sets of data with 2 times as many p_dims as n_points\n\n\nX_train, y_train = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\nX_test, y_test = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\n\n\nWe will now use our X_train and y_train data to train a model to 100% accuracy in order to demonstrate the concept of overfitting.\n\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\n\n\nloss_vec_train = []\nfor _ in range(100):\n    loss = LR_overfit.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt_overfit.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\nprint(\"Final training loss: \", loss_vec_train[-1])\n\nFinal training loss:  0.0025839933659881353\n\n\nWith our Gradient Descent with Momentum optimization complete, let’s look at how the loss changed over time:\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"blue\")\nplt.title(\"Loss overfitting on training data\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see, our loss approached 0 within a very few number of gradient descent iterations. Now let’s test our accuracy:\n\ntrain_acc = ((LR_overfit.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Training accuracy: \", train_acc)\n\nTraining accuracy:  1.0\n\n\nGreat, our training accuracy is 100% as expected! Let’s test the accuracy on the test data.\n\ntest_acc = ((LR_overfit.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy: \", test_acc)\n\nTest accuracy:  0.8799999952316284\n\n\nHmmm, so it seems that we have over-fit our model quite considerably. Our test accuracy is ~.16 worse than our training accuracy of 1.\n\n\n\n\nThe data from load_breast_cancer dataset is from the Wisconsin (diagnostic) breast cancer dataset. The features in this data were calculated from imaging of breast masses, and describe characteristics of the cell in the image.\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\nThis dataset has a unique identifier (id), 28 anonymized features about the transaction, the transaction amount, and the class. The class is 1 if the transaction is fraudulent and 0 if not.\n\n\nNow we split our data into Training, Validation, and Testing datasets using train_test_split frm sklearn. Data is split 60%, 20%, 20% respectively:\n\nfrom sklearn.model_selection import train_test_split\n# split train and test .6 and .4\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# splite temp into validation and test sets at .5 and .5\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nIn order to use this data on our Logistic Regression model we need to convert it into torch.Tensor’s:\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)  # or float32 for regression\n\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n\n\nX_train.size()\n\ntorch.Size([341, 30])\n\n\nNow we can train our Logisitc Regression Model on this data:\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\n\nfor _ in range(3500):\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = 0.00005, beta = 0)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, our loss after 3500 iterations with a learning rate of 0.0005 and a beta of 0 levels out around a loss of 1.\nLet’s see how it performs with momentum:\n\nLRm = LogisticRegression()\nopt_m = GradientDescentOptimizer(LRm)\nloss_vec_train_m = []\n\nfor _ in range(3500):\n    loss = LRm.loss(X_train, y_train).item()\n    loss_vec_train_m.append(loss)\n    opt_m.step(X_train, y_train, alpha = 0.00005, beta = 0.9)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train_m)+1), loss_vec_train_m, color = \"blue\")\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nplt.legend([\"With momentum\", \"Without momentum\"])\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nOur model reaches a lower loss on the training data when using momentum. Even though it does not make constant progress, it reaches a lower loss in fewer iterations. It is interesting to note how low the learning rate (alpha) had to be in order for this model to reach a good loss.\nLets test this model, with momentum on our training data:\n\ntrain_acc = ((LRm.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy with momentum: \", train_acc)\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", train_acc)\n\nTest accuracy with momentum:  0.9149560332298279\nTest accuracy without momentum:  0.8504399061203003\n\n\nAdding momentum gives us a slightly more accurate model on the training data, so we will use momentum going forward.\n\n\n\nIf we use a more robust method than above however, we can probably improve our accuracy by finding the best momentum (beta) and learning rate (alpha). We do this below by selecting samples of alphas and betas and finding the best combination.\n\nlearning_rates = [\n    1e-6, 3e-6,\n    1e-5, 3e-5,\n    1e-4, 3e-4,\n    1e-3, 3e-3,\n    1e-2, 3e-2,\n    \n]\nbetas = [\n    0.0, 0.1,\n    0.3, 0.5,\n    0.7, 0.9,\n    0.99, 0.999,\n]\n\nbest_loss = 1e10\nbest_alpha = None\nbest_beta = None\n\nfor beta in betas:\n    best_loss_per_beta = 1e10\n    best_alpha_per_beta = None\n    beta_loss_vec = []\n    for alpha in learning_rates:\n        LR = LogisticRegression()\n        opt = GradientDescentOptimizer(LR)\n        loss_vec_train = []\n        for _ in range(400):\n            loss = LR.loss(X_train, y_train).item()\n            loss_vec_train.append(loss)\n            \n            opt.step(X_train, y_train, alpha = alpha, beta = 0.9)\n        if loss_vec_train[-1] &lt; best_loss:\n            best_loss_per_beta = loss_vec_train[-1]\n            best_alpha_per_beta = alpha\n        \n    if best_loss_per_beta &lt; best_loss:\n        best_loss = best_loss_per_beta\n        best_alpha = best_alpha_per_beta\n        best_beta = beta\n      \n\n\nprint(\"Best learning rate: \", best_alpha)\nprint(\"Best beta: \", best_beta)\nprint(\"Best loss: \", best_loss)\n\nBest learning rate:  0.0003\nBest beta:  0.999\nBest loss:  0.6386049389839172\n\n\nIn validation, we are able to find our best learning rate and best beta to minimize our loss. Let’s now train our model on the training data, and we will track the loss on the validation and training data overtime so we can compare them.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\nloss_vec_val = []\nfor _ in range(400):\n    loss = LR.loss(X_train, y_train).item()\n    loss_val = LR.loss(X_val, y_val).item()\n    loss_vec_val.append(loss_val)\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = best_alpha, beta = best_beta)\n    plt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\n    plt.plot(torch.arange(1, len(loss_vec_val)+1), loss_vec_val, color = \"blue\")\nplt.title(\"Training and Validation Loss Using Gradient Descent With Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\nplt.legend([\"Training loss\", \"Validation loss\"])\n\n\n\n\n\n\n\n\nSurprisingly, our model performs better on the validation data than on the training data. Let’s check our accuracy on the validation data with our best alpha and beta values:\n\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Train accuracy: \", train_acc)\n\nval_acc = ((LR.predict(X_val) == y_val)*1.0).mean().item()\n\nprint(\"Validation accuracy: \", val_acc)\n\n\nTrain accuracy:  0.8914955854415894\nValidation accuracy:  0.9385964870452881\n\n\nAs we can see our model did even better on the validation data than on the training data. This is a great sign, and hopefully translates to our testing data as well.\n\n\n\nI think we can see how we have done on the testing data now:\n\ntest_acc = ((LR.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", test_acc)\n\nTest accuracy without momentum:  0.9736841917037964\n\n\nWe have achieved an incredibly high accuracy on our testing data! Yay."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#generate-data",
    "href": "posts/Logistic-Regression/index.html#generate-data",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "Let’s generate some data with a minimal amount of noise.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Classification Data\")\n\nText(0.5, 0.98, 'Classification Data')\n\n\n\n\n\n\n\n\n\nMake a function to plot decision regions:\n\ndef plot_decision_boundary(model, X, y, ax=None, highlight=False):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    grid = torch.stack([X1.ravel(), X2.ravel()], dim=1)\n\n    # Append column of ones to match model input shape (for intercept)\n    grid = torch.cat([grid, torch.ones(grid.shape[0], 1)], dim=1)\n\n    preds = model.score(grid).reshape(X1.shape)\n\n    # Decision boundary plot\n    ax.contourf(X1, X2, preds, levels=50, cmap=\"BrBG\", alpha=0.8)\n    ax.contour(X1, X2, preds, levels=[0.0], colors='black', linewidths=2)\n\n    # Plot original data\n    plot_classification_data(X, y, ax)"
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#train-model",
    "href": "posts/Logistic-Regression/index.html#train-model",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "For our frist itteration, we will train our model with a learning rate (alpha) of .1, and a beta of 0. This means we are performing a logistic regression without momentum.\n\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(10000):\n    loss_vec.append(LR.loss(X, y).item())\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n\n\n\nWith our training complete, lets plot the loss over time\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\nplot_decision_boundary(LR, X, y)\n\n\n\n\n\n\n\n\nA Logistic regression, as demonstrated by the graph above, is going to be limited in the loss it can achieve by the noise within the data. There is no line that would perfectly separate this data without any feature mapping.\n\n\n\nNow let’s train our model on the same data, but this time we will use a beta of .9.\n\n# instantiate a model and an optimizer\nLR_momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\n# for keeping track of loss values\nloss_vec_momentum = []\n\nfor _ in range(10000):\n    loss_vec_momentum.append(LR_momentum.loss(X, y).item())\n    opt_momentum.step(X, y, alpha = 0.1, beta = 0.9)\n\nWith the model trained, let’s plot the two loss vectors on the same plot to compare.\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\nplt.xlim(0, 100)\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see from the above graph, with momentum our model reaches the minimum loss (Variance/noise of the date) much faster. In-fact, within 100 iterations, the without momentum model does not reach the variance of the data.(minimum loss)\nHowever, if given enough time, both models will converge on the minimum loss:\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")"
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#overfitting",
    "href": "posts/Logistic-Regression/index.html#overfitting",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "Now we are going to generate a new set of data that. Let’s generate 2 sets of data with 2 times as many p_dims as n_points\n\n\nX_train, y_train = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\nX_test, y_test = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\n\n\nWe will now use our X_train and y_train data to train a model to 100% accuracy in order to demonstrate the concept of overfitting.\n\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\n\n\nloss_vec_train = []\nfor _ in range(100):\n    loss = LR_overfit.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt_overfit.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\nprint(\"Final training loss: \", loss_vec_train[-1])\n\nFinal training loss:  0.0025839933659881353\n\n\nWith our Gradient Descent with Momentum optimization complete, let’s look at how the loss changed over time:\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"blue\")\nplt.title(\"Loss overfitting on training data\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see, our loss approached 0 within a very few number of gradient descent iterations. Now let’s test our accuracy:\n\ntrain_acc = ((LR_overfit.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Training accuracy: \", train_acc)\n\nTraining accuracy:  1.0\n\n\nGreat, our training accuracy is 100% as expected! Let’s test the accuracy on the test data.\n\ntest_acc = ((LR_overfit.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy: \", test_acc)\n\nTest accuracy:  0.8799999952316284\n\n\nHmmm, so it seems that we have over-fit our model quite considerably. Our test accuracy is ~.16 worse than our training accuracy of 1."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#performance-on-empirical-data-wisconsin-breast-cancer-diagnostic",
    "href": "posts/Logistic-Regression/index.html#performance-on-empirical-data-wisconsin-breast-cancer-diagnostic",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "The data from load_breast_cancer dataset is from the Wisconsin (diagnostic) breast cancer dataset. The features in this data were calculated from imaging of breast masses, and describe characteristics of the cell in the image.\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\nThis dataset has a unique identifier (id), 28 anonymized features about the transaction, the transaction amount, and the class. The class is 1 if the transaction is fraudulent and 0 if not.\n\n\nNow we split our data into Training, Validation, and Testing datasets using train_test_split frm sklearn. Data is split 60%, 20%, 20% respectively:\n\nfrom sklearn.model_selection import train_test_split\n# split train and test .6 and .4\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# splite temp into validation and test sets at .5 and .5\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nIn order to use this data on our Logistic Regression model we need to convert it into torch.Tensor’s:\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)  # or float32 for regression\n\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n\n\nX_train.size()\n\ntorch.Size([341, 30])\n\n\nNow we can train our Logisitc Regression Model on this data:\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\n\nfor _ in range(3500):\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = 0.00005, beta = 0)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, our loss after 3500 iterations with a learning rate of 0.0005 and a beta of 0 levels out around a loss of 1.\nLet’s see how it performs with momentum:\n\nLRm = LogisticRegression()\nopt_m = GradientDescentOptimizer(LRm)\nloss_vec_train_m = []\n\nfor _ in range(3500):\n    loss = LRm.loss(X_train, y_train).item()\n    loss_vec_train_m.append(loss)\n    opt_m.step(X_train, y_train, alpha = 0.00005, beta = 0.9)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train_m)+1), loss_vec_train_m, color = \"blue\")\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nplt.legend([\"With momentum\", \"Without momentum\"])\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nOur model reaches a lower loss on the training data when using momentum. Even though it does not make constant progress, it reaches a lower loss in fewer iterations. It is interesting to note how low the learning rate (alpha) had to be in order for this model to reach a good loss.\nLets test this model, with momentum on our training data:\n\ntrain_acc = ((LRm.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy with momentum: \", train_acc)\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", train_acc)\n\nTest accuracy with momentum:  0.9149560332298279\nTest accuracy without momentum:  0.8504399061203003\n\n\nAdding momentum gives us a slightly more accurate model on the training data, so we will use momentum going forward.\n\n\n\nIf we use a more robust method than above however, we can probably improve our accuracy by finding the best momentum (beta) and learning rate (alpha). We do this below by selecting samples of alphas and betas and finding the best combination.\n\nlearning_rates = [\n    1e-6, 3e-6,\n    1e-5, 3e-5,\n    1e-4, 3e-4,\n    1e-3, 3e-3,\n    1e-2, 3e-2,\n    \n]\nbetas = [\n    0.0, 0.1,\n    0.3, 0.5,\n    0.7, 0.9,\n    0.99, 0.999,\n]\n\nbest_loss = 1e10\nbest_alpha = None\nbest_beta = None\n\nfor beta in betas:\n    best_loss_per_beta = 1e10\n    best_alpha_per_beta = None\n    beta_loss_vec = []\n    for alpha in learning_rates:\n        LR = LogisticRegression()\n        opt = GradientDescentOptimizer(LR)\n        loss_vec_train = []\n        for _ in range(400):\n            loss = LR.loss(X_train, y_train).item()\n            loss_vec_train.append(loss)\n            \n            opt.step(X_train, y_train, alpha = alpha, beta = 0.9)\n        if loss_vec_train[-1] &lt; best_loss:\n            best_loss_per_beta = loss_vec_train[-1]\n            best_alpha_per_beta = alpha\n        \n    if best_loss_per_beta &lt; best_loss:\n        best_loss = best_loss_per_beta\n        best_alpha = best_alpha_per_beta\n        best_beta = beta\n      \n\n\nprint(\"Best learning rate: \", best_alpha)\nprint(\"Best beta: \", best_beta)\nprint(\"Best loss: \", best_loss)\n\nBest learning rate:  0.0003\nBest beta:  0.999\nBest loss:  0.6386049389839172\n\n\nIn validation, we are able to find our best learning rate and best beta to minimize our loss. Let’s now train our model on the training data, and we will track the loss on the validation and training data overtime so we can compare them.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\nloss_vec_val = []\nfor _ in range(400):\n    loss = LR.loss(X_train, y_train).item()\n    loss_val = LR.loss(X_val, y_val).item()\n    loss_vec_val.append(loss_val)\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = best_alpha, beta = best_beta)\n    plt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\n    plt.plot(torch.arange(1, len(loss_vec_val)+1), loss_vec_val, color = \"blue\")\nplt.title(\"Training and Validation Loss Using Gradient Descent With Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\nplt.legend([\"Training loss\", \"Validation loss\"])\n\n\n\n\n\n\n\n\nSurprisingly, our model performs better on the validation data than on the training data. Let’s check our accuracy on the validation data with our best alpha and beta values:\n\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Train accuracy: \", train_acc)\n\nval_acc = ((LR.predict(X_val) == y_val)*1.0).mean().item()\n\nprint(\"Validation accuracy: \", val_acc)\n\n\nTrain accuracy:  0.8914955854415894\nValidation accuracy:  0.9385964870452881\n\n\nAs we can see our model did even better on the validation data than on the training data. This is a great sign, and hopefully translates to our testing data as well.\n\n\n\nI think we can see how we have done on the testing data now:\n\ntest_acc = ((LR.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", test_acc)\n\nTest accuracy without momentum:  0.9736841917037964\n\n\nWe have achieved an incredibly high accuracy on our testing data! Yay."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html",
    "href": "posts/Impact-Automated-Decision-Systems/index.html",
    "title": "Automated Decisions — Blog Post 2",
    "section": "",
    "text": "In this blog post, the design process, and the impact of the decisions made in that design process, of Automated Decision models were explored. Through a data set of credit applicants and their eventual repayment status, a model for maximizing the profits of a loan issuing bank/individual was created. The process of feature optimization and weighting, followed by decision threshold optimization allowed us to maximize the profits we received from the issuance of loans. This raised fascinating questions regarding fairness and prioritization when it comes to Automated Decisions. My model optimized for profits, resulting in a willingness to risk giving some loans that would default if it meant more that would be repaid."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html#abstract",
    "href": "posts/Impact-Automated-Decision-Systems/index.html#abstract",
    "title": "Automated Decisions — Blog Post 2",
    "section": "",
    "text": "In this blog post, the design process, and the impact of the decisions made in that design process, of Automated Decision models were explored. Through a data set of credit applicants and their eventual repayment status, a model for maximizing the profits of a loan issuing bank/individual was created. The process of feature optimization and weighting, followed by decision threshold optimization allowed us to maximize the profits we received from the issuance of loans. This raised fascinating questions regarding fairness and prioritization when it comes to Automated Decisions. My model optimized for profits, resulting in a willingness to risk giving some loans that would default if it meant more that would be repaid."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html#set-up-the-data",
    "href": "posts/Impact-Automated-Decision-Systems/index.html#set-up-the-data",
    "title": "Automated Decisions — Blog Post 2",
    "section": "Set Up The Data",
    "text": "Set Up The Data\nFirst let’s read in the training data, and split it into an X and Y training set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nX_train = df_train.drop(columns = [\"loan_status\", \"loan_grade\"])\ny_train = df_train[\"loan_status\"]\n\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nExploring the Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\n\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\np1 = sns.histplot(df_train, x = \"person_age\", hue = \"loan_intent\", element=\"step\", binwidth=5, multiple=\"fill\", stat=\"percent\")\np1.set_xlim(15, 80)\nplt.title(\"Percent of Age Group Loans by Loan Intent\")\nplt.show()\n\n\n\n\n\n\n\n\nThe most important trends to notice in my opinion, are that as a person gets older, education and venture oriented loans are replaced by more and more personal, and medical loans. Eventually, for individuals in the 75-80 years range, they are only getting medical loans. Additionally, it is important to note that when not done as a percent, there are simply more people getting loans at younger ages.\n\n\n\n# sns.pairplot(df_train, hue=\"person_home_ownership\")\n\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"loan_intent\", data = df_train)\nplt.title(\"Interest Rate by Loan Intent\")\nplt.show()\n\n\n\n\n\n\n\n\nI have not identified any goruping variables that result in a higher interest rate for specific individuals. As shown in the above example, for instance, there are no statistically significant differences based on loan intent.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"person_home_ownership\", data = df_train)\nplt.title(\"Interest Rate by Home Ownership\")\nplt.show()\n\n\n\n\n\n\n\n\nThe same is true for grouping by home ownership types. Because the medians all fall within the first and third quartiles of all other groups, it is hard to conclude that these groups are different.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"loan_grade\", data = df_train)\nplt.title(\"Interest Rate by Loan Grade\")\nplt.show()\n\n\n\n\n\n\n\n\nOnly once we start looking at things like credit history and loan grade do we begin to see different treatments in terms of interest rate, and this makes sense: a riskier loan is going to require a higher interest rate to make the risk worth it for the lender.\n\nplt.figure(figsize = (10, 10))\nsns.scatterplot(x = \"person_income\", y = \"loan_amnt\", data = df_train)\nplt.xlim(0, 200000)\nplt.title(\"Income vs. Loan Amount\")\n\nText(0.5, 1.0, 'Income vs. Loan Amount')\n\n\n\n\n\n\n\n\n\nIndividuals with larger incomes are more likely to have access to large lines of credit.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_amnt\", y = \"loan_intent\", data = df_train)\nplt.title(\"Interest Rate by Loan Grade\")\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, it does not seem like the intent of the loan impacts the size of the line of credit extended to the individual.\nWith these exploratory questions addressed, let’s fit our model.\n\n\nFitting the Model\nFirst, let’s perform an exhaustive search to determine the best quantitative and qualitative columns to use to maximize the accuracy of our Logistic Regression model:\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nall_qual_cols = [\"loan_intent\", \"person_home_ownership\", \"cb_person_default_on_file\"]\nall_quant_cols = ['person_emp_length', 'loan_amnt', 'loan_int_rate', \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 4):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    new_score = LR.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\nHighest Score:  0.8461605622735409\n\n\nOur highest scoreing model has been identified along with its highest scoring accuracy of .846.\n\nLR = LogisticRegression(max_iter = 10000)\nLR.fit(X_train, y_train)\n\nLogisticRegression(max_iter=10000)\n\n\nAlternatively, we can use sklearns feature selection RFECV tool to select them instead:\n\nfrom sklearn.feature_selection import RFECV\n\nselector = RFECV(LR, step = 1, cv = 5)\n\nselector.fit(X_train, y_train)\n\nselected_features = X_train.columns[selector.support_]\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n       'loan_int_rate', 'cb_person_cred_hist_length',\n       'person_home_ownership_MORTGAGE', 'person_home_ownership_OWN',\n       'person_home_ownership_RENT', 'loan_intent_VENTURE', 'loan_grade_A',\n       'loan_grade_B', 'loan_grade_D', 'cb_person_default_on_file_N',\n       'cb_person_default_on_file_Y'],\n      dtype='object')\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but RFECV was fitted with feature names\n  warnings.warn(\n\n\nLet’s use these instead and train our model with them:\n\nLR.fit(X_train[selected_features], y_train)\n\nLogisticRegression(max_iter=10000)\n\n\nNow that we have fit a mode, we can extract the weights, and use it to score each training loan. The resulting score can then be used to determine the best thresshold for making a decision in order to maximize our profits by giving a loan or not.\n\nimport numpy as np\nw = LR.coef_.reshape(-1)\nX = X_train[selected_features]\ns = X@w\n\nprint(s.max())\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 100, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\", range=[-20, 5])\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")\n\n1.5223252366858058\n\n\n\n\n\n\n\n\n\nWith these scores calculated, we are going to use the following simplified formula’s to determine the total profit made at each potential threshold:\n\nProfit on a loan repaid in full:\nloan_amnt(1 + 0.25loan_int_rate)**10 - loan_amnt\n\n\nBorrower Defaults on a Loan\nloan_amnt(1 + 0.25loan_int_rate)**3 - 1.7*loan_amnt\n\n\nCalculate Optimal Threshold\n\n\n\nbest_profit = float('-inf')\nbest_threshold = -20\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nprofits = []\nthresholds = np.linspace(-.5, 0.5, 101)\n\nloan_amnt = X_train['loan_amnt']  \nloan_int_rate = X_train['loan_int_rate'] \n\nfor t in np.linspace(-.5, 0.5, 101): \n    y_pred = s &gt; t\n    acc = (y_pred == y_train)\n    profit_repaid = ((acc &  -(y_train+1)) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**10 - loan_amnt))\n    loss_default = ((-(acc+1) & y_train) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**3 - .7 * loan_amnt))\n    profit = (profit_repaid - loss_default).mean()\n    profits.append(profit)\n    if profit &gt; best_profit: \n        best_profit = profit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nax.plot(thresholds, profits, color=\"steelblue\")\nax.axvline(best_threshold, linestyle=\"--\", color=\"grey\", zorder=-10)\n\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Profit Per Loan Applicant\", title = f\"Best profit {best_profit:.3f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nThe optimal threshold with our model is a score of -0.320. This results in a profit per analyzed grant of $1548.789. It is interesting that our threshold is negative, as it is a direct result of the fact that we are not optimizing for accuracy, but profits. It is okay to be at a threshold where more loans that are accepted, are not payed back, as long as the amount payed back and the amount from the loans that are payed back are greater than the losses from the loans which are defaulted on. I will discuss this topic more later on.\n\n\n\nTesting our Model, Weights, and Threshold\nNow let’s download the test data and prepare it.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\nWith our test data, let’s apply our weights to the selected features to get our score.\n\ntest_pred = X_test[selected_features]@w\n\n\n\n\nNow we can apply those scores to our optimal threshold to determine which loans we should issue.\n\ntest_pred = test_pred &gt; best_threshold\n\nWith our decisions made, we can now figure out how much money we have made on our test loans:\n\nloan_amnt = X_test['loan_amnt']  \nloan_int_rate = X_test['loan_int_rate'] \nacc = (test_pred == y_test)\nprofit_repaid = ((acc &  -(y_test+1)) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**10 - loan_amnt))\nloss_default = ((-(acc+1) & y_test) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**3 - .7 * loan_amnt))\n\nprofit = profit_repaid - loss_default\n\nprofit.mean()\n\n1496.914919368003\n\n\nWith our algorithm we made $1496.91 per loan analyzed! While this is slightly lower than the $1548 per loan analyzed on our training data, this seems pretty good.\nIt is important to note that since our goal was profit, this does not mean that we have maximized for reducing defaults on loans\n\nprofit.sum()\n\n8578819.402898025\n\n\nThis would have resulted in $8.58 Billion dollars in total profit from the test data loans.\n\n\nImpact from the Borrowers Perspective\n\ndf_pred = df_test.copy()\ndf_pred[\"pred_loan_status\"] = (test_pred)\n\ndf_pred\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\npred_loan_status\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\nFalse\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\nFalse\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\nFalse\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\nFalse\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6512\n26\n26000\nMORTGAGE\n4.0\nHOMEIMPROVEMENT\nB\n12000\nNaN\n0\n0.46\nN\n3\nNaN\n\n\n6513\n27\n44640\nRENT\n0.0\nMEDICAL\nB\n12800\n11.83\n0\n0.29\nN\n9\nFalse\n\n\n6514\n24\n48000\nOWN\n5.0\nVENTURE\nA\n10400\n7.37\n0\n0.22\nN\n3\nFalse\n\n\n6515\n26\n65000\nMORTGAGE\n6.0\nEDUCATION\nA\n6000\n9.07\n0\n0.09\nN\n3\nFalse\n\n\n6516\n29\n61000\nRENT\n12.0\nVENTURE\nD\n10000\n16.07\n0\n0.16\nN\n9\nFalse\n\n\n\n\n6517 rows × 13 columns\n\n\n\n\nAge\n\nbins = [0, 30, 40, 50, 60, 70, 80]\n\ndf_pred['age_group'] = pd.cut(df_pred['person_age'], bins, labels=[\"0-30\", \"31-40\", \"41-50\", \"51-60\", \"61-70\", \"71+\"], right=False)\n\ndf_pred.groupby([\"age_group\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nage_group\nloan_status\nmean\ncount\n\n\n\n\n0\n0-30\n0\n0.055729\n3194\n\n\n1\n0-30\n1\n0.414995\n947\n\n\n2\n31-40\n0\n0.050296\n1014\n\n\n3\n31-40\n1\n0.388462\n260\n\n\n4\n41-50\n0\n0.033654\n208\n\n\n5\n41-50\n1\n0.384615\n52\n\n\n6\n51-60\n0\n0.000000\n31\n\n\n7\n51-60\n1\n0.363636\n11\n\n\n8\n61-70\n0\n0.000000\n6\n\n\n9\n61-70\n1\n0.500000\n6\n\n\n10\n71+\n0\n0.000000\n1\n\n\n11\n71+\n1\n0.000000\n1\n\n\n\n\n\n\n\nThis demonstrates a trend that will repeat itself over the course of our entire analysis: our model makes it too easy for people who shouldn’t receive a loan to receive one. The false negative rates (people being denied a loan who deserve one) are incredibly low — below 6% in every category. The problem is that especially for younger individuals, our model is highly likely to issue a loan to someone who will default; our model only denies 42% of individuals who go on to default in the 0-30 category, 39% in the 31-40 category, 38% in the 41-50 category, 36% in the 51-60 category, and 50% in the 51-60 category. This is incredibly concerning because young individuals who are given a loan and deffault might not have the financial resources built up to overcome this challenge.\n\n\nLoan Intent\n\nPositive Prediction Rates\n\ndf_pred.groupby(\"loan_intent\")[\"pred_loan_status\", \"loan_status\"].agg([\"mean\", \"count\"])\n\n/var/folders/kn/65gyjdg54k7d_hx68jbshfz40000gn/T/ipykernel_40256/3005163201.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  df_pred.groupby(\"loan_intent\")[\"pred_loan_status\", \"loan_status\"].agg([\"mean\", \"count\"])\n\n\n\n\n\n\n\n\n\npred_loan_status\nloan_status\n\n\n\nmean\ncount\nmean\ncount\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.136062\n904\n0.279497\n1034\n\n\nEDUCATION\n0.142857\n1176\n0.167421\n1326\n\n\nHOMEIMPROVEMENT\n0.086039\n616\n0.246088\n703\n\n\nMEDICAL\n0.152842\n1073\n0.281553\n1236\n\n\nPERSONAL\n0.135271\n998\n0.219227\n1113\n\n\nVENTURE\n0.118257\n964\n0.145701\n1105\n\n\n\n\n\n\n\nOne thing stands out from simply showing the percent of the time our model predicts a default compared to the percent of the time a default actually occurs: 1, Our model under predicts defaults significantly due to its aim of profit as opposed to actual accuracy. Education and Venture loans seem to be the two categories in which the ratios are similar. This tells us nothing about the accuracy of the Loans, only the rates at which it predicts defaults in each category. While this means that it is easier to get a loan across the board than it should be, this does not necessarily mean it is better for the borrower. Banks probably have a much better sense for if an individual is going to default or not compared to the experience of an individual taking out a loan for the first time. The fact that the bank is abusing that power to prioritize profits could mean that many individuals are put into a financial hole due to unpayable debt.\n\n\nError-Rate Balance\nLet’s see how this breaks down in terms of actual accuracy:\n\np1 = sns.FacetGrid(df_pred, col=\"loan_intent\", hue=\"pred_loan_status\")\np1.map(sns.histplot, \"loan_status\")\np1.add_legend()\n\n\n\n\n\n\n\n\nWhile not the typical way, these visualizations show the TP and FP across categories. It is clear from these graphs that across the board, our model is pretty good at giving loans to people who deserve them, with only a few people being denied loans who would go on to pay back in full. On the other hand, our model is almost as likely to give a loan than not, if not more likely to give a loan than not, to people who would go on to default. This gun-ho model could be potentially predatory to borrowers, and strictly maximizing for profits could be a really bad idea (Think 2008 Financial Crisis).\n\ndf_pred.groupby([\"loan_intent\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\nmean\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\n0\n0.054348\n644\n\n\n1\nDEBTCONSOLIDATION\n1\n0.338462\n260\n\n\n2\nEDUCATION\n0\n0.070480\n979\n\n\n3\nEDUCATION\n1\n0.502538\n197\n\n\n4\nHOMEIMPROVEMENT\n0\n0.045455\n462\n\n\n5\nHOMEIMPROVEMENT\n1\n0.207792\n154\n\n\n6\nMEDICAL\n0\n0.040365\n768\n\n\n7\nMEDICAL\n1\n0.436066\n305\n\n\n8\nPERSONAL\n0\n0.055270\n778\n\n\n9\nPERSONAL\n1\n0.418182\n220\n\n\n10\nVENTURE\n0\n0.044957\n823\n\n\n11\nVENTURE\n1\n0.546099\n141\n\n\n\n\n\n\n\nThis Quantitative look at error rates reaffirms what we saw from the visualizations: our model has a very high True Negative Rate across categories, and also quite Low True Positive Rates ranging from .207 (Home Improvement) to .546 (Venture). Regardless of category, this means that our model is far from optimized for not issuing loans to people who have a high risk of default, it is optimized to give as many loans to people who will pay back as possible. As mentioned above this could have devastating impacts an individuals financial health.\n\n\n\nBy Income Level\n\nbins = [0, 50000, 100000, 150000, 200000, 10000000]\n\ndf_pred['income_group'] = pd.cut(df_pred['person_income'], bins, labels=[\"0-50K\", \"50-100K\", \"100-150K\", \"150-200K\", \"200K+\"], right=False)\n\ndf_pred.groupby([\"income_group\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nincome_group\nloan_status\nmean\ncount\n\n\n\n\n0\n0-50K\n0\n0.119768\n1553\n\n\n1\n0-50K\n1\n0.543837\n787\n\n\n2\n50-100K\n0\n0.023223\n2153\n\n\n3\n50-100K\n1\n0.229064\n406\n\n\n4\n100-150K\n0\n0.000000\n530\n\n\n5\n100-150K\n1\n0.000000\n61\n\n\n6\n150-200K\n0\n0.000000\n124\n\n\n7\n150-200K\n1\n0.000000\n12\n\n\n8\n200K+\n0\n0.000000\n94\n\n\n9\n200K+\n1\n0.000000\n11\n\n\n\n\n\n\n\nIt is fascinating to see that our income variable is an incredibly high predictor of if an individual will receive a loan or not. While not surprising, it seems that if an individual makes over 100K they will receive a loan. 0 Individuals over this income were denied even though many would have defaulted. Even for individuals with an income between 0 and 50 k, our model only rejected 54% of individuals who went on to default. That number is 23% for 50-100K earners, and 0 for everyone with a higher income.\n\n\n\nDiscussion\nThrough this blog post, I learned quite a bit about how different interests can lead to problematic results in a model. I also learned technical skills such as threshold optimization, and the importance of defining and exploring what occurs when you decide on what you are going to optimize for. In our model we optimized for profits, not fairness in any respect. This meant is was more difficult for people in particular groups—medical debt and young— to get a loan. People with medical expenses have higher default rates, and so it is harder for them to get a loan. In my opinion, and at this point we are in the realm of value judgement not fact, the lower availability of credit is not unfair. While I do not have a good definition of fairness, what comes to my mind is that two private* parties involved in a decision act with good faith towards one another and treat each other and everyone else as of equal value. This does not mean that one group should be forced to take unnecessary risk to overcome an existing unfairness. That sort of redistributive action is what governments are responsible for. I do not mean that private individuals can’t or shouldn’t try to actively overcome existing unfairness, I mean that they cannot be forced to or expected to just because something is unfair. In the case of Medical Debt, the unfair part is the fact that our medical system is so broken that medical expenses become crippling and un-creditable. There is no reason that banks or private creditors should shoulder this unfairness by giving loans that will not be paid back. The money that banks loan should not be thrown around without foresight, because that is the money of everyone else being held and safeguarded. Given the willingness of my model to predict repayment, a more relevant discussion is that our model was too willing to give loans to people who probably should not have been given them. Our model found a threshold where giving loans to people with a certain amount of risk meant that enough would repay such that profit would be made in-spite of the fact that a significant portion would default. This is predatory in nature and potentially catastrophic for the individual. This is especially concerning given the banks privilege knowledge of credit, while the borrower could potentially be much less informed regarding their decision."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for training:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\nThis is what the data looks like:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nLets group our data by Species and Clutch completion, and look at the mean and standard deviation for the quantitative columns. In doing so, we should be able to see some columns that might be useful for differentiating species.\n\ntable = train.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1).groupby(['Clutch Completion', 'Species']).aggregate(['mean', 'std'])\ntable.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n39.041667\n2.152571\n18.225000\n1.072063\n187.666667\n7.726381\n3764.583333\n605.604350\n8.913195\n0.328239\n-25.676738\n0.485634\n\n\nChinstrap\n49.040000\n4.627022\n18.040000\n0.987927\n194.400000\n7.121173\n3532.500000\n536.196740\n9.425587\n0.346548\n-24.560953\n0.320757\n\n\nGentoo\n46.485714\n3.632001\n14.714286\n1.206135\n216.000000\n6.708204\n4910.714286\n679.131974\n8.279576\n0.235220\n-26.247917\n0.577585\n\n\nYes\nAdelie\n38.962617\n2.698411\n18.429907\n1.235040\n190.355140\n6.549164\n3713.317757\n447.165045\n8.856116\n0.436348\n-25.810248\n0.578651\n\n\nChinstrap\n48.780851\n3.207982\n18.436170\n1.165151\n196.340426\n7.516002\n3788.297872\n366.195595\n9.310443\n0.375693\n-24.551794\n0.230540\n\n\n\n\n\n\n\nThis is helpful, but it might be even more insightful and intuitive to compute the range of 2 standard deviations for each quantitative column. With the standard deviations and means calculated, the range of 2 standard deviations can then be calculated by subtracting and adding 2 times the value of the std dev to/from the mean for that column.\nNote: I found the assign function used below in search of a pandas equivalent of the R mutate()function (See link below)\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html\n\n\ntable = table.assign(Culmen_Length_Max = table[\"Culmen Length (mm)\"][\"mean\"] + 2*table[\"Culmen Length (mm)\"][\"std\"], Culmen_Length_Min = table[\"Culmen Length (mm)\"][\"mean\"] - 2*table[\"Culmen Length (mm)\"][\"std\"])\ntable = table.assign(Culmen_Depth_Max = table[\"Culmen Depth (mm)\"][\"mean\"] + 2*table[\"Culmen Depth (mm)\"][\"std\"], Culmen_Depth_Min = table[\"Culmen Depth (mm)\"][\"mean\"] - 2*table[\"Culmen Depth (mm)\"][\"std\"])\ntable = table.assign(Body_Mass_Max = table[\"Body Mass (g)\"][\"mean\"] + 2*table[\"Body Mass (g)\"][\"std\"], Body_Mass_Min = table[\"Body Mass (g)\"][\"mean\"] - 2*table[\"Body Mass (g)\"][\"std\"])\ntable = table.assign(Flipper_Length_Max = table[\"Flipper Length (mm)\"][\"mean\"] + 2*table[\"Flipper Length (mm)\"][\"std\"], Flipper_Length_Min = table[\"Flipper Length (mm)\"][\"mean\"] - 2*table[\"Flipper Length (mm)\"][\"std\"])\ntable = table.assign(Delta_13_Max = table[\"Delta 13 C (o/oo)\"][\"mean\"] + 2*table[\"Delta 13 C (o/oo)\"][\"std\"], Delta_13_Min = table[\"Delta 13 C (o/oo)\"][\"mean\"] - 2*table[\"Delta 13 C (o/oo)\"][\"std\"])\ntable = table.assign(Delta_15_Max = table[\"Delta 15 N (o/oo)\"][\"mean\"] + 2*table[\"Delta 15 N (o/oo)\"][\"std\"], Delta_15_Min = table[\"Delta 15 N (o/oo)\"][\"mean\"] - 2*table[\"Delta 15 N (o/oo)\"][\"std\"])\ntable.drop([\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"], axis = 1)\n\n\n\n\n\n\n\n\n\nCulmen_Length_Max\nCulmen_Length_Min\nCulmen_Depth_Max\nCulmen_Depth_Min\nBody_Mass_Max\nBody_Mass_Min\nFlipper_Length_Max\nFlipper_Length_Min\nDelta_13_Max\nDelta_13_Min\nDelta_15_Max\nDelta_15_Min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n43.346808\n34.736525\n20.369125\n16.080875\n4975.792033\n2553.374633\n203.119429\n172.213904\n-24.705470\n-26.648007\n9.569673\n8.256716\n\n\nChinstrap\n58.294044\n39.785956\n20.015854\n16.064146\n4604.893481\n2460.106519\n208.642347\n180.157653\n-23.919438\n-25.202468\n10.118684\n8.732490\n\n\nGentoo\n53.749716\n39.221713\n17.126556\n12.302015\n6268.978234\n3552.450338\n229.416408\n202.583592\n-25.092747\n-27.403087\n8.750016\n7.809135\n\n\nYes\nAdelie\n44.359438\n33.565796\n20.899986\n15.959827\n4607.647847\n2818.987667\n203.453469\n177.256812\n-24.652947\n-26.967550\n9.728812\n7.983420\n\n\nChinstrap\n55.196815\n42.364888\n20.766472\n16.105869\n4520.689062\n3055.906683\n211.372430\n181.308421\n-24.090715\n-25.012873\n10.061828\n8.559058\n\n\nGentoo\n52.472111\n41.765667\n16.914536\n12.945464\n6021.446275\n4078.553725\n228.625764\n204.996458\n-25.027738\n-27.255542\n8.810899\n7.678713\n\n\n\n\n\n\n\nThis is extraordinarily insightful, as we can see which quantitative columns have distinct ranges when grouped by Culmen Depth and Species. Since 95.4% of data falls within 2 std deviations of the mean, distinct ranges in this category can help identify very high quality candidate columns to use as quantitative predictor variables.\nUpon analysis of this table, the Culmen Length, Culmen Depth, and Flipper Length appear to the columns with the most variation in ranges across species and Clutch Completion. For penguins with a clutch completion of No, Culmen Length ranges are potentially distinguishable for Adelie compared with Chinstrap and Gentoo, Culmen Depth ranges are distinguishable for Gentoo compared with Adelie and Chinstrap, and Flipper Length has all overlapping ranges although they are also very different with Adelie the low end, Chinstrap in the middle, and Gentoo at the top.\nFor penguins with clutch completion of yes, Culmen Length distinguishes Adelie from Chinstrap and Gentoo, Culmen Depth distinguishes Gentoo from Adelie and Chinstrap, and Flipper Length once again are overlapping but not completely.\nLet’s make visualizations of these differences to analyze them further.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\n# p1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\",  style = \"Island\", hue = \"Species\")\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[1],  style = \"Clutch Completion\", hue = \"Species\")\n\nsns.despine() # remove the top and right spines\n\np1.title.set_text(\"Culmen Length vs. Culmen Depth\")\n\n\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0],  style = \"Clutch Completion\", hue = \"Species\")\n\np2.title.set_text(\"Flipper Length vs. Culmen Length\") # set the title of the left visualization\nax[0].get_legend().remove() # remove the legend from the left graph to avoid duplication and messiness\n\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # move the legend to the right graph\n\n\n\n\n\n\n\n\n\n\nFrom these two Plots, it appears that while Flipper Length and Culmen Depth would be very useful for distinguishing Gentoo penguins from the other two species, it would be very difficult to distinguish between the Chinstrap and Adelie penguins. When replacing Flipper Length with Culmen Depth, it appears that while the overlap still exists it may be possible to develop an appropriate model for predicting species.\nRegardless, the most important takeaway is that it seems inlikely by looking at the data that a logistic regression will be capable of achieving 100% accuracy using these features, as there are some overlapping—or close to overlapping—regions on both of these graphs."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nNow that we have explored the data and established that a model may be effective and developed some hypothesis regarding which features and models might work best, we need to do an exhaustive and repeatable feature search to ensure that we create the best possible model.\n\nLogistic Regression\nFirst, let’s try using a Logistic Regression even though our visualizations suggested this might not be 100% effective.\nIn this code we cycle through all combinations of 1 qualitative and 2 quantitative predictor features, saving the score and feature set that perform the best.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    new_score = LR.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  0.99609375\n\n\nUsing a Logistic Regression, the highest score we find is 0.996 using Sex, Culmen Length, and Culmen depth. Because our aim is 100%, there is no logistic regression using 1 qualitative and 2 quantitative predictor features that will achieve our goal. let’s take a look at using a different model\n\n\nRandom Forest Classifier\nThe same process is followed as above, except the Logistic Regression is substituted for a Random Forrest Classifier.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    new_score = RF.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  1.0\n\n\nWonderful! Our search has found that using a Random Forest Classifier, CLutch Completion, Culmen Length, and Culmen Depth results in a perfect score when scored on the training data. Now we need to fit the model with the best columns found above."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Fitting the Model — Random Forest Classifier Model",
    "text": "Fitting the Model — Random Forest Classifier Model\nSince we have identified that the Random Forest Classifier model has a score of 1 with the columns of Clutch Completion, Culmen Length, and Culmen depth, let’s train our model:\n\nRF = RandomForestClassifier()\nRF.fit(X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\nRandomForestClassifier()\n\n\n\nCross Validation\nBefore Committing to the Random Forest Classifier Model, let’s cross validate it!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_RF = cross_val_score(RF, X_train, y_train, cv=5)\ncv_scores_RF\n\narray([0.98076923, 1.        , 1.        , 0.98039216, 1.        ])\n\n\n3 / 5 of our folds score perfectly in cross validation. This suggests that there are specific values in our dataset that are difficult to classify if they are in the testing data and not in the training data, but also that our model is performing very well.\nLet’s look at the mean score:\n\ncv_scores_RF.mean()\n\n0.9922322775263952\n\n\nThis gives us a mean score of .99, very close to our goal of 1. With this in mind, let’s test the data to see how we did."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Testing — Random Forest Classifier Model",
    "text": "Testing — Random Forest Classifier Model\nWith our model trained, we can now import the test data and see how our model performs.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nRF.score(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n1.0\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = RF.predict(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]])\nC = confusion_matrix(y_test, y_test_pred)\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOur model predicted the species of the test data set with 100% accuracy! Yay! As such, our confusion matrix tells us that every penguin was predicted to be the species that it actually belongs to."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision regions:",
    "text": "Plotting Decision regions:\nThe code below is code for plotting data points on top of the decision regions of a given model:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", bbox_to_anchor=(1.05, 1), handles = patches, loc='upper left', borderaxespad=0.)\n\n      \n      \n      plt.tight_layout()\n\nWith this function, we can now pass in our data and model.\n\nTraining Data and Classification Regions\nLets start with the decision regions plotted with the training data. This is insightful for seeing how the model was fit around our training dataset.\n\nplot_regions(RF, X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\n\n\n\n\n\n\n\nIt is interesting to note that this is potentially an overfit. Notice the distinct Adelie region in the Clutch Completion = Yes visualization or the sliver of Gentoo reaching into the Chinstrap region. Let’s take a look at how these did not end up being a problem in the prediction of our testing data species. These are likely the exact points that were causing certain cross validations to be worse than others.\n\n\nTesting Data and Classification Regions\n\nplot_regions(RF, X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n\n\n\n\n\n\nWe can see from these visualizations that this testing data has much less overlap than our training dataset—it does not have many of those “problematic” points seen in the training data. I would be interested to see if this model would hold up on a much larger testing dataset, or if we had been given a different split for test/train. It is also important to note that the testing dataset had very little data with no for clutch completion, and none in that category that were Adelie."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\n\nProcess and Results\nIt was found that it is possible to train a model, using the palmer penguins data set, that is capable of predicting the target value—species—with perfect accuracy. This was achieved through the use of a Random Forrest Classifier model, 3 predictor variables: Clutch Completion, Culmen Length, and Culmen Depth. While a Logistic Regression was attempted, our goal of a perfect prediction score could not be achieved using a Logistic Regression and any combination of 2 quantitative and 1 qualitative predictor variables. It should also be noted that there were particular points in the training dataset that were potentially overfit by our successful Random Forest Classification model. This was observed in cross validation, in which only 3/5 of our splits achieved a perfect score. It was also observed visually in the display of the training data plotted on top of the model’s decision regions. This did not materialize as a poor score in the final testing, because the data in the testing dataset coincidentally did not have many points in the regions where the potential overfitting occurred. A larger dataset, or a different train test split, would be interesting next steps to see if our model selection and feature selection were good ones.\n\n\nLearning\nApart from enhancing my technical skills for exploring a dataset, performing a train test split, creating a repeatable methodology for feature selection, and fitting and testing a model, this process taught me a lot about the potential problems that can come with classification models. Particularly, the limits on logistic regressions in classification, the potential for overfitting, and the importance of having enough varied data in both your training and testing data. I also learned how helpful visualizations can be for understanding what is going on throughout the model creation process."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html",
    "href": "posts/Perceptron-Algorithm/index.html",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n%load_ext autoreload\n%autoreload 2\nfrom minibatchperceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#abstract",
    "href": "posts/Perceptron-Algorithm/index.html#abstract",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Abstract",
    "text": "Abstract\nThis blogpost explores the implementation of the perceptron as well as an exploration of its properties, capabilities, and limitations. This includes exploring how it performs on linearly separable data compared to data that is not linearly separable. Then the perceptron on data with more than 2 features is explored to prove that it still works as long as the data is linearly separable. Then the concept of a minibatch perceptron in which more than one data point is passed to the perceptron at a time."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-source-code",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-source-code",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron Source Code",
    "text": "Perceptron Source Code\nUse this link to access the perceptron source code. Use this link to access the minibatch perceptron source code.\ndef grad(self, X, y):\n    y_ = 2 * y - 1  # convert labels\n    return (-1 * (self.score(X) * y_ &lt; 0).float() * y_ * X).view(-1)\nAs seen in the above code, the grad function works by first converting the target variable classifications from {0,1} to {-1,1}. Inside the parentheses, this result is multiplied by the points corresponding score. The result of this comparison will be negative if the classification is incorrect and positive otherwise. After comparing this result to 0, we are left with a 0 if the classification is correct, and a 1 if it is incorrect. This means that no change is made to correct classifications. For incorrect classifications, they are then multiplied by the true classification and their feature’s to create the change that is returned to the step function when called. The result is that the weight is adjusted by 0 if the classification is correct, and y_ multiplied by each feature if the classification is incorrect. .view(-1) ensures that the return is a 1d vector compatible with the implemented step algorithm.\nMinibatch functions"
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-basic-testing",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-basic-testing",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron Basic Testing",
    "text": "Perceptron Basic Testing\n\nLinearly Seperable Data\nLet’s start by creating and visualizing a set of 300 2D linearly classifiable data points, and visualize them on a 2D plane.\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\nData Subset\nLet’s start with a subset of this data for exploratory purposes:\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWhile we have not done so yet, it is easy to see how a line could “linearly classify” (divide) these groups so that each group is on one side of that line.\n\n\nMinimal Training Loop\nLet’s do a minimal training loop on this data to show that our perceptron can correctly classify this data.\n\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    ix = torch.randperm(X.size(0))[:1]\n    x_i = X[ix, :]  \n    y_i = y[ix]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n\n    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \nprint(loss_vec)\nprint(\"Final loss: \", loss)\n   \n    \n\n[tensor(0.5000), tensor(0.5000), tensor(0.)]\nFinal loss:  tensor(0.)\n\n\nOur perceptron came up with a result without reaching our maximum number of itterations, meaning that it found a classification line with a loss equal to 0. Let’s plot it to see what this looks like:\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nBeautiful, our model has created a linear classification that splits our two groups into their respective categories."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#complete-run",
    "href": "posts/Perceptron-Algorithm/index.html#complete-run",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Complete Run",
    "text": "Complete Run\nNow let’s do a complete run and demonstrate how our classification line adjusts with each update. Code from these Class notes “Complete Run” section was used to create this run and visualization.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n\n    i = torch.randperm(X.size(0))[:1]\n    x_i = X[i, :]  \n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThis chart shows each update of our weights (classification line). The previous line is shown dashed, the new line is shown solid, and the point the update is based upon is shown outlined in black. As demonstrated in this series of charts, our perceptron is picking a random point and updating the weights so that specific point is classified correctly. Because we only do this for points which are not correctly classified already we know that if the data is linearly separable our perceptron will converge.\n\nNon-Linearly Seperable 2D Data\nLet’s generate some linearly non-separable data:\n\ntorch.manual_seed(1234)\n\ndef inseparable_data(n_points = 50, noise = 0.4, p_dims = 2):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n\ndef plot_inseparable_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = inseparable_data()\nplot_inseparable_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs can be clearly seen just by looking, there is no line that can separate these two groups of data. The overlapping section means that our data is not linearly separable, and therefore our perceptron will never converge.\nWith setting a maximum number of iterations, we can see more closely how these points cause problems for our perceptron. The following code sets the maximum number of itertions at 1000, and tracks the first 12 updates to demonstrate the mechanisms for why this never converges.\nOnce again, this code is borrowed from the Class notes “Complete Run” section with some minor alterations to set a maximum number of iterations and allow for more updates to be visualized.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nfig, axarr = plt.subplots(3, 4, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nmax_iter = 1000\niter = 0\ncount = 0\n\nwhile (loss &gt; 0) & (max_iter &gt;= iter):\n    iter += 1\n    if count &lt; 12:\n        ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        count +=1\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if (local_loss &gt; 0) & (count &lt;= 12):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhile only visualizing the first 12 updates, it is already clear from the above visualization that this data is never going to converge. The overlapping points in the middle, mean that any corrections will leave some combination in that small area incorrectly classified, and therefore our loss will never reach 0. Instead, by the 4th update, all subsequent updates take part in this back and forth of correcting so that a different, but incomplete, set of points are correctly classified."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-on-5d-features",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-on-5d-features",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron on 5D Features",
    "text": "Perceptron on 5D Features\nIt will now be shown that our perceptron can function on linearly separable data of more than two (5) dimensions:\n\n\n\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 5)\n\nprint(X.shape)\n\n\ntorch.Size([300, 6])\n\n\nWith 5d data for 300 points, we can now do a minimal training loop and track the loss throughout in the same way done before:\n\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0.0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i], :]  # Transpose x_i to make its shape compatible for multiplication\n    y_i = y[i]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n\n    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \n   \n    \n\nSince our perceptron converged, let’s take a look at how the loss changed throughout our training loop:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nIt is interesting to see that with a single update, our loss was reduced to .0067 with no more updates for more than 150 iterations when a single further update reduced the loss to 0.\nI know that the data is linearly separable, because the loss ends at 0 and our perceptron converged. There is no way to know that your data is not linearly separable with higher dimensions where you cannot simply look at a visualization and know."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#mini-batch-perceptron",
    "href": "posts/Perceptron-Algorithm/index.html#mini-batch-perceptron",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Mini Batch Perceptron",
    "text": "Mini Batch Perceptron\nIn this section a Minibatch perceptron will be implemented which computes an update using k points at once, rather than a single point.\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 100, noise = 0.2, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe are once again starting with a subset of the same set of linearly separable points.\n\nMinimal Training Loop\nNow we will run our perceptron on a minimal training loop where we will pass in K random indices to the perceptron:\n\nimport torch\n\n# instantiate a model and an optimizer\np = MinibatchPerceptron() \nopt = MinibatchPerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0.0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix, :]  \n    y_i = y[ix]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \n\nprint(\"Final loss: \", loss)\nprint(\"loss vector: \", loss_vec)\n\nFinal loss:  tensor(0.)\nloss vector:  [tensor(0.5000), tensor(0.2700), tensor(0.4500), tensor(0.4500), tensor(0.4500), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.2700), tensor(0.0400), tensor(0.0400), tensor(0.0400), tensor(0.0400), tensor(0.)]\n\n\n\ntorch.manual_seed(1234567)\n\ndef min_batch_full_loop(X, y, k, x_plots, y_plots):\n    # initialize a perceptron \n    p = MinibatchPerceptron()\n    opt = MinibatchPerceptronOptimizer(p)\n    p.loss(X, y)\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (7, 5)\n    fig, axarr = plt.subplots(x_plots, y_plots, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n\n    while loss &gt; 0:\n        ax = axarr.ravel()[current_ax]\n\n        # save the old value of w for plotting later\n        old_w = torch.clone(p.w)\n\n        # make an optimization step -- this is where the update actually happens\n        # now p.w is the new value \n\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix, :]  \n        y_i = y[ix]\n        local_loss = p.loss(x_i, y_i).item()\n\n        if local_loss &gt; 0:\n            opt.step(x_i, y_i)\n        # if a change was made, plot the old and new decision boundaries\n        # also add the new loss to loss_vec for plotting below\n        if local_loss &gt; 0:\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            #ax.scatter(ix[[]])\n            for i in range(k): # cycle through the k points and outline them in black\n                ax.scatter(X[ix[i]][0], X[ix[i]][1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n    plt.tight_layout()\n\n\n\n\nK = 1 minibatch\nLet’s see what happens when we perform a minibatch with a k of size = 1:\n\nmin_batch_full_loop(X, y, 1, 2,2)\n\n\n\n\n\n\n\n\nAs show, the minibatch acts in exactly the same way as a normal perceptron, which makes sense because it is taking an average of a single point’s correction, which is the same as just basing the correction on that single point.\n\n\nMinibatch with k=10\nNow Let’s try a real minibatch with a k = 10:\n\nmin_batch_full_loop(X, y, 10, 3, 2)\n\n\n\n\n\n\n\n\nAs seen in the above visualization, the correction is done based on the classification of all of the selected points, which includes some who will be correctly classified resulting in a lower mean gradient calculation. That is why the changes are often much smaller than the corrections seen when k = 1.\n\n\nMinibatch when k = n (n=100)\n\nmin_batch_full_loop(X, y, 100, 3, 2)\n\n\n\n\n\n\n\n\nThis point is driven home even more clearly by the demonstration when k = n (100). While the first update is quite large to overcome the fact that half the points are wrongly classified , all subsequent updates are very small due to the fact that so few points of the ones selected are incorrectly classified."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#complexity-analysis",
    "href": "posts/Perceptron-Algorithm/index.html#complexity-analysis",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Complexity Analysis",
    "text": "Complexity Analysis\nWhat is the runtime complexity of a single iteration of the perceptron algorithm? Does the runtime complexity of a single iteration depend on the number of data points n? What about the number of features p?\nThe runtime of the perceptron algorithm is O(p). This is driven by the dot product calculation of the scoring function: X@self.w. As such, the number of features p has a direct impact on the complexity, as the complexity of a dot product is O(n), and in this case the length of the vectors is based on the number of features p. The total number of data points has no impact on this.\nIf you implemented minibatch perceptron, what is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\nFor the minibatch, where k is the size of the batch, a single iteration is O(k*p). This is because we must calculate the score for every one of the points in the batch."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#discussion",
    "href": "posts/Perceptron-Algorithm/index.html#discussion",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Discussion",
    "text": "Discussion\nThis blog post has explored the perceptron and minibatch perceptron algorithms. The perceptron is an effective algorithm for optimizing weights for linearly separable data, however it is ineffective when data is not linearly separable. It functions by randomly selecting a datapoint, and if it is incorrectly classified by current weights, the weights are update based on the gradient of the error. This process continues until all data points are correctly classified—only possible if the data is in fact linearly separable. This all applies for data that has 2 or more features. However, the time complexity of performing each iteration of the algorithm is increased by the number of features. The minibatch perceptron functions in much the same way as the perceptron, except instead of calculating the weight adjustment for a single datapoint, it takes the average of several. This is interesting as it allows for some adjustment minimizing for points which are classified correctly. Additionally, each additional point in a batch increases the time complexity of an iteration of the batch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression Implementation\n\n\n\n\n\nImplementing the Logistic Regression algorithm and performing experiments on it.\n\n\n\n\n\nJul 3, 2027\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Logistic Regression\n\n\n\n\n\nImplementing Sparse Kernel Machine and Experimenting on it.\n\n\n\n\n\nApr 4, 2027\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution Based Weight Vector Optimization\n\n\n\n\n\nImplementation of Evolution Based Weight Vector Optimization\n\n\n\n\n\nJul 5, 2026\n\n\nJames Cummings, Jiffy Lesica, Yahya Rahhawi, Lukka Wolff\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Algorithm — Blog Post 4\n\n\n\n\n\nImplementing the Perceptron Algorithm in Python\n\n\n\n\n\nMar 19, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nPerforming a bias audit on algorithm fit using folktables dataset\n\n\n\n\n\nMar 4, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Decisions — Blog Post 2\n\n\n\n\n\nDesign and Impact of Automated Decisions\n\n\n\n\n\nFeb 28, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nPalmer Penguins Classification blog post.\n\n\n\n\n\nFeb 18, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nUsing the Palmer Penguins dataset to create a predictive model for classifying species of palmer penguins!\n\n\n\n\n\nDec 2, 2024\n\n\nJames Cummings\n\n\n\n\n\n\nNo matching items"
  }
]