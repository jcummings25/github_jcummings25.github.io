[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "\"CSCI\" + \" 0451\"\n\n'CSCI 0451'"
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Classifying Palmer Penguins",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html#setting-up-the-data",
    "href": "posts/new-test-post/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for trainging:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what the data looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/new-test-post/index.html#lets-explore-the-data",
    "href": "posts/new-test-post/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\n\ntrain.groupby(['Species', 'Island']).aggregate(\"mean\")\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 1:10]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\n\n\n\n\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nPAL0708\n115\n22\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN61A1\nN11A2\n\n\nGentoo\nPAL0708\n124\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN8A2\nN11A1\n\n\nDream\nAdelie\nPAL0708\n152\n31\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN85A2\nN21A1\n\n\nChinstrap\nPAL0708\n68\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN99A2\nN100A1\n\n\nTorgersen\nAdelie\nPAL0708\n132\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN9A2\nN10A1\n\n\n\n\n\n\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 10:20]\n\n\n\n\n\n\n\n\n\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nYes\nNo\n11/9/09\n11/10/07\n45.6\n34.5\n21.1\n16.0\n203.0\n172.0\n\n\nGentoo\nYes\nNo\n12/3/07\n11/13/08\n55.9\n40.9\n17.3\n13.1\n230.0\n207.0\n\n\nDream\nAdelie\nYes\nNo\n11/9/07\n11/10/08\n44.1\n34.0\n21.2\n16.5\n208.0\n178.0\n\n\nChinstrap\nYes\nNo\n12/3/07\n11/14/08\n58.0\n40.9\n20.8\n16.4\n212.0\n178.0\n\n\nTorgersen\nAdelie\nYes\nNo\n11/9/08\n11/11/07\n46.0\n34.1\n21.5\n15.9\n210.0\n176.0\n\n\n\n\n\n\n\nPotentially egg date and Flipper length and island.\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 20:29]\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\n4725.0\n2850.0\n9.79532\n8.08138\n-24.36130\n-26.78958\n\n\nGentoo\n6300.0\n3950.0\n8.83352\n7.63220\n-25.00169\n-27.01854\n\n\nDream\nAdelie\n4650.0\n2975.0\n9.72764\n8.01485\n-24.52698\n-26.69543\n\n\nChinstrap\n4800.0\n2700.0\n10.02544\n8.47173\n-23.78767\n-25.14550\n\n\nTorgersen\nAdelie\n4700.0\n2900.0\n9.59462\n7.69778\n-23.90309\n-26.53870\n\n\n\n\n\n\n\n\ntrain.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\") \n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\nSpecies\nClutch Completion\nIsland\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nDream\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n2\n3\n\n\nTorgersen\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n\n\nYes\nBiscoe\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n0\n\n\nDream\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n41\n39\n39\n3\n\n\nTorgersen\n35\n35\n35\n35\n35\n35\n34\n34\n34\n34\n30\n29\n29\n8\n\n\nChinstrap\nNo\nDream\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n0\n\n\nYes\nDream\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n46\n47\n0\n\n\nGentoo\nNo\nBiscoe\n8\n8\n8\n8\n8\n8\n7\n7\n7\n7\n7\n7\n7\n0\n\n\nYes\nBiscoe\n90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n87\n89\n89\n0\n\n\n\n\n\n\n\n\ntabledf = train.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\")\ntabledf = tabledf.drop(tabledf.columns[1:], axis=1)\ntabledf = tabledf.rename(columns={\"studyName\": \"Count\"})\ntabledf.reset_index(level = ['Clutch Completion', 'Island'], inplace = True)\ntabledf\n\n\n\n\n\n\n\n\nClutch Completion\nIsland\nCount\n\n\nSpecies\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n\n\nAdelie\nNo\nDream\n3\n\n\nAdelie\nNo\nTorgersen\n7\n\n\nAdelie\nYes\nBiscoe\n31\n\n\nAdelie\nYes\nDream\n42\n\n\nAdelie\nYes\nTorgersen\n35\n\n\nChinstrap\nNo\nDream\n10\n\n\nChinstrap\nYes\nDream\n47\n\n\nGentoo\nNo\nBiscoe\n8\n\n\nGentoo\nYes\nBiscoe\n90\n\n\n\n\n\n\n\n\ntabledf.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"sum\") / tabledf.groupby(['Species', 'Island']).aggregate(\"sum\")\n\n\n\n\n\n\n\n\n\n\nCount\n\n\nSpecies\nIsland\nClutch Completion\n\n\n\n\n\nAdelie\nBiscoe\nNo\n0.060606\n\n\nYes\n0.939394\n\n\nDream\nNo\n0.066667\n\n\nYes\n0.933333\n\n\nTorgersen\nNo\n0.166667\n\n\nYes\n0.833333\n\n\nChinstrap\nDream\nNo\n0.175439\n\n\nYes\n0.824561\n\n\nGentoo\nBiscoe\nNo\n0.081633\n\n\nYes\n0.918367\n\n\n\n\n\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html",
    "href": "posts/Auditing-bias-blog-post/index.html",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog post, a Machine Learning Model is trained, using the American Community’s Public Use Microdata Sample data, to predict employment status of individuals using features other than race. A bias audit is then performed to evaluate if the model exhibits racial bias. In our bias audit, we prove that proxies are leading to individuals from specific racial categories being treated very differently with regards to error rate balance and calibration. This, and the implications with regards to our model, are discussed further in the final discussion."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#abstract",
    "href": "posts/Auditing-bias-blog-post/index.html#abstract",
    "title": "Auditing Bias",
    "section": "",
    "text": "In this blog post, a Machine Learning Model is trained, using the American Community’s Public Use Microdata Sample data, to predict employment status of individuals using features other than race. A bias audit is then performed to evaluate if the model exhibits racial bias. In our bias audit, we prove that proxies are leading to individuals from specific racial categories being treated very differently with regards to error rate balance and calibration. This, and the implications with regards to our model, are discussed further in the final discussion."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#descriptive-analysis",
    "href": "posts/Auditing-bias-blog-post/index.html#descriptive-analysis",
    "title": "Auditing Bias",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis"
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#loading-the-data",
    "href": "posts/Auditing-bias-blog-post/index.html#loading-the-data",
    "title": "Auditing Bias",
    "section": "Loading the Data",
    "text": "Loading the Data\nDownload the data from the PUMAs fataset for the survey from 2018, at the individual level, and for the state of California.\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport pandas as pd\nimport numpy as np\n\nSTATE = \"CA\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000004\n9\n1\n3701\n4\n6\n1013097\n32\n30\n...\n34\n60\n60\n7\n8\n59\n33\n8\n58\n32\n\n\n1\nP\n2018GQ0000013\n9\n1\n7306\n4\n6\n1013097\n45\n18\n...\n0\n0\n0\n91\n46\n46\n0\n89\n45\n0\n\n\n2\nP\n2018GQ0000016\n9\n1\n3755\n4\n6\n1013097\n109\n69\n...\n105\n232\n226\n110\n114\n217\n2\n111\n2\n106\n\n\n3\nP\n2018GQ0000020\n9\n1\n7319\n4\n6\n1013097\n34\n25\n...\n67\n0\n34\n34\n69\n0\n34\n35\n0\n0\n\n\n4\nP\n2018GQ0000027\n9\n1\n6511\n4\n6\n1013097\n46\n31\n...\n47\n81\n10\n11\n79\n47\n44\n81\n47\n10\n\n\n\n\n5 rows × 286 columns\n\n\n\nLet’s limit the scope of features which are relevant to our problem:\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\nRAC1P\nESR\n\n\n\n\n0\n30\n14.0\n1\n16\n2\nNaN\n1\n3.0\n4.0\n1\n1\n2\n2\n2.0\n1\n8\n6.0\n\n\n1\n18\n14.0\n5\n17\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n2\n69\n17.0\n1\n17\n1\nNaN\n1\n1.0\n2.0\n2\n1\n2\n2\n2.0\n1\n9\n6.0\n\n\n3\n25\n1.0\n5\n17\n1\nNaN\n1\n1.0\n4.0\n1\n1\n1\n2\n1.0\n1\n1\n6.0\n\n\n4\n31\n18.0\n5\n16\n2\nNaN\n1\n1.0\n4.0\n1\n1\n2\n2\n2.0\n2\n1\n6.0\n\n\n\n\n\n\n\nRAC1P is our data variable for race, and so that should be excluded from our set of predictor variables. ESR is our target variable for employment status. Let’s make a subset of features excluding these:\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\nNow we can construct a BasicProblem around our dataset and these different sets of features we have created. This problem defines our target (employment status) by transforming it from a categorical variable to a dichotemus one that is 1 if the individual is employed, and 0 for all other categories of employment and unemployment. It defines our grouping variable as the race of the individual (RAC1P).\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfor obj in [features, label, group]:\n  print(obj.shape)\n\n(378817, 15)\n(378817,)\n(378817,)\n\n\nNow that we have set up our features, label, and group we should perform our train test split:\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\nLet’s look through the data in dataframe form:\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\ndf.shape\n\n(303053, 17)\n\n\nThere are 303053 individuals who filled out the survey in California in the year of 2018.\nHow many of these individuals are employed?\n\ndf.head()\n\n\n\n\n\n\n\n\nAGEP\nSCHL\nMAR\nRELP\nDIS\nESP\nCIT\nMIG\nMIL\nANC\nNATIVITY\nDEAR\nDEYE\nDREM\nSEX\ngroup\nlabel\n\n\n\n\n0\n77.0\n16.0\n2.0\n0.0\n2.0\n0.0\n4.0\n1.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n2.0\n1\nTrue\n\n\n1\n29.0\n20.0\n5.0\n2.0\n2.0\n0.0\n3.0\n3.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n6\nTrue\n\n\n2\n60.0\n21.0\n1.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n2.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n\n\n3\n27.0\n19.0\n5.0\n13.0\n2.0\n0.0\n1.0\n1.0\n4.0\n4.0\n1.0\n2.0\n2.0\n2.0\n2.0\n1\nFalse\n\n\n4\n63.0\n23.0\n3.0\n0.0\n2.0\n0.0\n1.0\n1.0\n4.0\n1.0\n1.0\n2.0\n2.0\n2.0\n1.0\n1\nFalse\n\n\n\n\n\n\n\n\ndf.groupby(\"label\").size()\n\nlabel\nFalse    164678\nTrue     138375\ndtype: int64\n\n\nThere are 138,375 employed individuals in our data subset, and 164,678 unemployed individuals in our dataset.Let’s break that down by racial category.\n\ndf.groupby([\"group\", \"label\"]).size()\n\ngroup  label\n1      False    100725\n       True      85639\n2      False      8845\n       True       5702\n3      False      1371\n       True        895\n4      False        17\n       True          7\n5      False       410\n       True        307\n6      False     23987\n       True      23493\n7      False       517\n       True        459\n8      False     19069\n       True      16227\n9      False      9737\n       True       5646\ndtype: int64\n\n\nThe number of employed individuals who filled out the survey, based on racial category, are as follows: White: 85,639 Black/African American: 5,702 American Indian: 895 Alaska Native: 7 American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races: 307 Asian: 23,493 Native Hawaiian and Other Pacific Islander: 459 Any other race alone: 16,227 Two or More Races: 5,646\n\ntrue_group_prop = df.groupby(\"group\")[\"label\"].mean()\ntrue_group_prop\n\ngroup\n1    0.459525\n2    0.391971\n3    0.394969\n4    0.291667\n5    0.428173\n6    0.494798\n7    0.470287\n8    0.459740\n9    0.367029\nName: label, dtype: float64\n\n\nThe ratio of employed individuals in each racial category are as follows:\nWhite: 0.459525\nBlack/African American: 0.391971\nAmerican Indian: 0.394969\nAlaska Native: 0.291667\nAmerican Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races: 0.428173\nAsian: 0.494798\nNative Hawaiian and Other Pacific Islander: 0.470287\nAny other race alone: 0.459740\nTwo or More Races: 0.367029\n\nIntersectional Trends of Race, Gender, and Employment\nLet’s create a relational visualization of Employment based on Race and Gender:\n\nimport seaborn as sns\ndf_plot = df.groupby([\"group\", \"SEX\"])[\"label\"].mean().reset_index()\ndf_plot[\"SEX\"] = df_plot[\"SEX\"].replace({1: \"MALE\", 2: \"FEMALE\"})\n\n\np1 = sns.barplot(df_plot, x = \"group\", y = \"label\", hue = \"SEX\")\n\n\n\n\n\n\n\n\nWhile in most racial categories, Male employment rates (in our dataset) are higher than Female employment rates, there is a lot of variation. Other races, Asian, Alaskan Native, and White Male’s have especially higher levels of employment compared to their Female counterparts. Female’s outperform male’s in the Black/African American categories and the “American Indian and Alaska Native tribes specified, or American Indian or Alaska Native, not specified and no other races” category."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#training-the-model",
    "href": "posts/Auditing-bias-blog-post/index.html#training-the-model",
    "title": "Auditing Bias",
    "section": "Training the Model",
    "text": "Training the Model\nI chose to use a decision tree classifier for this model. In order to maximize the performance of this model, all max_depths from 1 to 20 are tested in order to identify the most accurate one. Accuracy in this sense is defined as the highest mean accuracy in a cross validation with cv = 5.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nmax_depth_best = 1\nCV_score_best = 0\nfor i in range(1, 20):\n    model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=i))\n    model.fit(X_train, y_train)\n    cv_scores_RF = cross_val_score(model, X_train, y_train, cv=5)\n    if cv_scores_RF.mean( ) &gt; CV_score_best:\n        max_depth_best = i\n        CV_score_best = cv_scores_RF.mean()\n\nprint(\"Best Maximum Depth: \", max_depth_best)\nprint(\"Cross Validation Mean: \", CV_score_best)\n\nBest Maximum Depth:  11\nCross Validation Mean:  0.8178305372286386\n\n\nWith the optimal maximum depth determined, let’s train our model with it, a standard scalar, and our training data. The pipeline first puts our data through a standard scalar. This scales our feature variables to all be on a standard scale so that features with a much larger scale don’t have a larger impact on our predictions. The scaled data is then fed through the pipeline to our decision tree classifier with the optimal maximum depth identified.\n\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth=max_depth_best))\nmodel.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('decisiontreeclassifier',\n                 DecisionTreeClassifier(max_depth=11))])\n\n\n\nCross Validation\nLet’s test our model using cross validation\n\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_RF = cross_val_score(model, X_train, y_train, cv=5)\ncv_scores_RF\n\narray([0.81867978, 0.81640296, 0.82061012, 0.81752186, 0.81598746])\n\n\nOur highest level of cross validation is achieved with a max depth of 11 in our Decision Classifier Tree. Our cross validation is scoring between .816 and .820 on all divisions."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#audit",
    "href": "posts/Auditing-bias-blog-post/index.html#audit",
    "title": "Auditing Bias",
    "section": "Audit",
    "text": "Audit\n\nOverall Accuracy\nLet’s see how our accurate our model performs on the test data:\n\ny_hat = model.predict(X_test)\ndf_audit = pd.DataFrame(X_test, columns = features_to_use)\ndf_audit[\"group\"] = group_test\ndf_audit[\"label\"] = y_test\ndf_audit[\"pred\"] = y_hat\n\n\n\n(y_hat == y_test).mean()\n\n0.8180270313077451\n\n\nOur overall testing accuracy is .818, which is in the same range as the values we observed in our cross validation. Knowing that our model is performing similar to how we expected based on the training data.\n\n\nOverall Positive Predictive Value\nLet’s calculate the value of our PPV:\n\ndf_audit[\"correct\"] = df_audit[\"label\"] == df_audit[\"pred\"]\ndf_audit.groupby(\"pred\")[\"correct\"].mean()\n\npred\nFalse    0.872495\nTrue     0.766095\nName: correct, dtype: float64\n\n\nThe model is more accurate at predicting unemployment than it is at predicting employment. Our positive predictive value (PPV) of .766 means that for all individuals predicted to be employed, the model was correct about that prediction 76.6% of the time.\n\n\nError Rate Balance\nNow Let’s calculate our overall error rate balance.\n\ndf_audit.groupby(\"label\")[\"pred\"].mean()\n\nlabel\nFalse    0.219470\nTrue     0.863048\nName: pred, dtype: float64\n\n\nThis shows us that the model predicted 86.3% of the employed individuals to be employed. It was similarly accurate on the unempoloyed—21.8% of unemployed individuals were incorrectly predicted to be employed. Our False positive rate is therefor 21.8% and our false negative rate is 13.7%.\n\n\nFNR and FPR\nSimilarly, we can demonstrate the overall False Negative and False Positive rates.\n\ndf_audit.groupby(\"label\")[\"pred\"].mean()\n\nlabel\nFalse    0.219470\nTrue     0.863048\nName: pred, dtype: float64\n\n\n\n\n\nTN = df_audit[(df_audit[\"label\"] == False) & (df_audit[\"pred\"] == False)].shape[0]\nFP = df_audit[(df_audit[\"label\"] == False) & (df_audit[\"pred\"] == True)].shape[0]\nFN = df_audit[(df_audit[\"label\"] == True) & (df_audit[\"pred\"] == False)].shape[0]\nTP = df_audit[(df_audit[\"label\"] == True) & (df_audit[\"pred\"] == True)].shape[0]\n\nFPR = FP / (FP + TN)\nFNR = FN / (FN + TP)\n\nFPR\n\n0.21946971163150764\n\n\nOur False Positive rate is .219. This means that for all individuals who are unemployed, 21.9% were predicted to be employed.\n\nFNR\n\n0.1369524805390961\n\n\nOur False Negative Rate (FNR) is .137. This means that for all individuals who are employed, 13.7% were predicted to be unemployed.\nThis implies that our model probably, although we have not statistically proven it, has a bias towards accuracy for individuals who are employed. Simply, our model is more likely to accurately predict an individuals employment status if they are employed than if they are unemployed.\nLet’s turn to an analysis of bias at the group level.\n\n\nGroup Level Audit\nLet’s see how the model performed when comparing performance on individuals of different races.\n\nAccuracy\n\ndf_audit.groupby(\"group\")[\"correct\"].mean()\n\ngroup\n1    0.818402\n2    0.819832\n3    0.794918\n4    0.750000\n5    0.768750\n6    0.808710\n7    0.767241\n8    0.815245\n9    0.857067\nName: correct, dtype: float64\n\n\nThe two groups which include alaskan natives, and pacific islanders have significantly lower accuracy than the other groups as seen above. The group with the highest accuracy is that for individuals who identify as more than 1 race.\n\n\nPPV — By group\nLet’s calculate the PPV again, but this time focussing on group level statistics.\n\nsufficiency_group = df_audit.groupby([\"pred\", \"group\"])[\"correct\"].mean().reset_index()\nsufficiency_group\n\n\n\n\n\n\n\n\npred\ngroup\ncorrect\n\n\n\n\n0\nFalse\n1\n0.867996\n\n\n1\nFalse\n2\n0.904984\n\n\n2\nFalse\n3\n0.910714\n\n\n3\nFalse\n4\n1.000000\n\n\n4\nFalse\n5\n0.789474\n\n\n5\nFalse\n6\n0.855773\n\n\n6\nFalse\n7\n0.859813\n\n\n7\nFalse\n8\n0.874412\n\n\n8\nFalse\n9\n0.927180\n\n\n9\nTrue\n1\n0.769908\n\n\n10\nTrue\n2\n0.726912\n\n\n11\nTrue\n3\n0.675277\n\n\n12\nTrue\n4\n0.666667\n\n\n13\nTrue\n5\n0.750000\n\n\n14\nTrue\n6\n0.772183\n\n\n15\nTrue\n7\n0.688000\n\n\n16\nTrue\n8\n0.761661\n\n\n17\nTrue\n9\n0.763060\n\n\n\n\n\n\n\nThe composite group for native north american’s have a significantly lower accuracy for negatively predicted values at 79%. This means that the model is more likely to predict individuals who are unemployed as employed from this group. It is interesting to not that this group does not have a corresponding different Positive predictive value. Instead, Alaskan Natives have the lowest PPV at .667. This means that alaskan natives, who are predicted to be employed are only employed 66.67% of the time. For both the NPV and PPV, their is significant variation among groups. While it is not perfect, I would say that this model is not horribly calibrated, especially when compared to it’s error rate balance below.\n\n\n\nError Rate Balance — By Group\n\ndf_audit.groupby([\"label\", \"group\"])[\"pred\"].mean()\n\nlabel  group\nFalse  1        0.213280\n       2        0.216629\n       3        0.256560\n       4        0.500000\n       5        0.259259\n       6        0.255399\n       7        0.297710\n       8        0.231342\n       9        0.160084\nTrue   1        0.856422\n       2        0.875171\n       3        0.879808\n       4        1.000000\n       5        0.797468\n       6        0.873391\n       7        0.851485\n       8        0.870073\n       9        0.886561\nName: pred, dtype: float64\n\n\nThe False FPR and TPRs shown above demonstrate that once again our model is treating different groups very differently. The range for FPR are from .16 for individuals of 2 or more races up to .5 for Alaskan Natives. Our model is very likely to predict Alaskan Native’s as being employed, with 100% of employed individuals accurately predicted and 50% of unemployed individuals predicted as employed as well. Most other race categories are in similar ranges of .21 - .26 for FPV and .86-.88 for TPV values. The group with the lowest FPR are those from 2 or more races with a FPR of .16 and a TPR of .89. Similarly with the PPVs, the group 5, for individuals from Alaskan Native or American Indian tribes not specified, the TPR is the lowest of all the groups at .797. Based on these numbers it is clear that our model is not quite calibrated. Certain groups clearly have been scrutinized much more by the data, and it is hard to predict what the negative consequences of a model like this that is not well calibrated might be. Particularly, the treatment of the group 5 worries me, as that group and Alaskan Natives are consistently treated differently from the other groups in terms of Error Rate Balance and PPV. While Black and White groups are balanced quite well, it is concerning to me that other, likely smaller and even more disadvantaged groups are not similarly balanced.\n\n\nStatistical Parity\nLet’s see how race influences the chances of being categorized as employed:\n\ndf_audit.groupby(\"group\")[\"pred\"].mean().reset_index()\n\n\n\n\n\n\n\n\ngroup\npred\n\n\n\n\n0\n1\n0.505611\n\n\n1\n2\n0.478190\n\n\n2\n3\n0.491833\n\n\n3\n4\n0.750000\n\n\n4\n5\n0.525000\n\n\n5\n6\n0.563024\n\n\n6\n7\n0.538793\n\n\n7\n8\n0.524757\n\n\n8\n9\n0.427205\n\n\n\n\n\n\n\nOur data shows that the model does not reach statistical parity. The range of predicted proportions of employed individuals across groups range from .75 to .427. While we have not proven that this is statistically significant, the difference is large enough that I highly doubt it would not be.\n\n\nFigure\nLet’s make a figure to explore the FPR and FNR.\nFirst lets make a function to get the rates by group and store them.\n\ndef get_rates(group):\n    FP = sum((group[\"pred\"] == 1) & (group[\"label\"] == 0))\n    FN = sum((group[\"pred\"] == 0) & (group[\"label\"] == 1))\n    TN = sum((group[\"pred\"] == 0) & (group[\"label\"] == 0))\n    TP = sum((group[\"pred\"] == 1) & (group[\"label\"] == 1))\n    FPR = FP / (FP + TN) if FP + TN &gt; 0 else 0\n    FNR = FN / (FN + TP) if FN + TP &gt; 0 else 0\n    return pd.Series({\"FPR\": FPR, \"FNR\": FNR})\n\nNow lets pass in the data to get the rates for each group, and lets make a color palette for our visualization.\n\nrates = df_audit.groupby(\"group\").apply(get_rates).reset_index()\nrates\n\n\n\n\n\n\n\n\n\n\n\n\ngroup\nFPR\nFNR\n\n\n\n\n0\n1\n0.213280\n0.143578\n\n\n1\n2\n0.216629\n0.124829\n\n\n2\n3\n0.256560\n0.120192\n\n\n3\n4\n0.500000\n0.000000\n\n\n4\n5\n0.259259\n0.202532\n\n\n5\n6\n0.255399\n0.126609\n\n\n6\n7\n0.297710\n0.148515\n\n\n7\n8\n0.231342\n0.129927\n\n\n8\n9\n0.160084\n0.113439\n\n\n\n\n\n\n\nWe will use the PPV of 0.666667, as it falls close to the middle of the existing values of our model for all groups. We can calculate each groups prevalence of employment by taking the mean of the label group.\nNow we can pass the data to a plot to create our visualization. This plot will include a scatter plot of the observed values, and lines showing the theoretical values.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nPPV = 0.666667\np = df_audit.groupby(\"group\")[\"label\"].mean().reset_index()\ndef calc_FPR(FNR, PPV, prev):\n    return ((prev)/(1-prev))*((1-PPV)/(PPV))*(1-FNR)\n\ndf_list = []\nfor i, row in p.iterrows():\n    FNR_vals = np.linspace(0, 1, 100)\n    FPR_vals = calc_FPR(FNR_vals, PPV, row[\"label\"])\n    df_FC = pd.DataFrame({\"FNR\": FNR_vals, \"FPR\": FPR_vals, \"group\": row[\"group\"]})\n    df_list.append(df_FC)\n\ndf_FC = pd.concat(df_list)\ndf_FC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFNR\nFPR\ngroup\n\n\n\n\n0\n0.000000\n0.416650\n1.0\n\n\n1\n0.010101\n0.412441\n1.0\n\n\n2\n0.020202\n0.408232\n1.0\n\n\n3\n0.030303\n0.404024\n1.0\n\n\n4\n0.040404\n0.399815\n1.0\n\n\n...\n...\n...\n...\n\n\n95\n0.959596\n0.011748\n9.0\n\n\n96\n0.969697\n0.008811\n9.0\n\n\n97\n0.979798\n0.005874\n9.0\n\n\n98\n0.989899\n0.002937\n9.0\n\n\n99\n1.000000\n0.000000\n9.0\n\n\n\n\n900 rows × 3 columns\n\n\n\n\npalette = [\n    \"#1f77b4\",  # Muted blue\n    \"#ff7f0e\",  # Safety orange\n    \"#2ca02c\",  # Cooked asparagus green\n    \"#d62728\",  # Brick red\n    \"#9467bd\",  # Muted purple\n    \"#8c564b\",  # Chestnut brown\n    \"#e377c2\",  # Raspberry yogurt pink\n    \"#7f7f7f\",  # Middle gray\n    \"#bcbd22\"   # Curry yellow-green\n]\n\ngroup_colors = {group: color for group, color in zip(df_FC['group'].unique(), palette)}\n\nfig, ax = plt.subplots(1,1,figsize=(10, 6))\nfor group, color in group_colors.items():\n    df_group = df_FC[df_FC['group'] == group]\n    df_rates = rates[rates['group'] == group]\n    ax.scatter(x=df_rates[\"FNR\"],y =df_rates[\"FPR\"],  color=color, label=f\"Scatter {group}\")\n    ax.plot(df_group[\"FNR\"], df_group[\"FPR\"],color=color, label=f\"Line {group}\")\n\nplt.ylabel(\"False Positive Rate\")\nplt.xlabel(\"False Negative Rate\")\nplt.title(\"Feasible (FNR, FPR) Combinations\")\nplt.suptitle(\"PPV = 0.666667\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nIn order, for example, to make our FPR for Native Hawaiian or Pacific Islander (group 7) to match our FPR for white individuals (group 1), we would need to increase the false negative rate for group 7 to about .5. This would significantly reduce our accuracy, and would not necessarily be beneficial for either group involved. For a group like alaskan native (group 4) we would have to accept a FNR of between .6 and .7. Either of these changes would probably result in a significant reduction in overall accuracy and not result in a more equal model."
  },
  {
    "objectID": "posts/Auditing-bias-blog-post/index.html#discussion",
    "href": "posts/Auditing-bias-blog-post/index.html#discussion",
    "title": "Auditing Bias",
    "section": "Discussion",
    "text": "Discussion\n\nWhat groups of people could stand to benefit from a system that is able to predict the label you predicted, such as income or employment status? For example, what kinds of companies might want to buy your model for commercial use?\nBased on your bias audit, what could be the impact of deploying your model for large-scale prediction in commercial or governmental settings?\nBased on your bias audit, do you feel that your model displays problematic bias? What kind (calibration, error rate, etc)?\nBeyond bias, are there other potential problems associated with deploying your model that make you uncomfortable? How would you propose addressing some of these problems?\n\nThe government might be interested in a model for predicting employment status in order to deliver aid to communities most in need. On the other hand, any commercial entity that requires economic trust in an individual(credit card companies, banks, land loards) might find it beneficial to be able to predict the employment status of individuals if they cannot sift through all applications they are receiving for their service.\nOur model would result in significant disparities in the ways in which different groups are treated. Most interestingly, the Alaskan Native population had a FPR of .5 and a FNR of 0. While such a low FNR might seem great, the tradeoff is that individuals are quite often being identified as employed when they are in-fact not. Even this seemingly beneficial categorization could have a very negative implication for the individual such as not receiving necessary aid, or being given a loan they cannot afford.\nMy model exhibits significant Error Rate balance issues, such as the one mentioned in the paragraph above. It is especially concerning to me how differently the various groups are being treated in this category, with FPR ranging from 0 to 21% and FNR ranging from 50 to 16%. Its error rate balance, while still exhibiting some levels of bias is much less concerning than the error rate balance.\nI generally do not feel comfortable deploying models that have to do with classifying individuals. I believe especially strongly about this when it comes to government use of these models. If governments are supposed to treat individuals equally, it makes no sense to allow them to treat you differently based on similarities between group categorizations. I think individuals deserve to be reviewed as an individual."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html",
    "href": "posts/Sparse-Kernel-Machines/index.html",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "This blog post explores how using a sparse kernel method and how different lambda and gamma values impact the fitting of machine learning methods. Adjusting each of these values impacts the fit and potential overfit of the model. Gamma determines how many of the closest points are taken into account in the classification of each other point, while lambda determines the strength of the regularization of the model—how many or few weights will be forced to be zero.\n\n%load_ext autoreload\n%autoreload 2\nfrom sparse_kernel_logistic import KernelLogisticRegression, GradientDescentOptimizer\n\n\nimport torch\nimport matplotlib.pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\n\nLet’s generate some data that is mostly linearly separable:\n\nimport torch \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.random.seed(123)\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Basic Classification Data\")\n\nText(0.5, 0.98, 'Basic Classification Data')\n\n\n\n\n\n\n\n\n\n\n\nUse this link to access the code for our Sparse Kernel Machine.\nFor this blog post we will be looking at the use of the rbf_kernel defined in the following code block.\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n\nTo start, let’s explore how the model is classified using a relatively low lambda of 0.1 and a relatively low gamma of 1. The low lambda will mean that more points will have non-zero weights, but the medium level of gamma means that each point will influence a larger region during classification\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.1700)\n\n\nLet’s define a function to plot decision boundaries as well as highlighting those points with non-zero values for their weights.\n\ndef plot_decision_boundary(KR, X, y, ax = None, highlight = True):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    x1 = X1.ravel()\n    x2 = X2.ravel()\n\n    X_ = torch.stack((x1, x2), dim = 1)\n\n    preds = KR.score(X_, recompute_kernel = True)\n    preds = 1.0*torch.reshape(preds, X1.size())\n\n\n    ax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \n    vmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n    )\n    plot_classification_data(X, y, ax)\n    if highlight:\n        ax.scatter(KR.Xt[ix, 0].detach(), KR.Xt[ix, 1].detach(), \n            facecolors=\"none\", edgecolors=\"black\", linewidths=1.5)\n\n\n\n\nplot_decision_boundary(KR, X, y, ax=None)\nplt.title(\"Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\\(\\lambda\\) is very large, there may be only one point in the training data with weight distinguishable from zero.\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 5, gamma = 1)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_decision_boundary(KR, X, y, ax)\nplt.title(\"Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\nAs we can see in the above visualization, when lambda is high, most of the weights for the points are regularized down to zero. Only a couple of points at most have non-zero weights.\n\n\n\nWith lambda reset to a normal value, and gamma made to a very high bandwidth/value of 10 we will see more points with non-zero weights, as each point has a very local decision region: each point only looks at the points very close to it for classification.\n\nKR1 = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n\nKR1.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\nplot_decision_boundary(KR1, X, y)\nplt.title(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$')\n\n\n\n\n\n\n\n\n\nAs we can see from the large quantity of highlighted plots, as well as the wavy decision boundaries, when gamma is high the decision regions become highly localized to nearby plots.\n\n\n\nLet’s use the Moons from Sklearn to generate non-linear data:\n\nfrom sklearn.datasets import make_moons\n\nX_np, y_np = make_moons(200, shuffle=True, noise=0.3)\n\n#convert to torch tensors\nX = torch.tensor(X_np, dtype=torch.float32)\ny = torch.tensor(y_np, dtype=torch.float32)\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.01)\n\n\nplot_decision_boundary(KR, X, y, ax = None)\nplt.suptitle(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 5$\")\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nBy adjusting the gamma value to be quite high, while leaving the lambda value relatively low, we enable the model to classify non-linearly separable data pretty well. However, the decision regions we have here are highly localized and therefore at risk of overfitting-a phenomenon we will explore more in Part B. The localized nature can be seen in the wavy decision boundry, and high number of points with non-zero weights. This is compounded by the fact that we have a low regularization/lambda value so few points have non-zero weights.\n\n\n\n\nSparse Kernel Machines give us a lot of flexibility in how the data will be fit by our models, but this added control means that we have a high risk of overfitting our model on our training data. Let’s explore this possibility further. Let’s define a function to generate data, and use that function to generate a train and test set:\n\ndef make_moons_train_test(n, shuffle=True, noise=0.4):\n    X, y = make_moons(n, shuffle=shuffle, noise=noise)\n    X = torch.tensor(X, dtype=torch.float32)\n    y = torch.tensor(y, dtype=torch.float32)\n    return X, y\n\n#convert to torch tensors\nX_train, y_train = make_moons_train_test(50, shuffle=True)\nX_test, y_test = make_moons_train_test(50, shuffle=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Test Data\")\nfig.suptitle(\"Overfitting: Nonlinearly Separable\", fontsize=16)\n\n\n\nText(0.5, 0.98, 'Overfitting: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nAs we can see our training and test data have similar trends, but also significant variation. if we fit the training data too closely we will most likely overfit the model.\nLet’s making a training loop so that we can track accuracy of train and test data at each iteration.\n\ndef train_model(model, num_steps, X_train, y_train, X_test, y_test, lr):\n    model.Xt = X_train\n\n    if model.a is None:\n        model.a = torch.zeros(X_train.shape[0], requires_grad=False)\n\n\n    # instantiate an optimizer -- gradient descent today\n    opt = GradientDescentOptimizer(model)\n\n    # collect the values of the loss in each step\n    acc_train_vec = []\n    acc_test_vec   = []\n\n\n\n    for i in range(num_steps): \n                \n        opt.step(X_train, y_train, lr=lr)  \n\n        # for tracking model progress on the training set\n        train_acc = (model.predict(X_train) == y_train).float().mean()   \n        acc_train_vec.append(train_acc.item())   \n\n        test_acc = (model.predict(X_test) == y_test).float().mean()\n        acc_test_vec.append(test_acc.item())\n\n    return acc_train_vec, acc_test_vec\n\n\n# KR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n# loss_train, loss_test = train_model(model = KR, num_steps=5000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)\n\nThe following experiment will track each step in the fitting of the model to our training data, recording the models accuracy on the training and testing data at each step. In doing so we can track the performance of the model with each iteration.\n\ndef plot_experiment(model):\n    fig, ax = plt.subplots(1, 3, figsize = (9, 3))\n    acc_train, acc_test = train_model(model = model, num_steps=50000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)  \n    ax[0].plot(acc_train, c = \"steelblue\", label = \"Training\")\n    ax[0].plot(acc_test, c = \"goldenrod\", label = \"Testing\")\n    ax[0].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n    ax[0].legend()\n    ax[0].set_title(\"Training and Testing Accuracy vs Iteration\")\n    ax[1].set_title(\"Training Data\")\n    ax[2].set_title(\"Testing Data\")    \n\n    plot_decision_boundary(model, X_train, y_train, ax = ax[1], highlight=True)  \n    plot_decision_boundary(model, X_test, y_test, ax = ax[2], highlight=False)   \n    plt.tight_layout()\n\n\nNow let’s plot our findings.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\nplot_experiment(KR)\n\n\n\n\n\n\n\n\nFascinating. As we can see in the first plot, our accuracy begins relatively high for both our testing and training data, but decreases on the testing data as more iterations occur. This makes sense when looking at the decision regions. We have highly localized decision regions built around the training data, that does not fit the variation of the testing data.\n\n\naccuracy = (KR.predict(X_train) == y_train).float().mean()\nprint(f\"Train accuracy: {accuracy:.2f}\")\n\nTrain accuracy: 0.94\n\n\nThe result of this is that we have a very high training accuracy, but this is not a good thing.\n\naccuracy = (KR.predict(X_test) == y_test).float().mean()\nprint(f\"Test accuracy: {accuracy:.2f}\")\n\nTest accuracy: 0.80\n\n\nOur testing accuracy is significantly worse.\n\n\n\nAs we have demonstrated in the blog post, using sparse kernel machines gives significantly more control over how our models are fit. With control of the bandwidth (gamma), or the relationship between the distance between two points and the similarity of them, and lambda, the regularization of the weights, we can get very high accuracy when training models. This can even be done on non-linearly separable data. But this comes with risks. If we fit the data too much on the training data, then variation in training and testing data will result in overfitting. This was demonstrated in Part B, where we showed that a highly localized and non-normalized training of non linear data with a decent amount of noise will result in a poor performance when tresting our model.\nAll of this blog post demonstrates the importance of cross validation for the selection of parameters during training in order to prevent overfitting. But cross validation requires more time, energy, and computing power."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#data-generation",
    "href": "posts/Sparse-Kernel-Machines/index.html#data-generation",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "Let’s generate some data that is mostly linearly separable:\n\nimport torch \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nnp.random.seed(123)\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Basic Classification Data\")\n\nText(0.5, 0.98, 'Basic Classification Data')\n\n\n\n\n\n\n\n\n\n\n\nUse this link to access the code for our Sparse Kernel Machine.\nFor this blog post we will be looking at the use of the rbf_kernel defined in the following code block.\n\ndef rbf_kernel(X_1, X_2, gamma):\n    return torch.exp(-gamma*torch.cdist(X_1, X_2)**2)\n\n\nTo start, let’s explore how the model is classified using a relatively low lambda of 0.1 and a relatively low gamma of 1. The low lambda will mean that more points will have non-zero weights, but the medium level of gamma means that each point will influence a larger region during classification\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.1700)\n\n\nLet’s define a function to plot decision boundaries as well as highlighting those points with non-zero values for their weights.\n\ndef plot_decision_boundary(KR, X, y, ax = None, highlight = True):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n    ix = torch.abs(KR.a) &gt; 0.001\n\n    x1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    x1 = X1.ravel()\n    x2 = X2.ravel()\n\n    X_ = torch.stack((x1, x2), dim = 1)\n\n    preds = KR.score(X_, recompute_kernel = True)\n    preds = 1.0*torch.reshape(preds, X1.size())\n\n\n    ax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \n    vmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n    )\n    plot_classification_data(X, y, ax)\n    if highlight:\n        ax.scatter(KR.Xt[ix, 0].detach(), KR.Xt[ix, 1].detach(), \n            facecolors=\"none\", edgecolors=\"black\", linewidths=1.5)\n\n\n\n\nplot_decision_boundary(KR, X, y, ax=None)\nplt.title(\"Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 0.1$ and $\\\\gamma = 1$')"
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#basic-experiments",
    "href": "posts/Sparse-Kernel-Machines/index.html#basic-experiments",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "When ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍\\(\\lambda\\) is very large, there may be only one point in the training data with weight distinguishable from zero.\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = 5, gamma = 1)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\n\nfig, ax = plt.subplots(1, 1)\nplot_decision_boundary(KR, X, y, ax)\nplt.title(\"Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = 5$ and $\\\\gamma = 1$')\n\n\n\n\n\n\n\n\n\nAs we can see in the above visualization, when lambda is high, most of the weights for the points are regularized down to zero. Only a couple of points at most have non-zero weights.\n\n\n\nWith lambda reset to a normal value, and gamma made to a very high bandwidth/value of 10 we will see more points with non-zero weights, as each point has a very local decision region: each point only looks at the points very close to it for classification.\n\nKR1 = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n\nKR1.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\n\nplot_decision_boundary(KR1, X, y)\nplt.title(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$\")\n\nText(0.5, 1.0, 'Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 10$')\n\n\n\n\n\n\n\n\n\nAs we can see from the large quantity of highlighted plots, as well as the wavy decision boundaries, when gamma is high the decision regions become highly localized to nearby plots.\n\n\n\nLet’s use the Moons from Sklearn to generate non-linear data:\n\nfrom sklearn.datasets import make_moons\n\nX_np, y_np = make_moons(200, shuffle=True, noise=0.3)\n\n#convert to torch tensors\nX = torch.tensor(X_np, dtype=torch.float32)\ny = torch.tensor(y_np, dtype=torch.float32)\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 5)\n\nKR.fit(X, y, m_epochs = 500000, lr = 0.01)\n\n\nplot_decision_boundary(KR, X, y, ax = None)\nplt.suptitle(\"Decision boundary with $\\\\lambda = .1$ and $\\\\gamma = 5$\")\nplt.title(\"Moons Dataset: Nonlinearly Separable\")\n\nText(0.5, 1.0, 'Moons Dataset: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nBy adjusting the gamma value to be quite high, while leaving the lambda value relatively low, we enable the model to classify non-linearly separable data pretty well. However, the decision regions we have here are highly localized and therefore at risk of overfitting-a phenomenon we will explore more in Part B. The localized nature can be seen in the wavy decision boundry, and high number of points with non-zero weights. This is compounded by the fact that we have a low regularization/lambda value so few points have non-zero weights."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#part-b-demonstrating-overfitting",
    "href": "posts/Sparse-Kernel-Machines/index.html#part-b-demonstrating-overfitting",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "Sparse Kernel Machines give us a lot of flexibility in how the data will be fit by our models, but this added control means that we have a high risk of overfitting our model on our training data. Let’s explore this possibility further. Let’s define a function to generate data, and use that function to generate a train and test set:\n\ndef make_moons_train_test(n, shuffle=True, noise=0.4):\n    X, y = make_moons(n, shuffle=shuffle, noise=noise)\n    X = torch.tensor(X, dtype=torch.float32)\n    y = torch.tensor(y, dtype=torch.float32)\n    return X, y\n\n#convert to torch tensors\nX_train, y_train = make_moons_train_test(50, shuffle=True)\nX_test, y_test = make_moons_train_test(50, shuffle=True)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 5))\nplot_classification_data(X_train, y_train, ax[0])\nax[0].set_title(\"Training Data\")\nplot_classification_data(X_test, y_test, ax[1])\nax[1].set_title(\"Test Data\")\nfig.suptitle(\"Overfitting: Nonlinearly Separable\", fontsize=16)\n\n\n\nText(0.5, 0.98, 'Overfitting: Nonlinearly Separable')\n\n\n\n\n\n\n\n\n\nAs we can see our training and test data have similar trends, but also significant variation. if we fit the training data too closely we will most likely overfit the model.\nLet’s making a training loop so that we can track accuracy of train and test data at each iteration.\n\ndef train_model(model, num_steps, X_train, y_train, X_test, y_test, lr):\n    model.Xt = X_train\n\n    if model.a is None:\n        model.a = torch.zeros(X_train.shape[0], requires_grad=False)\n\n\n    # instantiate an optimizer -- gradient descent today\n    opt = GradientDescentOptimizer(model)\n\n    # collect the values of the loss in each step\n    acc_train_vec = []\n    acc_test_vec   = []\n\n\n\n    for i in range(num_steps): \n                \n        opt.step(X_train, y_train, lr=lr)  \n\n        # for tracking model progress on the training set\n        train_acc = (model.predict(X_train) == y_train).float().mean()   \n        acc_train_vec.append(train_acc.item())   \n\n        test_acc = (model.predict(X_test) == y_test).float().mean()\n        acc_test_vec.append(test_acc.item())\n\n    return acc_train_vec, acc_test_vec\n\n\n# KR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\n# loss_train, loss_test = train_model(model = KR, num_steps=5000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)\n\nThe following experiment will track each step in the fitting of the model to our training data, recording the models accuracy on the training and testing data at each step. In doing so we can track the performance of the model with each iteration.\n\ndef plot_experiment(model):\n    fig, ax = plt.subplots(1, 3, figsize = (9, 3))\n    acc_train, acc_test = train_model(model = model, num_steps=50000, X_train = X_train, y_train = y_train, X_test = X_test, y_test = y_test, lr = 0.01)  \n    ax[0].plot(acc_train, c = \"steelblue\", label = \"Training\")\n    ax[0].plot(acc_test, c = \"goldenrod\", label = \"Testing\")\n    ax[0].set(xlabel = \"Iteration\", ylabel = \"Accuracy\")\n    ax[0].legend()\n    ax[0].set_title(\"Training and Testing Accuracy vs Iteration\")\n    ax[1].set_title(\"Training Data\")\n    ax[2].set_title(\"Testing Data\")    \n\n    plot_decision_boundary(model, X_train, y_train, ax = ax[1], highlight=True)  \n    plot_decision_boundary(model, X_test, y_test, ax = ax[2], highlight=False)   \n    plt.tight_layout()\n\n\nNow let’s plot our findings.\n\nKR = KernelLogisticRegression(rbf_kernel, lam = .1, gamma = 10)\nplot_experiment(KR)\n\n\n\n\n\n\n\n\nFascinating. As we can see in the first plot, our accuracy begins relatively high for both our testing and training data, but decreases on the testing data as more iterations occur. This makes sense when looking at the decision regions. We have highly localized decision regions built around the training data, that does not fit the variation of the testing data.\n\n\naccuracy = (KR.predict(X_train) == y_train).float().mean()\nprint(f\"Train accuracy: {accuracy:.2f}\")\n\nTrain accuracy: 0.94\n\n\nThe result of this is that we have a very high training accuracy, but this is not a good thing.\n\naccuracy = (KR.predict(X_test) == y_test).float().mean()\nprint(f\"Test accuracy: {accuracy:.2f}\")\n\nTest accuracy: 0.80\n\n\nOur testing accuracy is significantly worse."
  },
  {
    "objectID": "posts/Sparse-Kernel-Machines/index.html#discussion",
    "href": "posts/Sparse-Kernel-Machines/index.html#discussion",
    "title": "Sparse Kernel Logistic Regression",
    "section": "",
    "text": "As we have demonstrated in the blog post, using sparse kernel machines gives significantly more control over how our models are fit. With control of the bandwidth (gamma), or the relationship between the distance between two points and the similarity of them, and lambda, the regularization of the weights, we can get very high accuracy when training models. This can even be done on non-linearly separable data. But this comes with risks. If we fit the data too much on the training data, then variation in training and testing data will result in overfitting. This was demonstrated in Part B, where we showed that a highly localized and non-normalized training of non linear data with a decent amount of noise will result in a poor performance when tresting our model.\nAll of this blog post demonstrates the importance of cross validation for the selection of parameters during training in order to prevent overfitting. But cross validation requires more time, energy, and computing power."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html",
    "href": "posts/Logistic-Regression/index.html",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "This blog post explores the Logistic Regression Machine Learning model. By working through vanilla gradient descent, Gradient Descent with momentum, and then overfitting. These concepts are then applied to the Wisconsin Brest cancer dataset in order to practice building a machine learning pipeline without overfitting while optimizing learning and momentum rates.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nimport torch\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\nUse this link to access the Logistic Regression source code.\n\n\n\nLet’s generate some data with a minimal amount of noise.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Classification Data\")\n\nText(0.5, 0.98, 'Classification Data')\n\n\n\n\n\n\n\n\n\nMake a function to plot decision regions:\n\ndef plot_decision_boundary(model, X, y, ax=None, highlight=False):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    grid = torch.stack([X1.ravel(), X2.ravel()], dim=1)\n\n    # Append column of ones to match model input shape (for intercept)\n    grid = torch.cat([grid, torch.ones(grid.shape[0], 1)], dim=1)\n\n    preds = model.score(grid).reshape(X1.shape)\n\n    # Decision boundary plot\n    ax.contourf(X1, X2, preds, levels=50, cmap=\"BrBG\", alpha=0.8)\n    ax.contour(X1, X2, preds, levels=[0.0], colors='black', linewidths=2)\n\n    # Plot original data\n    plot_classification_data(X, y, ax)\n\n\n\n\n\n\nFor our frist itteration, we will train our model with a learning rate (alpha) of .1, and a beta of 0. This means we are performing a logistic regression without momentum.\n\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(10000):\n    loss_vec.append(LR.loss(X, y).item())\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n\n\n\nWith our training complete, lets plot the loss over time\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\nplot_decision_boundary(LR, X, y)\n\n\n\n\n\n\n\n\nA Logistic regression, as demonstrated by the graph above, is going to be limited in the loss it can achieve by the noise within the data. There is no line that would perfectly separate this data without any feature mapping.\n\n\n\nNow let’s train our model on the same data, but this time we will use a beta of .9.\n\n# instantiate a model and an optimizer\nLR_momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\n# for keeping track of loss values\nloss_vec_momentum = []\n\nfor _ in range(10000):\n    loss_vec_momentum.append(LR_momentum.loss(X, y).item())\n    opt_momentum.step(X, y, alpha = 0.1, beta = 0.9)\n\nWith the model trained, let’s plot the two loss vectors on the same plot to compare.\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\nplt.xlim(0, 100)\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see from the above graph, with momentum our model reaches the minimum loss (Variance/noise of the date) much faster. In-fact, within 100 iterations, the without momentum model does not reach the variance of the data.(minimum loss)\nHowever, if given enough time, both models will converge on the minimum loss:\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\n\n\n\nNow we are going to generate a new set of data that. Let’s generate 2 sets of data with 2 times as many p_dims as n_points\n\n\nX_train, y_train = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\nX_test, y_test = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\n\n\nWe will now use our X_train and y_train data to train a model to 100% accuracy in order to demonstrate the concept of overfitting.\n\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\n\n\nloss_vec_train = []\nfor _ in range(100):\n    loss = LR_overfit.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt_overfit.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\nprint(\"Final training loss: \", loss_vec_train[-1])\n\nFinal training loss:  0.0025839933659881353\n\n\nWith our Gradient Descent with Momentum optimization complete, let’s look at how the loss changed over time:\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"blue\")\nplt.title(\"Loss overfitting on training data\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see, our loss approached 0 within a very few number of gradient descent iterations. Now let’s test our accuracy:\n\ntrain_acc = ((LR_overfit.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Training accuracy: \", train_acc)\n\nTraining accuracy:  1.0\n\n\nGreat, our training accuracy is 100% as expected! Let’s test the accuracy on the test data.\n\ntest_acc = ((LR_overfit.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy: \", test_acc)\n\nTest accuracy:  0.8799999952316284\n\n\nHmmm, so it seems that we have over-fit our model quite considerably. Our test accuracy is ~.16 worse than our training accuracy of 1.\n\n\n\n\nThe data from load_breast_cancer dataset is from the Wisconsin (diagnostic) breast cancer dataset. The features in this data were calculated from imaging of breast masses, and describe characteristics of the cell in the image.\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\nThis dataset has a unique identifier (id), 28 anonymized features about the transaction, the transaction amount, and the class. The class is 1 if the transaction is fraudulent and 0 if not.\n\n\nNow we split our data into Training, Validation, and Testing datasets using train_test_split frm sklearn. Data is split 60%, 20%, 20% respectively:\n\nfrom sklearn.model_selection import train_test_split\n# split train and test .6 and .4\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# splite temp into validation and test sets at .5 and .5\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nIn order to use this data on our Logistic Regression model we need to convert it into torch.Tensor’s:\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)  # or float32 for regression\n\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n\n\nX_train.size()\n\ntorch.Size([341, 30])\n\n\nNow we can train our Logisitc Regression Model on this data:\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\n\nfor _ in range(3500):\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = 0.00005, beta = 0)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, our loss after 3500 iterations with a learning rate of 0.0005 and a beta of 0 levels out around a loss of 1.\nLet’s see how it performs with momentum:\n\nLRm = LogisticRegression()\nopt_m = GradientDescentOptimizer(LRm)\nloss_vec_train_m = []\n\nfor _ in range(3500):\n    loss = LRm.loss(X_train, y_train).item()\n    loss_vec_train_m.append(loss)\n    opt_m.step(X_train, y_train, alpha = 0.00005, beta = 0.9)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train_m)+1), loss_vec_train_m, color = \"blue\")\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nplt.legend([\"With momentum\", \"Without momentum\"])\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nOur model reaches a lower loss on the training data when using momentum. Even though it does not make constant progress, it reaches a lower loss in fewer iterations. It is interesting to note how low the learning rate (alpha) had to be in order for this model to reach a good loss.\nLets test this model, with momentum on our training data:\n\ntrain_acc = ((LRm.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy with momentum: \", train_acc)\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", train_acc)\n\nTest accuracy with momentum:  0.9149560332298279\nTest accuracy without momentum:  0.8504399061203003\n\n\nAdding momentum gives us a slightly more accurate model on the training data, so we will use momentum going forward.\n\n\n\nIf we use a more robust method than above however, we can probably improve our accuracy by finding the best momentum (beta) and learning rate (alpha). We do this below by selecting samples of alphas and betas and finding the best combination.\n\nlearning_rates = [\n    1e-6, 3e-6,\n    1e-5, 3e-5,\n    1e-4, 3e-4,\n    1e-3, 3e-3,\n    1e-2, 3e-2,\n    \n]\nbetas = [\n    0.0, 0.1,\n    0.3, 0.5,\n    0.7, 0.9,\n    0.99, 0.999,\n]\n\nbest_loss = 1e10\nbest_alpha = None\nbest_beta = None\n\nfor beta in betas:\n    best_loss_per_beta = 1e10\n    best_alpha_per_beta = None\n    beta_loss_vec = []\n    for alpha in learning_rates:\n        LR = LogisticRegression()\n        opt = GradientDescentOptimizer(LR)\n        loss_vec_train = []\n        for _ in range(400):\n            loss = LR.loss(X_train, y_train).item()\n            loss_vec_train.append(loss)\n            \n            opt.step(X_train, y_train, alpha = alpha, beta = 0.9)\n        if loss_vec_train[-1] &lt; best_loss:\n            best_loss_per_beta = loss_vec_train[-1]\n            best_alpha_per_beta = alpha\n        \n    if best_loss_per_beta &lt; best_loss:\n        best_loss = best_loss_per_beta\n        best_alpha = best_alpha_per_beta\n        best_beta = beta\n      \n\n\nprint(\"Best learning rate: \", best_alpha)\nprint(\"Best beta: \", best_beta)\nprint(\"Best loss: \", best_loss)\n\nBest learning rate:  0.0003\nBest beta:  0.999\nBest loss:  0.6386049389839172\n\n\nIn validation, we are able to find our best learning rate and best beta to minimize our loss. Let’s now train our model on the training data, and we will track the loss on the validation and training data overtime so we can compare them.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\nloss_vec_val = []\nfor _ in range(400):\n    loss = LR.loss(X_train, y_train).item()\n    loss_val = LR.loss(X_val, y_val).item()\n    loss_vec_val.append(loss_val)\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = best_alpha, beta = best_beta)\n    plt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\n    plt.plot(torch.arange(1, len(loss_vec_val)+1), loss_vec_val, color = \"blue\")\nplt.title(\"Training and Validation Loss Using Gradient Descent With Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\nplt.legend([\"Training loss\", \"Validation loss\"])\n\n\n\n\n\n\n\n\nSurprisingly, our model performs better on the validation data than on the training data. Let’s check our accuracy on the validation data with our best alpha and beta values:\n\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Train accuracy: \", train_acc)\n\nval_acc = ((LR.predict(X_val) == y_val)*1.0).mean().item()\n\nprint(\"Validation accuracy: \", val_acc)\n\n\nTrain accuracy:  0.8914955854415894\nValidation accuracy:  0.9385964870452881\n\n\nAs we can see our model did even better on the validation data than on the training data. This is a great sign, and hopefully translates to our testing data as well.\n\n\n\nI think we can see how we have done on the testing data now:\n\ntest_acc = ((LR.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", test_acc)\n\nTest accuracy without momentum:  0.9736841917037964\n\n\nWe have achieved an incredibly high accuracy on our testing data! Yay."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#generate-data",
    "href": "posts/Logistic-Regression/index.html#generate-data",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "Let’s generate some data with a minimal amount of noise.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\ndef plot_classification_data(X, y, ax):\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nplot_classification_data(X, y, ax)\nfig.suptitle(\"Classification Data\")\n\nText(0.5, 0.98, 'Classification Data')\n\n\n\n\n\n\n\n\n\nMake a function to plot decision regions:\n\ndef plot_decision_boundary(model, X, y, ax=None, highlight=False):\n    if ax is None:\n        fig, ax = plt.subplots(1, 1)\n\n    x1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\n    x2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\n\n    X1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\n    grid = torch.stack([X1.ravel(), X2.ravel()], dim=1)\n\n    # Append column of ones to match model input shape (for intercept)\n    grid = torch.cat([grid, torch.ones(grid.shape[0], 1)], dim=1)\n\n    preds = model.score(grid).reshape(X1.shape)\n\n    # Decision boundary plot\n    ax.contourf(X1, X2, preds, levels=50, cmap=\"BrBG\", alpha=0.8)\n    ax.contour(X1, X2, preds, levels=[0.0], colors='black', linewidths=2)\n\n    # Plot original data\n    plot_classification_data(X, y, ax)"
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#train-model",
    "href": "posts/Logistic-Regression/index.html#train-model",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "For our frist itteration, we will train our model with a learning rate (alpha) of .1, and a beta of 0. This means we are performing a logistic regression without momentum.\n\n\n# instantiate a model and an optimizer\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# for keeping track of loss values\nloss_vec = []\n\nfor _ in range(10000):\n    loss_vec.append(LR.loss(X, y).item())\n    opt.step(X, y, alpha = 0.1, beta = 0)\n\n\n\n\nWith our training complete, lets plot the loss over time\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\n\nplot_decision_boundary(LR, X, y)\n\n\n\n\n\n\n\n\nA Logistic regression, as demonstrated by the graph above, is going to be limited in the loss it can achieve by the noise within the data. There is no line that would perfectly separate this data without any feature mapping.\n\n\n\nNow let’s train our model on the same data, but this time we will use a beta of .9.\n\n# instantiate a model and an optimizer\nLR_momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\n# for keeping track of loss values\nloss_vec_momentum = []\n\nfor _ in range(10000):\n    loss_vec_momentum.append(LR_momentum.loss(X, y).item())\n    opt_momentum.step(X, y, alpha = 0.1, beta = 0.9)\n\nWith the model trained, let’s plot the two loss vectors on the same plot to compare.\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\nplt.xlim(0, 100)\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see from the above graph, with momentum our model reaches the minimum loss (Variance/noise of the date) much faster. In-fact, within 100 iterations, the without momentum model does not reach the variance of the data.(minimum loss)\nHowever, if given enough time, both models will converge on the minimum loss:\n\nplt.plot(torch.arange(1, len(loss_vec)+1), loss_vec, color = \"black\")\nplt.plot(torch.arange(1, len(loss_vec_momentum)+1), loss_vec_momentum, color = \"blue\")\n\nplt.title(\"Comparing Gradient Descent With and Without Momentum\")\nplt.legend([\"Without momentum\", \"With momentum\"])\n\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")"
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#overfitting",
    "href": "posts/Logistic-Regression/index.html#overfitting",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "Now we are going to generate a new set of data that. Let’s generate 2 sets of data with 2 times as many p_dims as n_points\n\n\nX_train, y_train = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\nX_test, y_test = classification_data(noise = 0.5, n_points=50, p_dims=100)\n\n\n\nWe will now use our X_train and y_train data to train a model to 100% accuracy in order to demonstrate the concept of overfitting.\n\nLR_overfit = LogisticRegression() \nopt_overfit = GradientDescentOptimizer(LR_overfit)\n\n\nloss_vec_train = []\nfor _ in range(100):\n    loss = LR_overfit.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt_overfit.step(X_train, y_train, alpha = 0.1, beta = 0.9)\n\nprint(\"Final training loss: \", loss_vec_train[-1])\n\nFinal training loss:  0.0025839933659881353\n\n\nWith our Gradient Descent with Momentum optimization complete, let’s look at how the loss changed over time:\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"blue\")\nplt.title(\"Loss overfitting on training data\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nAs we can see, our loss approached 0 within a very few number of gradient descent iterations. Now let’s test our accuracy:\n\ntrain_acc = ((LR_overfit.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Training accuracy: \", train_acc)\n\nTraining accuracy:  1.0\n\n\nGreat, our training accuracy is 100% as expected! Let’s test the accuracy on the test data.\n\ntest_acc = ((LR_overfit.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy: \", test_acc)\n\nTest accuracy:  0.8799999952316284\n\n\nHmmm, so it seems that we have over-fit our model quite considerably. Our test accuracy is ~.16 worse than our training accuracy of 1."
  },
  {
    "objectID": "posts/Logistic-Regression/index.html#performance-on-empirical-data-wisconsin-breast-cancer-diagnostic",
    "href": "posts/Logistic-Regression/index.html#performance-on-empirical-data-wisconsin-breast-cancer-diagnostic",
    "title": "Logistic Regression Implementation",
    "section": "",
    "text": "The data from load_breast_cancer dataset is from the Wisconsin (diagnostic) breast cancer dataset. The features in this data were calculated from imaging of breast masses, and describe characteristics of the cell in the image.\n\nfrom sklearn.datasets import load_breast_cancer\n\nX, y = load_breast_cancer(return_X_y=True)\n\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version &gt;=1.16.5 and &lt;1.23.0 is required for this version of SciPy (detected version 1.26.4\n  warnings.warn(f\"A NumPy version &gt;={np_minversion} and &lt;{np_maxversion}\"\n\n\nThis dataset has a unique identifier (id), 28 anonymized features about the transaction, the transaction amount, and the class. The class is 1 if the transaction is fraudulent and 0 if not.\n\n\nNow we split our data into Training, Validation, and Testing datasets using train_test_split frm sklearn. Data is split 60%, 20%, 20% respectively:\n\nfrom sklearn.model_selection import train_test_split\n# split train and test .6 and .4\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# splite temp into validation and test sets at .5 and .5\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n\n\nIn order to use this data on our Logistic Regression model we need to convert it into torch.Tensor’s:\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.long)  # or float32 for regression\n\nX_val = torch.tensor(X_val, dtype=torch.float32)\ny_val = torch.tensor(y_val, dtype=torch.long)\n\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.long)\n\n\n\nX_train.size()\n\ntorch.Size([341, 30])\n\n\nNow we can train our Logisitc Regression Model on this data:\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\n\nfor _ in range(3500):\n    loss = LR.loss(X_train, y_train).item()\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = 0.00005, beta = 0)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nGreat, our loss after 3500 iterations with a learning rate of 0.0005 and a beta of 0 levels out around a loss of 1.\nLet’s see how it performs with momentum:\n\nLRm = LogisticRegression()\nopt_m = GradientDescentOptimizer(LRm)\nloss_vec_train_m = []\n\nfor _ in range(3500):\n    loss = LRm.loss(X_train, y_train).item()\n    loss_vec_train_m.append(loss)\n    opt_m.step(X_train, y_train, alpha = 0.00005, beta = 0.9)\n\n\nplt.plot(torch.arange(1, len(loss_vec_train_m)+1), loss_vec_train_m, color = \"blue\")\nplt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\nplt.title(\"Loss Using Gradient Descent Without Momentum\")\nplt.legend([\"With momentum\", \"Without momentum\"])\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\n\n\n\n\n\n\n\n\nOur model reaches a lower loss on the training data when using momentum. Even though it does not make constant progress, it reaches a lower loss in fewer iterations. It is interesting to note how low the learning rate (alpha) had to be in order for this model to reach a good loss.\nLets test this model, with momentum on our training data:\n\ntrain_acc = ((LRm.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy with momentum: \", train_acc)\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", train_acc)\n\nTest accuracy with momentum:  0.9149560332298279\nTest accuracy without momentum:  0.8504399061203003\n\n\nAdding momentum gives us a slightly more accurate model on the training data, so we will use momentum going forward.\n\n\n\nIf we use a more robust method than above however, we can probably improve our accuracy by finding the best momentum (beta) and learning rate (alpha). We do this below by selecting samples of alphas and betas and finding the best combination.\n\nlearning_rates = [\n    1e-6, 3e-6,\n    1e-5, 3e-5,\n    1e-4, 3e-4,\n    1e-3, 3e-3,\n    1e-2, 3e-2,\n    \n]\nbetas = [\n    0.0, 0.1,\n    0.3, 0.5,\n    0.7, 0.9,\n    0.99, 0.999,\n]\n\nbest_loss = 1e10\nbest_alpha = None\nbest_beta = None\n\nfor beta in betas:\n    best_loss_per_beta = 1e10\n    best_alpha_per_beta = None\n    beta_loss_vec = []\n    for alpha in learning_rates:\n        LR = LogisticRegression()\n        opt = GradientDescentOptimizer(LR)\n        loss_vec_train = []\n        for _ in range(400):\n            loss = LR.loss(X_train, y_train).item()\n            loss_vec_train.append(loss)\n            \n            opt.step(X_train, y_train, alpha = alpha, beta = 0.9)\n        if loss_vec_train[-1] &lt; best_loss:\n            best_loss_per_beta = loss_vec_train[-1]\n            best_alpha_per_beta = alpha\n        \n    if best_loss_per_beta &lt; best_loss:\n        best_loss = best_loss_per_beta\n        best_alpha = best_alpha_per_beta\n        best_beta = beta\n      \n\n\nprint(\"Best learning rate: \", best_alpha)\nprint(\"Best beta: \", best_beta)\nprint(\"Best loss: \", best_loss)\n\nBest learning rate:  0.0003\nBest beta:  0.999\nBest loss:  0.6386049389839172\n\n\nIn validation, we are able to find our best learning rate and best beta to minimize our loss. Let’s now train our model on the training data, and we will track the loss on the validation and training data overtime so we can compare them.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\nloss_vec_train = []\nloss_vec_val = []\nfor _ in range(400):\n    loss = LR.loss(X_train, y_train).item()\n    loss_val = LR.loss(X_val, y_val).item()\n    loss_vec_val.append(loss_val)\n    loss_vec_train.append(loss)\n    opt.step(X_train, y_train, alpha = best_alpha, beta = best_beta)\n    plt.plot(torch.arange(1, len(loss_vec_train)+1), loss_vec_train, color = \"black\")\n    plt.plot(torch.arange(1, len(loss_vec_val)+1), loss_vec_val, color = \"blue\")\nplt.title(\"Training and Validation Loss Using Gradient Descent With Momentum\")\nlabs = plt.gca().set(xlabel = \"Number of gradient descent iterations\", ylabel = \"Loss (binary cross entropy)\")\nplt.legend([\"Training loss\", \"Validation loss\"])\n\n\n\n\n\n\n\n\nSurprisingly, our model performs better on the validation data than on the training data. Let’s check our accuracy on the validation data with our best alpha and beta values:\n\ntrain_acc = ((LR.predict(X_train) == y_train)*1.0).mean().item()\n\nprint(\"Train accuracy: \", train_acc)\n\nval_acc = ((LR.predict(X_val) == y_val)*1.0).mean().item()\n\nprint(\"Validation accuracy: \", val_acc)\n\n\nTrain accuracy:  0.8914955854415894\nValidation accuracy:  0.9385964870452881\n\n\nAs we can see our model did even better on the validation data than on the training data. This is a great sign, and hopefully translates to our testing data as well.\n\n\n\nI think we can see how we have done on the testing data now:\n\ntest_acc = ((LR.predict(X_test) == y_test)*1.0).mean().item()\n\nprint(\"Test accuracy without momentum: \", test_acc)\n\nTest accuracy without momentum:  0.9736841917037964\n\n\nWe have achieved an incredibly high accuracy on our testing data! Yay."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html",
    "href": "posts/Impact-Automated-Decision-Systems/index.html",
    "title": "Automated Decisions — Blog Post 2",
    "section": "",
    "text": "In this blog post, the design process, and the impact of the decisions made in that design process, of Automated Decision models were explored. Through a data set of credit applicants and their eventual repayment status, a model for maximizing the profits of a loan issuing bank/individual was created. The process of feature optimization and weighting, followed by decision threshold optimization allowed us to maximize the profits we received from the issuance of loans. This raised fascinating questions regarding fairness and prioritization when it comes to Automated Decisions. My model optimized for profits, resulting in a willingness to risk giving some loans that would default if it meant more that would be repaid."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html#abstract",
    "href": "posts/Impact-Automated-Decision-Systems/index.html#abstract",
    "title": "Automated Decisions — Blog Post 2",
    "section": "",
    "text": "In this blog post, the design process, and the impact of the decisions made in that design process, of Automated Decision models were explored. Through a data set of credit applicants and their eventual repayment status, a model for maximizing the profits of a loan issuing bank/individual was created. The process of feature optimization and weighting, followed by decision threshold optimization allowed us to maximize the profits we received from the issuance of loans. This raised fascinating questions regarding fairness and prioritization when it comes to Automated Decisions. My model optimized for profits, resulting in a willingness to risk giving some loans that would default if it meant more that would be repaid."
  },
  {
    "objectID": "posts/Impact-Automated-Decision-Systems/index.html#set-up-the-data",
    "href": "posts/Impact-Automated-Decision-Systems/index.html#set-up-the-data",
    "title": "Automated Decisions — Blog Post 2",
    "section": "Set Up The Data",
    "text": "Set Up The Data\nFirst let’s read in the training data, and split it into an X and Y training set.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\nX_train = df_train.drop(columns = [\"loan_status\", \"loan_grade\"])\ny_train = df_train[\"loan_status\"]\n\ndf_train.head()\n\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n\n\n\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(df_train[\"loan_status\"])\n\ndef prepare_data(df):\n  df = df.dropna()\n  y = le.transform(df[\"loan_status\"])\n  df = df.drop([\"loan_status\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(df_train)\n\n\nExploring the Data\nCreate at least two visualizations and one summary table in which you explore patterns in the data. You might consider some questions like:\n\nHow does loan intent vary with the age, length of employment, or homeownership status of an individual?\nWhich segments of prospective borrowers are offered low interest rates? Which segments are offered high interest rates?\nWhich segments of prospective borrowers have access to large lines of credit?\n\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\np1 = sns.histplot(df_train, x = \"person_age\", hue = \"loan_intent\", element=\"step\", binwidth=5, multiple=\"fill\", stat=\"percent\")\np1.set_xlim(15, 80)\nplt.title(\"Percent of Age Group Loans by Loan Intent\")\nplt.show()\n\n\n\n\n\n\n\n\nThe most important trends to notice in my opinion, are that as a person gets older, education and venture oriented loans are replaced by more and more personal, and medical loans. Eventually, for individuals in the 75-80 years range, they are only getting medical loans. Additionally, it is important to note that when not done as a percent, there are simply more people getting loans at younger ages.\n\n\n\n# sns.pairplot(df_train, hue=\"person_home_ownership\")\n\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"loan_intent\", data = df_train)\nplt.title(\"Interest Rate by Loan Intent\")\nplt.show()\n\n\n\n\n\n\n\n\nI have not identified any goruping variables that result in a higher interest rate for specific individuals. As shown in the above example, for instance, there are no statistically significant differences based on loan intent.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"person_home_ownership\", data = df_train)\nplt.title(\"Interest Rate by Home Ownership\")\nplt.show()\n\n\n\n\n\n\n\n\nThe same is true for grouping by home ownership types. Because the medians all fall within the first and third quartiles of all other groups, it is hard to conclude that these groups are different.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_int_rate\", y = \"loan_grade\", data = df_train)\nplt.title(\"Interest Rate by Loan Grade\")\nplt.show()\n\n\n\n\n\n\n\n\nOnly once we start looking at things like credit history and loan grade do we begin to see different treatments in terms of interest rate, and this makes sense: a riskier loan is going to require a higher interest rate to make the risk worth it for the lender.\n\nplt.figure(figsize = (10, 10))\nsns.scatterplot(x = \"person_income\", y = \"loan_amnt\", data = df_train)\nplt.xlim(0, 200000)\nplt.title(\"Income vs. Loan Amount\")\n\nText(0.5, 1.0, 'Income vs. Loan Amount')\n\n\n\n\n\n\n\n\n\nIndividuals with larger incomes are more likely to have access to large lines of credit.\n\nplt.figure(figsize = (10, 10))\nsns.boxplot(x = \"loan_amnt\", y = \"loan_intent\", data = df_train)\nplt.title(\"Interest Rate by Loan Grade\")\nplt.show()\n\n\n\n\n\n\n\n\nInterestingly, it does not seem like the intent of the loan impacts the size of the line of credit extended to the individual.\nWith these exploratory questions addressed, let’s fit our model.\n\n\nFitting the Model\nFirst, let’s perform an exhaustive search to determine the best quantitative and qualitative columns to use to maximize the accuracy of our Logistic Regression model:\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n\n\nall_qual_cols = [\"loan_intent\", \"person_home_ownership\", \"cb_person_default_on_file\"]\nall_quant_cols = ['person_emp_length', 'loan_amnt', 'loan_int_rate', \"loan_percent_income\", \"cb_person_cred_hist_length\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 4):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    new_score = LR.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['person_home_ownership_MORTGAGE', 'person_home_ownership_OTHER', 'person_home_ownership_OWN', 'person_home_ownership_RENT', 'person_emp_length', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\nHighest Score:  0.8461605622735409\n\n\nOur highest scoreing model has been identified along with its highest scoring accuracy of .846.\n\nLR = LogisticRegression(max_iter = 10000)\nLR.fit(X_train, y_train)\n\nLogisticRegression(max_iter=10000)\n\n\nAlternatively, we can use sklearns feature selection RFECV tool to select them instead:\n\nfrom sklearn.feature_selection import RFECV\n\nselector = RFECV(LR, step = 1, cv = 5)\n\nselector.fit(X_train, y_train)\n\nselected_features = X_train.columns[selector.support_]\nprint(\"Selected features:\", selected_features)\n\nSelected features: Index(['person_age', 'person_income', 'person_emp_length', 'loan_amnt',\n       'loan_int_rate', 'cb_person_cred_hist_length',\n       'person_home_ownership_MORTGAGE', 'person_home_ownership_OWN',\n       'person_home_ownership_RENT', 'loan_intent_VENTURE', 'loan_grade_A',\n       'loan_grade_B', 'loan_grade_D', 'cb_person_default_on_file_N',\n       'cb_person_default_on_file_Y'],\n      dtype='object')\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/base.py:445: UserWarning: X does not have valid feature names, but RFECV was fitted with feature names\n  warnings.warn(\n\n\nLet’s use these instead and train our model with them:\n\nLR.fit(X_train[selected_features], y_train)\n\nLogisticRegression(max_iter=10000)\n\n\nNow that we have fit a mode, we can extract the weights, and use it to score each training loan. The resulting score can then be used to determine the best thresshold for making a decision in order to maximize our profits by giving a loan or not.\n\nimport numpy as np\nw = LR.coef_.reshape(-1)\nX = X_train[selected_features]\ns = X@w\n\nprint(s.max())\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nhist = ax.hist(s, bins = 100, color = \"steelblue\", alpha = 0.6, linewidth = 1, edgecolor = \"black\", range=[-20, 5])\nlabs = ax.set(xlabel = r\"Score $s$\", ylabel = \"Frequency\")\n\n1.5223252366858058\n\n\n\n\n\n\n\n\n\nWith these scores calculated, we are going to use the following simplified formula’s to determine the total profit made at each potential threshold:\n\nProfit on a loan repaid in full:\nloan_amnt(1 + 0.25loan_int_rate)**10 - loan_amnt\n\n\nBorrower Defaults on a Loan\nloan_amnt(1 + 0.25loan_int_rate)**3 - 1.7*loan_amnt\n\n\nCalculate Optimal Threshold\n\n\n\nbest_profit = float('-inf')\nbest_threshold = -20\n\nfig, ax = plt.subplots(1, 1, figsize = (6, 4))\nprofits = []\nthresholds = np.linspace(-.5, 0.5, 101)\n\nloan_amnt = X_train['loan_amnt']  \nloan_int_rate = X_train['loan_int_rate'] \n\nfor t in np.linspace(-.5, 0.5, 101): \n    y_pred = s &gt; t\n    acc = (y_pred == y_train)\n    profit_repaid = ((acc &  -(y_train+1)) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**10 - loan_amnt))\n    loss_default = ((-(acc+1) & y_train) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**3 - .7 * loan_amnt))\n    profit = (profit_repaid - loss_default).mean()\n    profits.append(profit)\n    if profit &gt; best_profit: \n        best_profit = profit\n        best_threshold = t\n\nax.axvline(best_threshold, linestyle = \"--\", color = \"grey\", zorder = -10)\nax.plot(thresholds, profits, color=\"steelblue\")\nax.axvline(best_threshold, linestyle=\"--\", color=\"grey\", zorder=-10)\n\nlabs = ax.set(xlabel = r\"Threshold $t$\", ylabel = \"Profit Per Loan Applicant\", title = f\"Best profit {best_profit:.3f} at best threshold t = {best_threshold:.3f}\")\n\n\n\n\n\n\n\n\nThe optimal threshold with our model is a score of -0.320. This results in a profit per analyzed grant of $1548.789. It is interesting that our threshold is negative, as it is a direct result of the fact that we are not optimizing for accuracy, but profits. It is okay to be at a threshold where more loans that are accepted, are not payed back, as long as the amount payed back and the amount from the loans that are payed back are greater than the losses from the loans which are defaulted on. I will discuss this topic more later on.\n\n\n\nTesting our Model, Weights, and Threshold\nNow let’s download the test data and prepare it.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\n\nX_test, y_test = prepare_data(df_test)\n\nWith our test data, let’s apply our weights to the selected features to get our score.\n\ntest_pred = X_test[selected_features]@w\n\n\n\n\nNow we can apply those scores to our optimal threshold to determine which loans we should issue.\n\ntest_pred = test_pred &gt; best_threshold\n\nWith our decisions made, we can now figure out how much money we have made on our test loans:\n\nloan_amnt = X_test['loan_amnt']  \nloan_int_rate = X_test['loan_int_rate'] \nacc = (test_pred == y_test)\nprofit_repaid = ((acc &  -(y_test+1)) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**10 - loan_amnt))\nloss_default = ((-(acc+1) & y_test) * (loan_amnt * (1 + 0.25 * (loan_int_rate/100))**3 - .7 * loan_amnt))\n\nprofit = profit_repaid - loss_default\n\nprofit.mean()\n\n1496.914919368003\n\n\nWith our algorithm we made $1496.91 per loan analyzed! While this is slightly lower than the $1548 per loan analyzed on our training data, this seems pretty good.\nIt is important to note that since our goal was profit, this does not mean that we have maximized for reducing defaults on loans\n\nprofit.sum()\n\n8578819.402898025\n\n\nThis would have resulted in $8.58 Billion dollars in total profit from the test data loans.\n\n\nImpact from the Borrowers Perspective\n\ndf_pred = df_test.copy()\ndf_pred[\"pred_loan_status\"] = (test_pred)\n\ndf_pred\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\npred_loan_status\n\n\n\n\n0\n21\n42000\nRENT\n5.0\nVENTURE\nD\n1000\n15.58\n1\n0.02\nN\n4\nFalse\n\n\n1\n32\n51000\nMORTGAGE\n2.0\nDEBTCONSOLIDATION\nB\n15000\n11.36\n0\n0.29\nN\n9\nFalse\n\n\n2\n35\n54084\nRENT\n2.0\nDEBTCONSOLIDATION\nC\n3000\n12.61\n0\n0.06\nN\n6\nFalse\n\n\n3\n28\n66300\nMORTGAGE\n11.0\nMEDICAL\nD\n12000\n14.11\n1\n0.15\nN\n6\nFalse\n\n\n4\n22\n70550\nRENT\n0.0\nMEDICAL\nE\n7000\n15.88\n1\n0.08\nN\n3\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6512\n26\n26000\nMORTGAGE\n4.0\nHOMEIMPROVEMENT\nB\n12000\nNaN\n0\n0.46\nN\n3\nNaN\n\n\n6513\n27\n44640\nRENT\n0.0\nMEDICAL\nB\n12800\n11.83\n0\n0.29\nN\n9\nFalse\n\n\n6514\n24\n48000\nOWN\n5.0\nVENTURE\nA\n10400\n7.37\n0\n0.22\nN\n3\nFalse\n\n\n6515\n26\n65000\nMORTGAGE\n6.0\nEDUCATION\nA\n6000\n9.07\n0\n0.09\nN\n3\nFalse\n\n\n6516\n29\n61000\nRENT\n12.0\nVENTURE\nD\n10000\n16.07\n0\n0.16\nN\n9\nFalse\n\n\n\n\n6517 rows × 13 columns\n\n\n\n\nAge\n\nbins = [0, 30, 40, 50, 60, 70, 80]\n\ndf_pred['age_group'] = pd.cut(df_pred['person_age'], bins, labels=[\"0-30\", \"31-40\", \"41-50\", \"51-60\", \"61-70\", \"71+\"], right=False)\n\ndf_pred.groupby([\"age_group\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nage_group\nloan_status\nmean\ncount\n\n\n\n\n0\n0-30\n0\n0.055729\n3194\n\n\n1\n0-30\n1\n0.414995\n947\n\n\n2\n31-40\n0\n0.050296\n1014\n\n\n3\n31-40\n1\n0.388462\n260\n\n\n4\n41-50\n0\n0.033654\n208\n\n\n5\n41-50\n1\n0.384615\n52\n\n\n6\n51-60\n0\n0.000000\n31\n\n\n7\n51-60\n1\n0.363636\n11\n\n\n8\n61-70\n0\n0.000000\n6\n\n\n9\n61-70\n1\n0.500000\n6\n\n\n10\n71+\n0\n0.000000\n1\n\n\n11\n71+\n1\n0.000000\n1\n\n\n\n\n\n\n\nThis demonstrates a trend that will repeat itself over the course of our entire analysis: our model makes it too easy for people who shouldn’t receive a loan to receive one. The false negative rates (people being denied a loan who deserve one) are incredibly low — below 6% in every category. The problem is that especially for younger individuals, our model is highly likely to issue a loan to someone who will default; our model only denies 42% of individuals who go on to default in the 0-30 category, 39% in the 31-40 category, 38% in the 41-50 category, 36% in the 51-60 category, and 50% in the 51-60 category. This is incredibly concerning because young individuals who are given a loan and deffault might not have the financial resources built up to overcome this challenge.\n\n\nLoan Intent\n\nPositive Prediction Rates\n\ndf_pred.groupby(\"loan_intent\")[\"pred_loan_status\", \"loan_status\"].agg([\"mean\", \"count\"])\n\n/var/folders/kn/65gyjdg54k7d_hx68jbshfz40000gn/T/ipykernel_40256/3005163201.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n  df_pred.groupby(\"loan_intent\")[\"pred_loan_status\", \"loan_status\"].agg([\"mean\", \"count\"])\n\n\n\n\n\n\n\n\n\npred_loan_status\nloan_status\n\n\n\nmean\ncount\nmean\ncount\n\n\nloan_intent\n\n\n\n\n\n\n\n\nDEBTCONSOLIDATION\n0.136062\n904\n0.279497\n1034\n\n\nEDUCATION\n0.142857\n1176\n0.167421\n1326\n\n\nHOMEIMPROVEMENT\n0.086039\n616\n0.246088\n703\n\n\nMEDICAL\n0.152842\n1073\n0.281553\n1236\n\n\nPERSONAL\n0.135271\n998\n0.219227\n1113\n\n\nVENTURE\n0.118257\n964\n0.145701\n1105\n\n\n\n\n\n\n\nOne thing stands out from simply showing the percent of the time our model predicts a default compared to the percent of the time a default actually occurs: 1, Our model under predicts defaults significantly due to its aim of profit as opposed to actual accuracy. Education and Venture loans seem to be the two categories in which the ratios are similar. This tells us nothing about the accuracy of the Loans, only the rates at which it predicts defaults in each category. While this means that it is easier to get a loan across the board than it should be, this does not necessarily mean it is better for the borrower. Banks probably have a much better sense for if an individual is going to default or not compared to the experience of an individual taking out a loan for the first time. The fact that the bank is abusing that power to prioritize profits could mean that many individuals are put into a financial hole due to unpayable debt.\n\n\nError-Rate Balance\nLet’s see how this breaks down in terms of actual accuracy:\n\np1 = sns.FacetGrid(df_pred, col=\"loan_intent\", hue=\"pred_loan_status\")\np1.map(sns.histplot, \"loan_status\")\np1.add_legend()\n\n\n\n\n\n\n\n\nWhile not the typical way, these visualizations show the TP and FP across categories. It is clear from these graphs that across the board, our model is pretty good at giving loans to people who deserve them, with only a few people being denied loans who would go on to pay back in full. On the other hand, our model is almost as likely to give a loan than not, if not more likely to give a loan than not, to people who would go on to default. This gun-ho model could be potentially predatory to borrowers, and strictly maximizing for profits could be a really bad idea (Think 2008 Financial Crisis).\n\ndf_pred.groupby([\"loan_intent\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nloan_intent\nloan_status\nmean\ncount\n\n\n\n\n0\nDEBTCONSOLIDATION\n0\n0.054348\n644\n\n\n1\nDEBTCONSOLIDATION\n1\n0.338462\n260\n\n\n2\nEDUCATION\n0\n0.070480\n979\n\n\n3\nEDUCATION\n1\n0.502538\n197\n\n\n4\nHOMEIMPROVEMENT\n0\n0.045455\n462\n\n\n5\nHOMEIMPROVEMENT\n1\n0.207792\n154\n\n\n6\nMEDICAL\n0\n0.040365\n768\n\n\n7\nMEDICAL\n1\n0.436066\n305\n\n\n8\nPERSONAL\n0\n0.055270\n778\n\n\n9\nPERSONAL\n1\n0.418182\n220\n\n\n10\nVENTURE\n0\n0.044957\n823\n\n\n11\nVENTURE\n1\n0.546099\n141\n\n\n\n\n\n\n\nThis Quantitative look at error rates reaffirms what we saw from the visualizations: our model has a very high True Negative Rate across categories, and also quite Low True Positive Rates ranging from .207 (Home Improvement) to .546 (Venture). Regardless of category, this means that our model is far from optimized for not issuing loans to people who have a high risk of default, it is optimized to give as many loans to people who will pay back as possible. As mentioned above this could have devastating impacts an individuals financial health.\n\n\n\nBy Income Level\n\nbins = [0, 50000, 100000, 150000, 200000, 10000000]\n\ndf_pred['income_group'] = pd.cut(df_pred['person_income'], bins, labels=[\"0-50K\", \"50-100K\", \"100-150K\", \"150-200K\", \"200K+\"], right=False)\n\ndf_pred.groupby([\"income_group\", \"loan_status\"])[\"pred_loan_status\"].agg([\"mean\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\nincome_group\nloan_status\nmean\ncount\n\n\n\n\n0\n0-50K\n0\n0.119768\n1553\n\n\n1\n0-50K\n1\n0.543837\n787\n\n\n2\n50-100K\n0\n0.023223\n2153\n\n\n3\n50-100K\n1\n0.229064\n406\n\n\n4\n100-150K\n0\n0.000000\n530\n\n\n5\n100-150K\n1\n0.000000\n61\n\n\n6\n150-200K\n0\n0.000000\n124\n\n\n7\n150-200K\n1\n0.000000\n12\n\n\n8\n200K+\n0\n0.000000\n94\n\n\n9\n200K+\n1\n0.000000\n11\n\n\n\n\n\n\n\nIt is fascinating to see that our income variable is an incredibly high predictor of if an individual will receive a loan or not. While not surprising, it seems that if an individual makes over 100K they will receive a loan. 0 Individuals over this income were denied even though many would have defaulted. Even for individuals with an income between 0 and 50 k, our model only rejected 54% of individuals who went on to default. That number is 23% for 50-100K earners, and 0 for everyone with a higher income.\n\n\n\nDiscussion\nThrough this blog post, I learned quite a bit about how different interests can lead to problematic results in a model. I also learned technical skills such as threshold optimization, and the importance of defining and exploring what occurs when you decide on what you are going to optimize for. In our model we optimized for profits, not fairness in any respect. This meant is was more difficult for people in particular groups—medical debt and young— to get a loan. People with medical expenses have higher default rates, and so it is harder for them to get a loan. In my opinion, and at this point we are in the realm of value judgement not fact, the lower availability of credit is not unfair. While I do not have a good definition of fairness, what comes to my mind is that two private* parties involved in a decision act with good faith towards one another and treat each other and everyone else as of equal value. This does not mean that one group should be forced to take unnecessary risk to overcome an existing unfairness. That sort of redistributive action is what governments are responsible for. I do not mean that private individuals can’t or shouldn’t try to actively overcome existing unfairness, I mean that they cannot be forced to or expected to just because something is unfair. In the case of Medical Debt, the unfair part is the fact that our medical system is so broken that medical expenses become crippling and un-creditable. There is no reason that banks or private creditors should shoulder this unfairness by giving loans that will not be paid back. The money that banks loan should not be thrown around without foresight, because that is the money of everyone else being held and safeguarded. Given the willingness of my model to predict repayment, a more relevant discussion is that our model was too willing to give loans to people who probably should not have been given them. Our model found a threshold where giving loans to people with a certain amount of risk meant that enough would repay such that profit would be made in-spite of the fact that a significant portion would default. This is predatory in nature and potentially catastrophic for the individual. This is especially concerning given the banks privilege knowledge of credit, while the borrower could potentially be much less informed regarding their decision."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for training:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\nThis is what the data looks like:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nLets group our data by Species and Clutch completion, and look at the mean and standard deviation for the quantitative columns. In doing so, we should be able to see some columns that might be useful for differentiating species.\n\ntable = train.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1).groupby(['Clutch Completion', 'Species']).aggregate(['mean', 'std'])\ntable.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n39.041667\n2.152571\n18.225000\n1.072063\n187.666667\n7.726381\n3764.583333\n605.604350\n8.913195\n0.328239\n-25.676738\n0.485634\n\n\nChinstrap\n49.040000\n4.627022\n18.040000\n0.987927\n194.400000\n7.121173\n3532.500000\n536.196740\n9.425587\n0.346548\n-24.560953\n0.320757\n\n\nGentoo\n46.485714\n3.632001\n14.714286\n1.206135\n216.000000\n6.708204\n4910.714286\n679.131974\n8.279576\n0.235220\n-26.247917\n0.577585\n\n\nYes\nAdelie\n38.962617\n2.698411\n18.429907\n1.235040\n190.355140\n6.549164\n3713.317757\n447.165045\n8.856116\n0.436348\n-25.810248\n0.578651\n\n\nChinstrap\n48.780851\n3.207982\n18.436170\n1.165151\n196.340426\n7.516002\n3788.297872\n366.195595\n9.310443\n0.375693\n-24.551794\n0.230540\n\n\n\n\n\n\n\nThis is helpful, but it might be even more insightful and intuitive to compute the range of 2 standard deviations for each quantitative column. With the standard deviations and means calculated, the range of 2 standard deviations can then be calculated by subtracting and adding 2 times the value of the std dev to/from the mean for that column.\nNote: I found the assign function used below in search of a pandas equivalent of the R mutate()function (See link below)\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html\n\n\ntable = table.assign(Culmen_Length_Max = table[\"Culmen Length (mm)\"][\"mean\"] + 2*table[\"Culmen Length (mm)\"][\"std\"], Culmen_Length_Min = table[\"Culmen Length (mm)\"][\"mean\"] - 2*table[\"Culmen Length (mm)\"][\"std\"])\ntable = table.assign(Culmen_Depth_Max = table[\"Culmen Depth (mm)\"][\"mean\"] + 2*table[\"Culmen Depth (mm)\"][\"std\"], Culmen_Depth_Min = table[\"Culmen Depth (mm)\"][\"mean\"] - 2*table[\"Culmen Depth (mm)\"][\"std\"])\ntable = table.assign(Body_Mass_Max = table[\"Body Mass (g)\"][\"mean\"] + 2*table[\"Body Mass (g)\"][\"std\"], Body_Mass_Min = table[\"Body Mass (g)\"][\"mean\"] - 2*table[\"Body Mass (g)\"][\"std\"])\ntable = table.assign(Flipper_Length_Max = table[\"Flipper Length (mm)\"][\"mean\"] + 2*table[\"Flipper Length (mm)\"][\"std\"], Flipper_Length_Min = table[\"Flipper Length (mm)\"][\"mean\"] - 2*table[\"Flipper Length (mm)\"][\"std\"])\ntable = table.assign(Delta_13_Max = table[\"Delta 13 C (o/oo)\"][\"mean\"] + 2*table[\"Delta 13 C (o/oo)\"][\"std\"], Delta_13_Min = table[\"Delta 13 C (o/oo)\"][\"mean\"] - 2*table[\"Delta 13 C (o/oo)\"][\"std\"])\ntable = table.assign(Delta_15_Max = table[\"Delta 15 N (o/oo)\"][\"mean\"] + 2*table[\"Delta 15 N (o/oo)\"][\"std\"], Delta_15_Min = table[\"Delta 15 N (o/oo)\"][\"mean\"] - 2*table[\"Delta 15 N (o/oo)\"][\"std\"])\ntable.drop([\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"], axis = 1)\n\n\n\n\n\n\n\n\n\nCulmen_Length_Max\nCulmen_Length_Min\nCulmen_Depth_Max\nCulmen_Depth_Min\nBody_Mass_Max\nBody_Mass_Min\nFlipper_Length_Max\nFlipper_Length_Min\nDelta_13_Max\nDelta_13_Min\nDelta_15_Max\nDelta_15_Min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n43.346808\n34.736525\n20.369125\n16.080875\n4975.792033\n2553.374633\n203.119429\n172.213904\n-24.705470\n-26.648007\n9.569673\n8.256716\n\n\nChinstrap\n58.294044\n39.785956\n20.015854\n16.064146\n4604.893481\n2460.106519\n208.642347\n180.157653\n-23.919438\n-25.202468\n10.118684\n8.732490\n\n\nGentoo\n53.749716\n39.221713\n17.126556\n12.302015\n6268.978234\n3552.450338\n229.416408\n202.583592\n-25.092747\n-27.403087\n8.750016\n7.809135\n\n\nYes\nAdelie\n44.359438\n33.565796\n20.899986\n15.959827\n4607.647847\n2818.987667\n203.453469\n177.256812\n-24.652947\n-26.967550\n9.728812\n7.983420\n\n\nChinstrap\n55.196815\n42.364888\n20.766472\n16.105869\n4520.689062\n3055.906683\n211.372430\n181.308421\n-24.090715\n-25.012873\n10.061828\n8.559058\n\n\nGentoo\n52.472111\n41.765667\n16.914536\n12.945464\n6021.446275\n4078.553725\n228.625764\n204.996458\n-25.027738\n-27.255542\n8.810899\n7.678713\n\n\n\n\n\n\n\nThis is extraordinarily insightful, as we can see which quantitative columns have distinct ranges when grouped by Culmen Depth and Species. Since 95.4% of data falls within 2 std deviations of the mean, distinct ranges in this category can help identify very high quality candidate columns to use as quantitative predictor variables.\nUpon analysis of this table, the Culmen Length, Culmen Depth, and Flipper Length appear to the columns with the most variation in ranges across species and Clutch Completion. For penguins with a clutch completion of No, Culmen Length ranges are potentially distinguishable for Adelie compared with Chinstrap and Gentoo, Culmen Depth ranges are distinguishable for Gentoo compared with Adelie and Chinstrap, and Flipper Length has all overlapping ranges although they are also very different with Adelie the low end, Chinstrap in the middle, and Gentoo at the top.\nFor penguins with clutch completion of yes, Culmen Length distinguishes Adelie from Chinstrap and Gentoo, Culmen Depth distinguishes Gentoo from Adelie and Chinstrap, and Flipper Length once again are overlapping but not completely.\nLet’s make visualizations of these differences to analyze them further.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\n# p1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\",  style = \"Island\", hue = \"Species\")\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[1],  style = \"Clutch Completion\", hue = \"Species\")\n\nsns.despine() # remove the top and right spines\n\np1.title.set_text(\"Culmen Length vs. Culmen Depth\")\n\n\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0],  style = \"Clutch Completion\", hue = \"Species\")\n\np2.title.set_text(\"Flipper Length vs. Culmen Length\") # set the title of the left visualization\nax[0].get_legend().remove() # remove the legend from the left graph to avoid duplication and messiness\n\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # move the legend to the right graph\n\n\n\n\n\n\n\n\n\n\nFrom these two Plots, it appears that while Flipper Length and Culmen Depth would be very useful for distinguishing Gentoo penguins from the other two species, it would be very difficult to distinguish between the Chinstrap and Adelie penguins. When replacing Flipper Length with Culmen Depth, it appears that while the overlap still exists it may be possible to develop an appropriate model for predicting species.\nRegardless, the most important takeaway is that it seems inlikely by looking at the data that a logistic regression will be capable of achieving 100% accuracy using these features, as there are some overlapping—or close to overlapping—regions on both of these graphs."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nNow that we have explored the data and established that a model may be effective and developed some hypothesis regarding which features and models might work best, we need to do an exhaustive and repeatable feature search to ensure that we create the best possible model.\n\nLogistic Regression\nFirst, let’s try using a Logistic Regression even though our visualizations suggested this might not be 100% effective.\nIn this code we cycle through all combinations of 1 qualitative and 2 quantitative predictor features, saving the score and feature set that perform the best.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    new_score = LR.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  0.99609375\n\n\nUsing a Logistic Regression, the highest score we find is 0.996 using Sex, Culmen Length, and Culmen depth. Because our aim is 100%, there is no logistic regression using 1 qualitative and 2 quantitative predictor features that will achieve our goal. let’s take a look at using a different model\n\n\nRandom Forest Classifier\nThe same process is followed as above, except the Logistic Regression is substituted for a Random Forrest Classifier.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    new_score = RF.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  1.0\n\n\nWonderful! Our search has found that using a Random Forest Classifier, CLutch Completion, Culmen Length, and Culmen Depth results in a perfect score when scored on the training data. Now we need to fit the model with the best columns found above."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Fitting the Model — Random Forest Classifier Model",
    "text": "Fitting the Model — Random Forest Classifier Model\nSince we have identified that the Random Forest Classifier model has a score of 1 with the columns of Clutch Completion, Culmen Length, and Culmen depth, let’s train our model:\n\nRF = RandomForestClassifier()\nRF.fit(X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\nRandomForestClassifier()\n\n\n\nCross Validation\nBefore Committing to the Random Forest Classifier Model, let’s cross validate it!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_RF = cross_val_score(RF, X_train, y_train, cv=5)\ncv_scores_RF\n\narray([0.98076923, 1.        , 1.        , 0.98039216, 1.        ])\n\n\n3 / 5 of our folds score perfectly in cross validation. This suggests that there are specific values in our dataset that are difficult to classify if they are in the testing data and not in the training data, but also that our model is performing very well.\nLet’s look at the mean score:\n\ncv_scores_RF.mean()\n\n0.9922322775263952\n\n\nThis gives us a mean score of .99, very close to our goal of 1. With this in mind, let’s test the data to see how we did."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Testing — Random Forest Classifier Model",
    "text": "Testing — Random Forest Classifier Model\nWith our model trained, we can now import the test data and see how our model performs.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nRF.score(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n1.0\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = RF.predict(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]])\nC = confusion_matrix(y_test, y_test_pred)\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOur model predicted the species of the test data set with 100% accuracy! Yay! As such, our confusion matrix tells us that every penguin was predicted to be the species that it actually belongs to."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision regions:",
    "text": "Plotting Decision regions:\nThe code below is code for plotting data points on top of the decision regions of a given model:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", bbox_to_anchor=(1.05, 1), handles = patches, loc='upper left', borderaxespad=0.)\n\n      \n      \n      plt.tight_layout()\n\nWith this function, we can now pass in our data and model.\n\nTraining Data and Classification Regions\nLets start with the decision regions plotted with the training data. This is insightful for seeing how the model was fit around our training dataset.\n\nplot_regions(RF, X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\n\n\n\n\n\n\n\nIt is interesting to note that this is potentially an overfit. Notice the distinct Adelie region in the Clutch Completion = Yes visualization or the sliver of Gentoo reaching into the Chinstrap region. Let’s take a look at how these did not end up being a problem in the prediction of our testing data species. These are likely the exact points that were causing certain cross validations to be worse than others.\n\n\nTesting Data and Classification Regions\n\nplot_regions(RF, X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n\n\n\n\n\n\nWe can see from these visualizations that this testing data has much less overlap than our training dataset—it does not have many of those “problematic” points seen in the training data. I would be interested to see if this model would hold up on a much larger testing dataset, or if we had been given a different split for test/train. It is also important to note that the testing dataset had very little data with no for clutch completion, and none in that category that were Adelie."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\n\nProcess and Results\nIt was found that it is possible to train a model, using the palmer penguins data set, that is capable of predicting the target value—species—with perfect accuracy. This was achieved through the use of a Random Forrest Classifier model, 3 predictor variables: Clutch Completion, Culmen Length, and Culmen Depth. While a Logistic Regression was attempted, our goal of a perfect prediction score could not be achieved using a Logistic Regression and any combination of 2 quantitative and 1 qualitative predictor variables. It should also be noted that there were particular points in the training dataset that were potentially overfit by our successful Random Forest Classification model. This was observed in cross validation, in which only 3/5 of our splits achieved a perfect score. It was also observed visually in the display of the training data plotted on top of the model’s decision regions. This did not materialize as a poor score in the final testing, because the data in the testing dataset coincidentally did not have many points in the regions where the potential overfitting occurred. A larger dataset, or a different train test split, would be interesting next steps to see if our model selection and feature selection were good ones.\n\n\nLearning\nApart from enhancing my technical skills for exploring a dataset, performing a train test split, creating a repeatable methodology for feature selection, and fitting and testing a model, this process taught me a lot about the potential problems that can come with classification models. Particularly, the limits on logistic regressions in classification, the potential for overfitting, and the importance of having enough varied data in both your training and testing data. I also learned how helpful visualizations can be for understanding what is going on throughout the model creation process."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html",
    "href": "posts/Perceptron-Algorithm/index.html",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n%load_ext autoreload\n%autoreload 2\nfrom minibatchperceptron import MinibatchPerceptron, MinibatchPerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#abstract",
    "href": "posts/Perceptron-Algorithm/index.html#abstract",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Abstract",
    "text": "Abstract\nThis blogpost explores the implementation of the perceptron as well as an exploration of its properties, capabilities, and limitations. This includes exploring how it performs on linearly separable data compared to data that is not linearly separable. Then the perceptron on data with more than 2 features is explored to prove that it still works as long as the data is linearly separable. Then the concept of a minibatch perceptron in which more than one data point is passed to the perceptron at a time."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-source-code",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-source-code",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron Source Code",
    "text": "Perceptron Source Code\nUse this link to access the perceptron source code. Use this link to access the minibatch perceptron source code.\ndef grad(self, X, y):\n    y_ = 2 * y - 1  # convert labels\n    return (-1 * (self.score(X) * y_ &lt; 0).float() * y_ * X).view(-1)\nAs seen in the above code, the grad function works by first converting the target variable classifications from {0,1} to {-1,1}. Inside the parentheses, this result is multiplied by the points corresponding score. The result of this comparison will be negative if the classification is incorrect and positive otherwise. After comparing this result to 0, we are left with a 0 if the classification is correct, and a 1 if it is incorrect. This means that no change is made to correct classifications. For incorrect classifications, they are then multiplied by the true classification and their feature’s to create the change that is returned to the step function when called. The result is that the weight is adjusted by 0 if the classification is correct, and y_ multiplied by each feature if the classification is incorrect. .view(-1) ensures that the return is a 1d vector compatible with the implemented step algorithm.\nMinibatch functions"
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-basic-testing",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-basic-testing",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron Basic Testing",
    "text": "Perceptron Basic Testing\n\nLinearly Seperable Data\nLet’s start by creating and visualizing a set of 300 2D linearly classifiable data points, and visualize them on a 2D plane.\n\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\n\n\nData Subset\nLet’s start with a subset of this data for exploratory purposes:\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 50, noise = 0.2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWhile we have not done so yet, it is easy to see how a line could “linearly classify” (divide) these groups so that each group is on one side of that line.\n\n\nMinimal Training Loop\nLet’s do a minimal training loop on this data to show that our perceptron can correctly classify this data.\n\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    ix = torch.randperm(X.size(0))[:1]\n    x_i = X[ix, :]  \n    y_i = y[ix]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n\n    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \nprint(loss_vec)\nprint(\"Final loss: \", loss)\n   \n    \n\n[tensor(0.5000), tensor(0.5000), tensor(0.)]\nFinal loss:  tensor(0.)\n\n\nOur perceptron came up with a result without reaching our maximum number of itterations, meaning that it found a classification line with a loss equal to 0. Let’s plot it to see what this looks like:\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplot_perceptron_data(X, y, ax)\ndraw_line(p.w, -1, 2, ax, color = \"black\")\n\n\n\n\n\n\n\n\nBeautiful, our model has created a linear classification that splits our two groups into their respective categories."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#complete-run",
    "href": "posts/Perceptron-Algorithm/index.html#complete-run",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Complete Run",
    "text": "Complete Run\nNow let’s do a complete run and demonstrate how our classification line adjusts with each update. Code from these Class notes “Complete Run” section was used to create this run and visualization.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n\n    i = torch.randperm(X.size(0))[:1]\n    x_i = X[i, :]  \n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThis chart shows each update of our weights (classification line). The previous line is shown dashed, the new line is shown solid, and the point the update is based upon is shown outlined in black. As demonstrated in this series of charts, our perceptron is picking a random point and updating the weights so that specific point is classified correctly. Because we only do this for points which are not correctly classified already we know that if the data is linearly separable our perceptron will converge.\n\nNon-Linearly Seperable 2D Data\nLet’s generate some linearly non-separable data:\n\ntorch.manual_seed(1234)\n\ndef inseparable_data(n_points = 50, noise = 0.4, p_dims = 2):\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n\ndef plot_inseparable_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = 2*y[ix]-1, facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nX, y = inseparable_data()\nplot_inseparable_data(X, y, ax)\n\n\n\n\n\n\n\n\nAs can be clearly seen just by looking, there is no line that can separate these two groups of data. The overlapping section means that our data is not linearly separable, and therefore our perceptron will never converge.\nWith setting a maximum number of iterations, we can see more closely how these points cause problems for our perceptron. The following code sets the maximum number of itertions at 1000, and tracks the first 12 updates to demonstrate the mechanisms for why this never converges.\nOnce again, this code is borrowed from the Class notes “Complete Run” section with some minor alterations to set a maximum number of iterations and allow for more updates to be visualized.\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (15, 10)\nfig, axarr = plt.subplots(3, 4, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nmax_iter = 1000\niter = 0\ncount = 0\n\nwhile (loss &gt; 0) & (max_iter &gt;= iter):\n    iter += 1\n    if count &lt; 12:\n        ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    local_loss = p.loss(x_i, y_i).item()\n\n    if local_loss &gt; 0:\n        count +=1\n        opt.step(x_i, y_i)\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if (local_loss &gt; 0) & (count &lt;= 12):\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWhile only visualizing the first 12 updates, it is already clear from the above visualization that this data is never going to converge. The overlapping points in the middle, mean that any corrections will leave some combination in that small area incorrectly classified, and therefore our loss will never reach 0. Instead, by the 4th update, all subsequent updates take part in this back and forth of correcting so that a different, but incomplete, set of points are correctly classified."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#perceptron-on-5d-features",
    "href": "posts/Perceptron-Algorithm/index.html#perceptron-on-5d-features",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Perceptron on 5D Features",
    "text": "Perceptron on 5D Features\nIt will now be shown that our perceptron can function on linearly separable data of more than two (5) dimensions:\n\n\n\ntorch.manual_seed(1234)\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims = 5)\n\nprint(X.shape)\n\n\ntorch.Size([300, 6])\n\n\nWith 5d data for 300 points, we can now do a minimal training loop and track the loss throughout in the same way done before:\n\nimport torch\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0.0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i], :]  # Transpose x_i to make its shape compatible for multiplication\n    y_i = y[i]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\n\n    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \n   \n    \n\nSince our perceptron converged, let’s take a look at how the loss changed throughout our training loop:\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nIt is interesting to see that with a single update, our loss was reduced to .0067 with no more updates for more than 150 iterations when a single further update reduced the loss to 0.\nI know that the data is linearly separable, because the loss ends at 0 and our perceptron converged. There is no way to know that your data is not linearly separable with higher dimensions where you cannot simply look at a visualization and know."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#mini-batch-perceptron",
    "href": "posts/Perceptron-Algorithm/index.html#mini-batch-perceptron",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Mini Batch Perceptron",
    "text": "Mini Batch Perceptron\nIn this section a Minibatch perceptron will be implemented which computes an update using k points at once, rather than a single point.\n\ntorch.manual_seed(1234)\nX, y = perceptron_data(n_points = 100, noise = 0.2, p_dims = 2)\nfig, ax = plt.subplots(1, 1, figsize = (4, 4))\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nWe are once again starting with a subset of the same set of linearly separable points.\n\nMinimal Training Loop\nNow we will run our perceptron on a minimal training loop where we will pass in K random indices to the perceptron:\n\nimport torch\n\n# instantiate a model and an optimizer\np = MinibatchPerceptron() \nopt = MinibatchPerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\nmax_iter = 100000\niter = 0\n\nwhile ((loss &gt; 0.0) & (iter &lt;= max_iter)): # dangerous -- only terminates if data is linearly separable\n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    # pick a random data point\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix, :]  \n    y_i = y[ix]\n \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)    \n    iter = 1 + iter\n\n    if iter == max_iter:\n        print(\"Max iterations reached\")\n        \n\nprint(\"Final loss: \", loss)\nprint(\"loss vector: \", loss_vec)\n\nFinal loss:  tensor(0.)\nloss vector:  [tensor(0.5000), tensor(0.2700), tensor(0.4500), tensor(0.4500), tensor(0.4500), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.0100), tensor(0.2700), tensor(0.0400), tensor(0.0400), tensor(0.0400), tensor(0.0400), tensor(0.)]\n\n\n\ntorch.manual_seed(1234567)\n\ndef min_batch_full_loop(X, y, k, x_plots, y_plots):\n    # initialize a perceptron \n    p = MinibatchPerceptron()\n    opt = MinibatchPerceptronOptimizer(p)\n    p.loss(X, y)\n\n    # set up the figure\n    plt.rcParams[\"figure.figsize\"] = (7, 5)\n    fig, axarr = plt.subplots(x_plots, y_plots, sharex = True, sharey = True)\n    markers = [\"o\", \",\"]\n    marker_map = {-1 : 0, 1 : 1}\n\n    # initialize for main loop\n    current_ax = 0\n    loss = 1\n    loss_vec = []\n\n    while loss &gt; 0:\n        ax = axarr.ravel()[current_ax]\n\n        # save the old value of w for plotting later\n        old_w = torch.clone(p.w)\n\n        # make an optimization step -- this is where the update actually happens\n        # now p.w is the new value \n\n        ix = torch.randperm(X.size(0))[:k]\n        x_i = X[ix, :]  \n        y_i = y[ix]\n        local_loss = p.loss(x_i, y_i).item()\n\n        if local_loss &gt; 0:\n            opt.step(x_i, y_i)\n        # if a change was made, plot the old and new decision boundaries\n        # also add the new loss to loss_vec for plotting below\n        if local_loss &gt; 0:\n            plot_perceptron_data(X, y, ax)\n            draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n            loss = p.loss(X, y).item()\n            loss_vec.append(loss)\n            draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n            #ax.scatter(ix[[]])\n            for i in range(k): # cycle through the k points and outline them in black\n                ax.scatter(X[ix[i]][0], X[ix[i]][1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[2*(y[i].item())-1]])\n            ax.set_title(f\"loss = {loss:.3f}\")\n            ax.set(xlim = (-1, 2), ylim = (-1, 2))\n            current_ax += 1\n    plt.tight_layout()\n\n\n\n\nK = 1 minibatch\nLet’s see what happens when we perform a minibatch with a k of size = 1:\n\nmin_batch_full_loop(X, y, 1, 2,2)\n\n\n\n\n\n\n\n\nAs show, the minibatch acts in exactly the same way as a normal perceptron, which makes sense because it is taking an average of a single point’s correction, which is the same as just basing the correction on that single point.\n\n\nMinibatch with k=10\nNow Let’s try a real minibatch with a k = 10:\n\nmin_batch_full_loop(X, y, 10, 3, 2)\n\n\n\n\n\n\n\n\nAs seen in the above visualization, the correction is done based on the classification of all of the selected points, which includes some who will be correctly classified resulting in a lower mean gradient calculation. That is why the changes are often much smaller than the corrections seen when k = 1.\n\n\nMinibatch when k = n (n=100)\n\nmin_batch_full_loop(X, y, 100, 3, 2)\n\n\n\n\n\n\n\n\nThis point is driven home even more clearly by the demonstration when k = n (100). While the first update is quite large to overcome the fact that half the points are wrongly classified , all subsequent updates are very small due to the fact that so few points of the ones selected are incorrectly classified."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#complexity-analysis",
    "href": "posts/Perceptron-Algorithm/index.html#complexity-analysis",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Complexity Analysis",
    "text": "Complexity Analysis\nWhat is the runtime complexity of a single iteration of the perceptron algorithm? Does the runtime complexity of a single iteration depend on the number of data points n? What about the number of features p?\nThe runtime of the perceptron algorithm is O(p). This is driven by the dot product calculation of the scoring function: X@self.w. As such, the number of features p has a direct impact on the complexity, as the complexity of a dot product is O(n), and in this case the length of the vectors is based on the number of features p. The total number of data points has no impact on this.\nIf you implemented minibatch perceptron, what is the runtime complexity of a single iteration of the minibatch perceptron algorithm?\nFor the minibatch, where k is the size of the batch, a single iteration is O(k*p). This is because we must calculate the score for every one of the points in the batch."
  },
  {
    "objectID": "posts/Perceptron-Algorithm/index.html#discussion",
    "href": "posts/Perceptron-Algorithm/index.html#discussion",
    "title": "Perceptron Algorithm — Blog Post 4",
    "section": "Discussion",
    "text": "Discussion\nThis blog post has explored the perceptron and minibatch perceptron algorithms. The perceptron is an effective algorithm for optimizing weights for linearly separable data, however it is ineffective when data is not linearly separable. It functions by randomly selecting a datapoint, and if it is incorrectly classified by current weights, the weights are update based on the gradient of the error. This process continues until all data points are correctly classified—only possible if the data is in fact linearly separable. This all applies for data that has 2 or more features. However, the time complexity of performing each iteration of the algorithm is increased by the number of features. The minibatch perceptron functions in much the same way as the perceptron, except instead of calculating the weight adjustment for a single datapoint, it takes the average of several. This is interesting as it allows for some adjustment minimizing for points which are classified correctly. Additionally, each additional point in a batch increases the time complexity of an iteration of the batch."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Logistic Regression Implementation\n\n\n\n\n\nImplementing the Logistic Regression algorithm and performing experiments on it.\n\n\n\n\n\nJul 3, 2027\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Logistic Regression\n\n\n\n\n\nImplementing Sparse Kernel Machine and Experimenting on it.\n\n\n\n\n\nApr 4, 2027\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nPerceptron Algorithm — Blog Post 4\n\n\n\n\n\nImplementing the Perceptron Algorithm in Python\n\n\n\n\n\nMar 19, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias\n\n\n\n\n\nPerforming a bias audit on algorithm fit using folktables dataset\n\n\n\n\n\nMar 4, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Decisions — Blog Post 2\n\n\n\n\n\nDesign and Impact of Automated Decisions\n\n\n\n\n\nFeb 28, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nPalmer Penguins Classification blog post.\n\n\n\n\n\nFeb 18, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nUsing the Palmer Penguins dataset to create a predictive model for classifying species of palmer penguins!\n\n\n\n\n\nDec 2, 2024\n\n\nJames Cummings\n\n\n\n\n\n\nNo matching items"
  }
]