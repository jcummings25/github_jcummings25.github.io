[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#abstract",
    "href": "posts/classifying-palmer-penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this blog post is to develop a classification model, trained on the Palmer Penguins dataset, that is capable of predicting the species (Gentoo, Adelie, or Chinstrap) of an unknown penguin. In the process, the goal is to refine skills of feature selection, model fitting, and model testing. This includes creating repeatable and exhaustive feature selection processes, using both testing and training data sets, and analyzing the resulting model for potential underlying issues. The goal is to identify a model with 100% accuracy on the training data at identifying the species of a penguin based on two quantitative and one qualitative predictor feature. A Random Forest Classifier, using Clutch Completion (Yes/No), Culmen Length, and Culmen Depth as predictor variables, yielded a 100% accuracy when tested."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for training:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "href": "posts/classifying-palmer-penguins/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\nThis is what the data looks like:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nLets group our data by Species and Clutch completion, and look at the mean and standard deviation for the quantitative columns. In doing so, we should be able to see some columns that might be useful for differentiating species.\n\ntable = train.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1).groupby(['Clutch Completion', 'Species']).aggregate(['mean', 'std'])\ntable.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\nmean\nstd\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n39.041667\n2.152571\n18.225000\n1.072063\n187.666667\n7.726381\n3764.583333\n605.604350\n8.913195\n0.328239\n-25.676738\n0.485634\n\n\nChinstrap\n49.040000\n4.627022\n18.040000\n0.987927\n194.400000\n7.121173\n3532.500000\n536.196740\n9.425587\n0.346548\n-24.560953\n0.320757\n\n\nGentoo\n46.485714\n3.632001\n14.714286\n1.206135\n216.000000\n6.708204\n4910.714286\n679.131974\n8.279576\n0.235220\n-26.247917\n0.577585\n\n\nYes\nAdelie\n38.962617\n2.698411\n18.429907\n1.235040\n190.355140\n6.549164\n3713.317757\n447.165045\n8.856116\n0.436348\n-25.810248\n0.578651\n\n\nChinstrap\n48.780851\n3.207982\n18.436170\n1.165151\n196.340426\n7.516002\n3788.297872\n366.195595\n9.310443\n0.375693\n-24.551794\n0.230540\n\n\n\n\n\n\n\nThis is helpful, but it might be even more insightful and intuitive to compute the range of 2 standard deviations for each quantitative column. With the standard deviations and means calculated, the range of 2 standard deviations can then be calculated by subtracting and adding 2 times the value of the std dev to/from the mean for that column.\nNote: I found the assign function used below in search of a pandas equivalent of the R mutate()function (See link below)\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html\n\n\ntable = table.assign(Culmen_Length_Max = table[\"Culmen Length (mm)\"][\"mean\"] + 2*table[\"Culmen Length (mm)\"][\"std\"], Culmen_Length_Min = table[\"Culmen Length (mm)\"][\"mean\"] - 2*table[\"Culmen Length (mm)\"][\"std\"])\ntable = table.assign(Culmen_Depth_Max = table[\"Culmen Depth (mm)\"][\"mean\"] + 2*table[\"Culmen Depth (mm)\"][\"std\"], Culmen_Depth_Min = table[\"Culmen Depth (mm)\"][\"mean\"] - 2*table[\"Culmen Depth (mm)\"][\"std\"])\ntable = table.assign(Body_Mass_Max = table[\"Body Mass (g)\"][\"mean\"] + 2*table[\"Body Mass (g)\"][\"std\"], Body_Mass_Min = table[\"Body Mass (g)\"][\"mean\"] - 2*table[\"Body Mass (g)\"][\"std\"])\ntable = table.assign(Flipper_Length_Max = table[\"Flipper Length (mm)\"][\"mean\"] + 2*table[\"Flipper Length (mm)\"][\"std\"], Flipper_Length_Min = table[\"Flipper Length (mm)\"][\"mean\"] - 2*table[\"Flipper Length (mm)\"][\"std\"])\ntable = table.assign(Delta_13_Max = table[\"Delta 13 C (o/oo)\"][\"mean\"] + 2*table[\"Delta 13 C (o/oo)\"][\"std\"], Delta_13_Min = table[\"Delta 13 C (o/oo)\"][\"mean\"] - 2*table[\"Delta 13 C (o/oo)\"][\"std\"])\ntable = table.assign(Delta_15_Max = table[\"Delta 15 N (o/oo)\"][\"mean\"] + 2*table[\"Delta 15 N (o/oo)\"][\"std\"], Delta_15_Min = table[\"Delta 15 N (o/oo)\"][\"mean\"] - 2*table[\"Delta 15 N (o/oo)\"][\"std\"])\ntable.drop([\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Body Mass (g)\", \"Flipper Length (mm)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"], axis = 1)\n\n\n\n\n\n\n\n\n\nCulmen_Length_Max\nCulmen_Length_Min\nCulmen_Depth_Max\nCulmen_Depth_Min\nBody_Mass_Max\nBody_Mass_Min\nFlipper_Length_Max\nFlipper_Length_Min\nDelta_13_Max\nDelta_13_Min\nDelta_15_Max\nDelta_15_Min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClutch Completion\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo\nAdelie\n43.346808\n34.736525\n20.369125\n16.080875\n4975.792033\n2553.374633\n203.119429\n172.213904\n-24.705470\n-26.648007\n9.569673\n8.256716\n\n\nChinstrap\n58.294044\n39.785956\n20.015854\n16.064146\n4604.893481\n2460.106519\n208.642347\n180.157653\n-23.919438\n-25.202468\n10.118684\n8.732490\n\n\nGentoo\n53.749716\n39.221713\n17.126556\n12.302015\n6268.978234\n3552.450338\n229.416408\n202.583592\n-25.092747\n-27.403087\n8.750016\n7.809135\n\n\nYes\nAdelie\n44.359438\n33.565796\n20.899986\n15.959827\n4607.647847\n2818.987667\n203.453469\n177.256812\n-24.652947\n-26.967550\n9.728812\n7.983420\n\n\nChinstrap\n55.196815\n42.364888\n20.766472\n16.105869\n4520.689062\n3055.906683\n211.372430\n181.308421\n-24.090715\n-25.012873\n10.061828\n8.559058\n\n\nGentoo\n52.472111\n41.765667\n16.914536\n12.945464\n6021.446275\n4078.553725\n228.625764\n204.996458\n-25.027738\n-27.255542\n8.810899\n7.678713\n\n\n\n\n\n\n\nThis is extraordinarily insightful, as we can see which quantitative columns have distinct ranges when grouped by Culmen Depth and Species. Since 95.4% of data falls within 2 std deviations of the mean, distinct ranges in this category can help identify very high quality candidate columns to use as quantitative predictor variables.\nUpon analysis of this table, the Culmen Length, Culmen Depth, and Flipper Length appear to the columns with the most variation in ranges across species and Clutch Completion. For penguins with a clutch completion of No, Culmen Length ranges are potentially distinguishable for Adelie compared with Chinstrap and Gentoo, Culmen Depth ranges are distinguishable for Gentoo compared with Adelie and Chinstrap, and Flipper Length has all overlapping ranges although they are also very different with Adelie the low end, Chinstrap in the middle, and Gentoo at the top.\nFor penguins with clutch completion of yes, Culmen Length distinguishes Adelie from Chinstrap and Gentoo, Culmen Depth distinguishes Gentoo from Adelie and Chinstrap, and Flipper Length once again are overlapping but not completely.\nLet’s make visualizations of these differences to analyze them further.\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize = (8, 3.5))\n\n# p1 = sns.scatterplot(train, x = \"Flipper Length (mm)\", y = \"Culmen Length (mm)\",  style = \"Island\", hue = \"Species\")\np1 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Culmen Depth (mm)\", ax = ax[1],  style = \"Clutch Completion\", hue = \"Species\")\n\nsns.despine() # remove the top and right spines\n\np1.title.set_text(\"Culmen Length vs. Culmen Depth\")\n\n\np2 = sns.scatterplot(train, x = \"Culmen Length (mm)\", y = \"Flipper Length (mm)\", ax = ax[0],  style = \"Clutch Completion\", hue = \"Species\")\n\np2.title.set_text(\"Flipper Length vs. Culmen Length\") # set the title of the left visualization\nax[0].get_legend().remove() # remove the legend from the left graph to avoid duplication and messiness\n\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # move the legend to the right graph\n\n\n\n\n\n\n\n\n\n\nFrom these two Plots, it appears that while Flipper Length and Culmen Depth would be very useful for distinguishing Gentoo penguins from the other two species, it would be very difficult to distinguish between the Chinstrap and Adelie penguins. When replacing Flipper Length with Culmen Depth, it appears that while the overlap still exists it may be possible to develop an appropriate model for predicting species.\nRegardless, the most important takeaway is that it seems inlikely by looking at the data that a logistic regression will be capable of achieving 100% accuracy using these features, as there are some overlapping—or close to overlapping—regions on both of these graphs."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "href": "posts/classifying-palmer-penguins/index.html#choosing-features",
    "title": "Classifying Palmer Penguins",
    "section": "Choosing Features",
    "text": "Choosing Features\nNow that we have explored the data and established that a model may be effective and developed some hypothesis regarding which features and models might work best, we need to do an exhaustive and repeatable feature search to ensure that we create the best possible model.\n\nLogistic Regression\nFirst, let’s try using a Logistic Regression even though our visualizations suggested this might not be 100% effective.\nIn this code we cycle through all combinations of 1 qualitative and 2 quantitative predictor features, saving the score and feature set that perform the best.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    LR = LogisticRegression(max_iter = 10000)\n    LR.fit(X_train[cols], y_train)\n    new_score = LR.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  0.99609375\n\n\nUsing a Logistic Regression, the highest score we find is 0.996 using Sex, Culmen Length, and Culmen depth. Because our aim is 100%, there is no logistic regression using 1 qualitative and 2 quantitative predictor features that will achieve our goal. let’s take a look at using a different model\n\n\nRandom Forest Classifier\nThe same process is followed as above, except the Logistic Regression is substituted for a Random Forrest Classifier.\n\nfrom itertools import combinations\nfrom sklearn.ensemble import RandomForestClassifier\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', \"Body Mass (g)\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\"]\nbest_score = 0\nbest_cols = []\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    new_score = RF.score(X_train[cols], y_train)\n    if new_score &gt; best_score:\n      best_score = new_score\n      best_cols = cols  \n\n\n\n\nprint(\"Highest Scoring Columns: \", best_cols)\nprint(\"Highest Score: \", best_score)\n\nHighest Scoring Columns:  ['Clutch Completion_No', 'Clutch Completion_Yes', 'Culmen Length (mm)', 'Culmen Depth (mm)']\nHighest Score:  1.0\n\n\nWonderful! Our search has found that using a Random Forest Classifier, CLutch Completion, Culmen Length, and Culmen Depth results in a perfect score when scored on the training data. Now we need to fit the model with the best columns found above."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#fitting-the-model-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Fitting the Model — Random Forest Classifier Model",
    "text": "Fitting the Model — Random Forest Classifier Model\nSince we have identified that the Random Forest Classifier model has a score of 1 with the columns of Clutch Completion, Culmen Length, and Culmen depth, let’s train our model:\n\nRF = RandomForestClassifier()\nRF.fit(X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\nRandomForestClassifier()\n\n\n\nCross Validation\nBefore Committing to the Random Forest Classifier Model, let’s cross validate it!\n\nfrom sklearn.model_selection import cross_val_score\ncv_scores_RF = cross_val_score(RF, X_train, y_train, cv=5)\ncv_scores_RF\n\narray([0.98076923, 1.        , 1.        , 0.98039216, 1.        ])\n\n\n3 / 5 of our folds score perfectly in cross validation. This suggests that there are specific values in our dataset that are difficult to classify if they are in the testing data and not in the training data, but also that our model is performing very well.\nLet’s look at the mean score:\n\ncv_scores_RF.mean()\n\n0.9922322775263952\n\n\nThis gives us a mean score of .99, very close to our goal of 1. With this in mind, let’s test the data to see how we did."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "href": "posts/classifying-palmer-penguins/index.html#testing-random-forest-classifier-model",
    "title": "Classifying Palmer Penguins",
    "section": "Testing — Random Forest Classifier Model",
    "text": "Testing — Random Forest Classifier Model\nWith our model trained, we can now import the test data and see how our model performs.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\nX_test, y_test = prepare_data(test)\nRF.score(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n1.0\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\ny_test_pred = RF.predict(X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]])\nC = confusion_matrix(y_test, y_test_pred)\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {C[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Adelie Penguin (Pygoscelis adeliae) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 11 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 0 Chinstrap penguin (Pygoscelis antarctica) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Adelie Penguin (Pygoscelis adeliae).\nThere were 0 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Chinstrap penguin (Pygoscelis antarctica).\nThere were 26 Gentoo penguin (Pygoscelis papua) penguin(s) who were classified as Gentoo penguin (Pygoscelis papua).\n\n\nOur model predicted the species of the test data set with 100% accuracy! Yay! As such, our confusion matrix tells us that every penguin was predicted to be the species that it actually belongs to."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "href": "posts/classifying-palmer-penguins/index.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision regions:",
    "text": "Plotting Decision regions:\nThe code below is code for plotting data points on top of the decision regions of a given model:\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", bbox_to_anchor=(1.05, 1), handles = patches, loc='upper left', borderaxespad=0.)\n\n      \n      \n      plt.tight_layout()\n\nWith this function, we can now pass in our data and model.\n\nTraining Data and Classification Regions\nLets start with the decision regions plotted with the training data. This is insightful for seeing how the model was fit around our training dataset.\n\nplot_regions(RF, X_train[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_train)\n\n\n\n\n\n\n\n\nIt is interesting to note that this is potentially an overfit. Notice the distinct Adelie region in the Clutch Completion = Yes visualization or the sliver of Gentoo reaching into the Chinstrap region. Let’s take a look at how these did not end up being a problem in the prediction of our testing data species. These are likely the exact points that were causing certain cross validations to be worse than others.\n\n\nTesting Data and Classification Regions\n\nplot_regions(RF, X_test[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]], y_test)\n\n\n\n\n\n\n\n\nWe can see from these visualizations that this testing data has much less overlap than our training dataset—it does not have many of those “problematic” points seen in the training data. I would be interested to see if this model would hold up on a much larger testing dataset, or if we had been given a different split for test/train. It is also important to note that the testing dataset had very little data with no for clutch completion, and none in that category that were Adelie."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#discussion",
    "href": "posts/classifying-palmer-penguins/index.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\n\nProcess and Results\nIt was found that it is possible to train a model, using the palmer penguins data set, that is capable of predicting the target value—species—with perfect accuracy. This was achieved through the use of a Random Forrest Classifier model, 3 predictor variables: Clutch Completion, Culmen Length, and Culmen Depth. While a Logistic Regression was attempted, our goal of a perfect prediction score could not be achieved using a Logistic Regression and any combination of 2 quantitative and 1 qualitative predictor variables. It should also be noted that there were particular points in the training dataset that were potentially overfit by our successful Random Forest Classification model. This was observed in cross validation, in which only 3/5 of our splits achieved a perfect score. It was also observed visually in the display of the training data plotted on top of the model’s decision regions. This did not materialize as a poor score in the final testing, because the data in the testing dataset coincidentally did not have many points in the regions where the potential overfitting occurred. A larger dataset, or a different train test split, would be interesting next steps to see if our model selection and feature selection were good ones.\n\n\nLearning\nApart from enhancing my technical skills for exploring a dataset, performing a train test split, creating a repeatable methodology for feature selection, and fitting and testing a model, this process taught me a lot about the potential problems that can come with classification models. Particularly, the limits on logistic regressions in classification, the potential for overfitting, and the importance of having enough varied data in both your training and testing data. I also learned how helpful visualizations can be for understanding what is going on throughout the model creation process."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "\"CSCI\" + \" 0451\"\n\n'CSCI 0451'"
  },
  {
    "objectID": "posts/new-test-post/index.html#setting-up-the-data",
    "href": "posts/new-test-post/index.html#setting-up-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Setting Up the Data",
    "text": "Setting Up the Data\nImport pandas, and the data for trainging:\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nThis is what the data looks like:\n\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/new-test-post/index.html#lets-explore-the-data",
    "href": "posts/new-test-post/index.html#lets-explore-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s Explore the data:",
    "text": "Let’s Explore the data:\n\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\n\n\ntrain.groupby(['Species', 'Island']).aggregate(\"mean\")\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 1:10]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\n\n\n\n\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nPAL0708\n115\n22\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN61A1\nN11A2\n\n\nGentoo\nPAL0708\n124\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN8A2\nN11A1\n\n\nDream\nAdelie\nPAL0708\n152\n31\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN85A2\nN21A1\n\n\nChinstrap\nPAL0708\n68\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN99A2\nN100A1\n\n\nTorgersen\nAdelie\nPAL0708\n132\n1\nAnvers\nAnvers\nAdult, 1 Egg Stage\nAdult, 1 Egg Stage\nN9A2\nN10A1\n\n\n\n\n\n\n\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 10:20]\n\n\n\n\n\n\n\n\n\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\nYes\nNo\n11/9/09\n11/10/07\n45.6\n34.5\n21.1\n16.0\n203.0\n172.0\n\n\nGentoo\nYes\nNo\n12/3/07\n11/13/08\n55.9\n40.9\n17.3\n13.1\n230.0\n207.0\n\n\nDream\nAdelie\nYes\nNo\n11/9/07\n11/10/08\n44.1\n34.0\n21.2\n16.5\n208.0\n178.0\n\n\nChinstrap\nYes\nNo\n12/3/07\n11/14/08\n58.0\n40.9\n20.8\n16.4\n212.0\n178.0\n\n\nTorgersen\nAdelie\nYes\nNo\n11/9/08\n11/11/07\n46.0\n34.1\n21.5\n15.9\n210.0\n176.0\n\n\n\n\n\n\n\nPotentially egg date and Flipper length and island.\n\ntrain.groupby(['Island', 'Species']).aggregate(['max', 'min']).iloc[:, 20:29]\n\n\n\n\n\n\n\n\n\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\n\n\n\n\nmax\nmin\nmax\nmin\nmax\nmin\n\n\nIsland\nSpecies\n\n\n\n\n\n\n\n\n\n\nBiscoe\nAdelie\n4725.0\n2850.0\n9.79532\n8.08138\n-24.36130\n-26.78958\n\n\nGentoo\n6300.0\n3950.0\n8.83352\n7.63220\n-25.00169\n-27.01854\n\n\nDream\nAdelie\n4650.0\n2975.0\n9.72764\n8.01485\n-24.52698\n-26.69543\n\n\nChinstrap\n4800.0\n2700.0\n10.02544\n8.47173\n-23.78767\n-25.14550\n\n\nTorgersen\nAdelie\n4700.0\n2900.0\n9.59462\n7.69778\n-23.90309\n-26.53870\n\n\n\n\n\n\n\n\ntrain.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\") \n\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nRegion\nStage\nIndividual ID\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\nSpecies\nClutch Completion\nIsland\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n\n\nDream\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n3\n2\n2\n3\n\n\nTorgersen\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n\n\nYes\nBiscoe\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n31\n0\n\n\nDream\n42\n42\n42\n42\n42\n42\n42\n42\n42\n42\n41\n39\n39\n3\n\n\nTorgersen\n35\n35\n35\n35\n35\n35\n34\n34\n34\n34\n30\n29\n29\n8\n\n\nChinstrap\nNo\nDream\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n10\n0\n\n\nYes\nDream\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n47\n46\n47\n0\n\n\nGentoo\nNo\nBiscoe\n8\n8\n8\n8\n8\n8\n7\n7\n7\n7\n7\n7\n7\n0\n\n\nYes\nBiscoe\n90\n90\n90\n90\n90\n90\n90\n90\n90\n90\n87\n89\n89\n0\n\n\n\n\n\n\n\n\ntabledf = train.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"count\")\ntabledf = tabledf.drop(tabledf.columns[1:], axis=1)\ntabledf = tabledf.rename(columns={\"studyName\": \"Count\"})\ntabledf.reset_index(level = ['Clutch Completion', 'Island'], inplace = True)\ntabledf\n\n\n\n\n\n\n\n\nClutch Completion\nIsland\nCount\n\n\nSpecies\n\n\n\n\n\n\n\nAdelie\nNo\nBiscoe\n2\n\n\nAdelie\nNo\nDream\n3\n\n\nAdelie\nNo\nTorgersen\n7\n\n\nAdelie\nYes\nBiscoe\n31\n\n\nAdelie\nYes\nDream\n42\n\n\nAdelie\nYes\nTorgersen\n35\n\n\nChinstrap\nNo\nDream\n10\n\n\nChinstrap\nYes\nDream\n47\n\n\nGentoo\nNo\nBiscoe\n8\n\n\nGentoo\nYes\nBiscoe\n90\n\n\n\n\n\n\n\n\ntabledf.groupby(['Species', 'Clutch Completion', 'Island']).aggregate(\"sum\") / tabledf.groupby(['Species', 'Island']).aggregate(\"sum\")\n\n\n\n\n\n\n\n\n\n\nCount\n\n\nSpecies\nIsland\nClutch Completion\n\n\n\n\n\nAdelie\nBiscoe\nNo\n0.060606\n\n\nYes\n0.939394\n\n\nDream\nNo\n0.066667\n\n\nYes\n0.933333\n\n\nTorgersen\nNo\n0.166667\n\n\nYes\n0.833333\n\n\nChinstrap\nDream\nNo\n0.175439\n\n\nYes\n0.824561\n\n\nGentoo\nBiscoe\nNo\n0.081633\n\n\nYes\n0.918367\n\n\n\n\n\n\n\n\nfrom itertools import combinations\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    print(cols)\n    # you could train models and score them here, keeping the list of \n    # columns for the model that has the best score."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nPalmer Penguins Classification blog post.\n\n\n\n\n\nFeb 18, 2025\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nUsing the Palmer Penguins dataset to create a predictive model for classifying species of palmer penguins!\n\n\n\n\n\nDec 2, 2024\n\n\nJames Cummings\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]